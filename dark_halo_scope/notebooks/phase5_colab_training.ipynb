{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 5: Dark Halo Scope - Lens Finder Training\n",
        "\n",
        "**GPU Required**: Runtime ‚Üí Change runtime type ‚Üí T4/A100 GPU\n",
        "\n",
        "## Setup Instructions (Do Once)\n",
        "1. Click the **key icon** üîë in the left sidebar (Secrets)\n",
        "2. Add two secrets:\n",
        "   - Name: `AWS_ACCESS_KEY_ID` ‚Üí Value: Your AWS access key\n",
        "   - Name: `AWS_SECRET_ACCESS_KEY` ‚Üí Value: Your AWS secret key\n",
        "3. Toggle \"Notebook access\" ON for both secrets\n",
        "4. Run cells in order from top to bottom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1. Check GPU and Install Dependencies\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv'], \n",
        "                       capture_output=True, text=True)\n",
        "print(\"GPU Available:\")\n",
        "print(result.stdout)\n",
        "\n",
        "# Install dependencies\n",
        "%pip install -q boto3 s3fs pyarrow fsspec tqdm\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2. Configure AWS Credentials\n",
        "import os\n",
        "\n",
        "# Load from Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "    os.environ['AWS_DEFAULT_REGION'] = 'us-east-2'\n",
        "    print(\"‚úÖ AWS credentials loaded from Colab Secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading secrets: {e}\")\n",
        "    print(\"Please add AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to Colab Secrets (key icon)\")\n",
        "\n",
        "# Verify connection\n",
        "import boto3\n",
        "try:\n",
        "    s3 = boto3.client('s3')\n",
        "    response = s3.list_objects_v2(Bucket='darkhaloscope', Prefix='phase4_pipeline/', MaxKeys=1)\n",
        "    print(\"‚úÖ S3 connection verified\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå S3 connection failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3. Download Training Data from S3\n",
        "import boto3\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration - CHANGE THIS FOR YOUR RUN\n",
        "BUCKET = 'darkhaloscope'\n",
        "\n",
        "# For SMOKE TEST (debug tier, ~6k rows):\n",
        "S3_PREFIX = 'phase4_pipeline/phase4c/v3_color_relaxed/stamps/debug_stamp64_bandsgrz_gridgrid_small/'\n",
        "\n",
        "# For FULL TRAINING (train tier, ~10M rows) - uncomment below:\n",
        "# S3_PREFIX = 'phase4_pipeline/phase4c/v3_color_relaxed/stamps/train_stamp64_v2/'\n",
        "\n",
        "LOCAL_DIR = '/content/data/stamps'\n",
        "MAX_FILES = None  # None = download all files in the prefix\n",
        "\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# List all parquet files\n",
        "print(f\"Listing files from s3://{BUCKET}/{S3_PREFIX}...\")\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "all_files = []\n",
        "for page in paginator.paginate(Bucket=BUCKET, Prefix=S3_PREFIX):\n",
        "    for obj in page.get('Contents', []):\n",
        "        if obj['Key'].endswith('.parquet'):\n",
        "            all_files.append(obj)\n",
        "\n",
        "print(f\"Found {len(all_files)} parquet files\")\n",
        "\n",
        "# Download files\n",
        "files_to_download = all_files[:MAX_FILES] if MAX_FILES else all_files\n",
        "total_size = sum(f['Size'] for f in files_to_download) / (1024**3)\n",
        "print(f\"Downloading {len(files_to_download)} files ({total_size:.2f} GB)...\")\n",
        "\n",
        "for obj in tqdm(files_to_download):\n",
        "    key = obj['Key']\n",
        "    filename = key.split('/')[-1]\n",
        "    local_path = os.path.join(LOCAL_DIR, filename)\n",
        "    if not os.path.exists(local_path):\n",
        "        s3.download_file(BUCKET, key, local_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Downloaded {len(files_to_download)} files to {LOCAL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4. Validate Data\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "import io\n",
        "import glob\n",
        "\n",
        "parquet_files = sorted(glob.glob('/content/data/stamps/*.parquet'))\n",
        "print(f\"Found {len(parquet_files)} local parquet files\")\n",
        "\n",
        "# Check first file\n",
        "pf = pq.ParquetFile(parquet_files[0])\n",
        "print(f\"\\nSchema columns ({len(pf.schema.names)}):\")\n",
        "print(pf.schema.names[:20], \"...\" if len(pf.schema.names) > 20 else \"\")\n",
        "print(f\"\\nRows in first file: {pf.metadata.num_rows}\")\n",
        "\n",
        "# Read one row and decode stamp_npz\n",
        "table = pf.read_row_group(0)\n",
        "stamp_npz_bytes = table['stamp_npz'][0].as_py()\n",
        "with np.load(io.BytesIO(stamp_npz_bytes)) as npz:\n",
        "    print(f\"\\nStamp NPZ contents: {list(npz.keys())}\")\n",
        "    for k in npz.keys():\n",
        "        print(f\"  {k}: shape={npz[k].shape}, dtype={npz[k].dtype}\")\n",
        "\n",
        "# Count total rows\n",
        "total_rows = sum(pq.ParquetFile(f).metadata.num_rows for f in parquet_files)\n",
        "print(f\"\\n‚úÖ Total rows across all files: {total_rows:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5. Training Code (Self-Contained)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "import io\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# ============== Model ==============\n",
        "class LensFinderCNN(nn.Module):\n",
        "    \"\"\"ResNet-18 style CNN for lens detection.\"\"\"\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(64, 64, 2)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512, 1)\n",
        "    \n",
        "    def _make_layer(self, in_ch, out_ch, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(self._block(in_ch, out_ch, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(self._block(out_ch, out_ch, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def _block(self, in_ch, out_ch, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ============== Dataset ==============\n",
        "class StampDataset(Dataset):\n",
        "    def __init__(self, parquet_files, augment=False):\n",
        "        self.files = parquet_files\n",
        "        self.augment = augment\n",
        "        self._build_index()\n",
        "    \n",
        "    def _build_index(self):\n",
        "        self.index = []\n",
        "        for fpath in self.files:\n",
        "            pf = pq.ParquetFile(fpath)\n",
        "            n = pf.metadata.num_rows\n",
        "            for i in range(n):\n",
        "                self.index.append((fpath, i))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        fpath, row_idx = self.index[idx]\n",
        "        table = pq.read_table(fpath, columns=['stamp_npz', 'is_control'])\n",
        "        \n",
        "        # Decode stamp\n",
        "        npz_bytes = table['stamp_npz'][row_idx].as_py()\n",
        "        with np.load(io.BytesIO(npz_bytes)) as npz:\n",
        "            img_g = npz['image_g'].astype(np.float32)\n",
        "            img_r = npz['image_r'].astype(np.float32)\n",
        "            img_z = npz['image_z'].astype(np.float32)\n",
        "        \n",
        "        # Stack and normalize\n",
        "        img = np.stack([img_g, img_r, img_z], axis=0)\n",
        "        for c in range(3):\n",
        "            med = np.nanmedian(img[c])\n",
        "            mad = np.nanmedian(np.abs(img[c] - med)) + 1e-8\n",
        "            img[c] = (img[c] - med) / (mad * 1.4826)\n",
        "        img = np.nan_to_num(img, nan=0.0)\n",
        "        img = np.clip(img, -10, 10)\n",
        "        \n",
        "        # Augmentation\n",
        "        if self.augment:\n",
        "            if np.random.rand() > 0.5:\n",
        "                img = img[:, ::-1, :].copy()\n",
        "            if np.random.rand() > 0.5:\n",
        "                img = img[:, :, ::-1].copy()\n",
        "            k = np.random.randint(4)\n",
        "            img = np.rot90(img, k, axes=(1, 2)).copy()\n",
        "        \n",
        "        # Label: is_control=1 means no lens (negative), is_control=0 means injection (positive)\n",
        "        is_control = table['is_control'][row_idx].as_py()\n",
        "        label = 0.0 if is_control == 1 else 1.0\n",
        "        \n",
        "        return torch.from_numpy(img), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "print(\"‚úÖ Training code defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6. Run Training\n",
        "# Configuration\n",
        "EPOCHS = 10  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.0003  #@param {type:\"number\"}\n",
        "VAL_SPLIT = 0.1  #@param {type:\"number\"}\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data - shuffle files first to mix classes\n",
        "import random\n",
        "parquet_files = sorted(glob.glob('/content/data/stamps/*.parquet'))\n",
        "random.seed(42)\n",
        "random.shuffle(parquet_files)\n",
        "\n",
        "n_val = max(1, int(len(parquet_files) * VAL_SPLIT))\n",
        "val_files = parquet_files[:n_val]\n",
        "train_files = parquet_files[n_val:]\n",
        "\n",
        "print(f\"Train files: {len(train_files)}, Val files: {len(val_files)}\")\n",
        "\n",
        "train_dataset = StampDataset(train_files, augment=True)\n",
        "val_dataset = StampDataset(val_files, augment=False)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# Check class distribution\n",
        "train_labels = [train_dataset[i][1].item() for i in range(min(1000, len(train_dataset)))]\n",
        "val_labels_check = [val_dataset[i][1].item() for i in range(min(500, len(val_dataset)))]\n",
        "print(f\"Train class balance (sample): {sum(train_labels)}/{len(train_labels)} positives\")\n",
        "print(f\"Val class balance (sample): {sum(val_labels_check)}/{len(val_labels_check)} positives\")\n",
        "\n",
        "if len(set(val_labels_check)) < 2:\n",
        "    print(\"‚ö†Ô∏è WARNING: Validation set has only ONE class! AUROC will be undefined.\")\n",
        "    print(\"   This is fine for smoke test - training still works, metrics just won't compute.\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model\n",
        "model = LensFinderCNN().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "os.makedirs('/content/checkpoints', exist_ok=True)\n",
        "best_auroc = -1.0  # Start at -1 so we always save at least once\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs).squeeze(-1)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    scheduler.step()\n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # Validate\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = model(imgs).squeeze(-1)\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_loss /= len(val_loader)\n",
        "    \n",
        "    # Calculate AUROC only if both classes present\n",
        "    if len(set(val_labels)) >= 2:\n",
        "        auroc = roc_auc_score(val_labels, val_preds)\n",
        "    else:\n",
        "        auroc = float('nan')\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, AUROC={auroc:.4f}\")\n",
        "    \n",
        "    # Save best model - use AUROC if available, else use val_loss\n",
        "    save_checkpoint = False\n",
        "    if not np.isnan(auroc) and auroc > best_auroc:\n",
        "        best_auroc = auroc\n",
        "        save_checkpoint = True\n",
        "        save_reason = f\"AUROC={auroc:.4f}\"\n",
        "    elif np.isnan(auroc) and val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        save_checkpoint = True\n",
        "        save_reason = f\"Val Loss={val_loss:.4f}\"\n",
        "    \n",
        "    if save_checkpoint:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'auroc': auroc if not np.isnan(auroc) else None,\n",
        "            'val_loss': val_loss,\n",
        "        }, '/content/checkpoints/checkpoint_best.pt')\n",
        "        print(f\"  ‚úÖ New best model saved! {save_reason}\")\n",
        "    \n",
        "    # Always save last checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'auroc': auroc if not np.isnan(auroc) else None,\n",
        "        'val_loss': val_loss,\n",
        "    }, '/content/checkpoints/checkpoint_last.pt')\n",
        "\n",
        "print(f\"\\nüéâ Training complete!\")\n",
        "print(f\"   Best AUROC: {best_auroc:.4f}\" if best_auroc > 0 else \"   Best Val Loss: {best_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 7. Upload Results to S3\n",
        "import boto3\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Find available checkpoint\n",
        "checkpoint_paths = [\n",
        "    '/content/checkpoints/checkpoint_best.pt',\n",
        "    '/content/checkpoints/checkpoint_last.pt'\n",
        "]\n",
        "\n",
        "checkpoint_path = None\n",
        "for path in checkpoint_paths:\n",
        "    if os.path.exists(path):\n",
        "        checkpoint_path = path\n",
        "        break\n",
        "\n",
        "if checkpoint_path is None:\n",
        "    print(\"‚ùå No checkpoint found! Training may have failed.\")\n",
        "else:\n",
        "    # Upload best/last checkpoint\n",
        "    s3_key = f'phase5/models/colab/checkpoint_{timestamp}.pt'\n",
        "    print(f\"Uploading {checkpoint_path} to s3://darkhaloscope/{s3_key}...\")\n",
        "    s3.upload_file(checkpoint_path, 'darkhaloscope', s3_key)\n",
        "    print(f\"‚úÖ Uploaded to s3://darkhaloscope/{s3_key}\")\n",
        "\n",
        "    # Also save as latest\n",
        "    s3.upload_file(checkpoint_path, 'darkhaloscope', 'phase5/models/colab/checkpoint_latest.pt')\n",
        "    print(f\"‚úÖ Uploaded to s3://darkhaloscope/phase5/models/colab/checkpoint_latest.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Optional: Download Full Dataset\n",
        "\n",
        "If the smoke test above works, run this cell to download ALL data (~50GB, ~30 min).\n",
        "Then re-run cells 4-7 with more epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 8. Download Full Dataset (Optional - ~50GB)\n",
        "# ‚ö†Ô∏è Only run this after smoke test passes!\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "BUCKET = 'darkhaloscope'\n",
        "S3_PREFIX = 'phase4_pipeline/v3_color_relaxed/stage4c/train_stamp64_v2/stamps/'\n",
        "LOCAL_DIR = '/content/data/stamps'\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# List ALL files\n",
        "print(\"Listing all files (this may take a minute)...\")\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "all_files = []\n",
        "for page in paginator.paginate(Bucket=BUCKET, Prefix=S3_PREFIX):\n",
        "    for obj in page.get('Contents', []):\n",
        "        if obj['Key'].endswith('.parquet'):\n",
        "            all_files.append(obj)\n",
        "\n",
        "total_size = sum(f['Size'] for f in all_files) / (1024**3)\n",
        "print(f\"Total: {len(all_files)} files, {total_size:.2f} GB\")\n",
        "\n",
        "# Download all\n",
        "for obj in tqdm(all_files, desc='Downloading'):\n",
        "    key = obj['Key']\n",
        "    filename = key.split('/')[-1]\n",
        "    local_path = os.path.join(LOCAL_DIR, filename)\n",
        "    if not os.path.exists(local_path):\n",
        "        s3.download_file(BUCKET, key, local_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Full dataset downloaded! Now re-run cells 4-7 with EPOCHS=50\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
