THere is a lot of technical papers relevant to Strong gravitational lenses in DESI . I searched for the author Christopher Storfer and found this list : https://inspirehep.net/authors/2091894 some of them is very latest in Jan 2026. Please carefully review all the papers and strongly look at how we can set ourselves for success in original research with clear concrete goal definition and steps for achieving them . Note clearly specify the research question and methodology. Please validate your answer by look at other relevant support. As we are expecting original research and not replicating them for MNRAS, we need a good amount of original novelty, focus on that . provide multiple options here and rank them by what looks most promising and doable. I believe we can find very good suggestions and approaches here and we can get it published. Provide full concrete plan which should be scientifically meaningful and pass the question : is this research at the level the folks in the papers are doing ?  Below are four original-research directions that are incremental relative to Huang et al. Papers I–IV, but still plausibly MNRAS-level if executed cleanly with strong controls, ablations, and honest limitations. I’m ranking by a mix of novelty, doability in 4–8 weeks, and likelihood reviewers accept the contribution as “new” rather than a replication.
I am grounding the “what they did” claims in Paper IV (your uploaded PDF), including their deployment population, candidate triage, and ensemble/meta-learner specifics.

Option 1 (Most promising + doable): A selection-function audit that separates “detectability” from “model bias”
Research question
For DR10 lens searches, what fraction of “known lenses” are non-detections because the lens is intrinsically not visible in DR10, versus non-detections due to model failure?
This is the key gap in most ML lens papers: they report recall, but don’t decompose why recall fails.
Core novelty
A two-stage selection function:
1. Observability / detectability gate (data-limited): “Would a human or a generic arc-energy detector see any arc-like signal in DR10 at all?”
2. Model gate (algorithm-limited): “Given it is observable, does the model recover it?”
This creates a defensible separation between “not in the data” and “in the data but missed”.
Methodology blueprint
1. Define detectability metrics computed from DR10 cutouts (no ML needed):
    * Arc-annulus energy after subtracting an azimuthal model (you already have this concept).
    * Multi-band consistency: arc-energy must cohere in g/r/z (or at least r/z).
    * A “background stability” metric: noise stationarity in the annulus.
2. Calibrate detectability thresholds using:
    * Your Tier-A anchors (confirmed).
    * A matched random galaxy sample (negatives).
3. Compute two recalls:
    * Observable recall: fraction of anchors passing detectability gate.
    * Conditional model recall: fraction of those observable anchors recovered by the CNN.
4. Report completeness surfaces:
    * completeness vs (PSF FWHM, depth, z_nexp), and any lens proxy you have (theta_E for anchors).
5. Failure mode atlas:
    * For false negatives, label the failure as:
        * “Not detectable in DR10” vs “Detectable but missed”.
    * Provide representative cutouts.
Why reviewers will accept this as original
It directly addresses the fairness critique: “you evaluated on lenses that are not visible at DR10 resolution.” Your paper would formalize that as a measurable selection function instead of hand-waving.
Success criteria (paper-level)
* A figure that cleanly shows detectability vs model performance, and how they vary with observing conditions.
* A reproducible procedure: anyone can apply the detectability gate and reproduce the selection function.
Key pitfalls (and how to preempt)
* Pitfall: Detectability gate becomes “another classifier” in disguise. Preempt: Keep it deliberately simple and interpretable (energy in annulus, multi-band coherence).
* Pitfall: Small-N anchors cause noisy surfaces. Preempt: Use coarse binning + Bayesian/binomial intervals and clearly mark bins with insufficient support.

Option 2: Contaminant taxonomy + prevalence-aware FPR for DR10 lens searches
Research question
What contaminant morphologies dominate the top-scored candidates in DR10, and how does FPR change by object type and observing conditions?
Paper IV reports candidate counts and breaks down by Tractor type in DR10. A strong follow-on is to make false positives measurable and actionable.
Core novelty
A structured contaminant set plus category-conditional FPR that can be plugged into search planning:
* “If you scan 43M galaxies (their deployment scale), what fraction of the top 0.01% are rings vs spirals vs mergers vs artifacts?”
Methodology blueprint
1. Expand your contaminant set to include the categories that actually dominate top candidates:
    * rings, spirals, mergers, edge-on disks, bright-star artifacts, plus blue clumpy star-formers (often the sleeper confuser).
2. For each category:
    * Measure FPR at multiple thresholds (top-K, top p>t).
    * Stratify by (PSF, depth, z_nexp).
3. Add a prevalence model (even if rough):
    * Use a large random sample of DR10 galaxies and run a lightweight morphology tagger or Tractor-type + color cuts as a proxy.
    * Convert category-conditional FPR into an estimated overall FPR for a realistic search population.
4. Deliverable that reviewers like:
    * A table: “Expected confuser composition of top candidates for a DR10-like search.”
Pitfalls
* Prevalence estimation can be attacked. Make it explicitly “scenario-based” with sensitivity tests.

Option 3: Ensemble diversification that is truly “different data”, not just “different architecture”
Paper IV uses ResNet + EfficientNet and stacks them with a small meta-learner. They also emphasize deployment-scale scanning and top-recommendation selection.
Research question
Can we improve reliability (calibration + robustness to confusers) by training domain-specialized models on disjoint DR10 regimes, and combining them with an uncertainty-aware ensemble?
Core novelty
Not “new backbone”, but new training regime:
* Split by observing condition regime (seeing/depth/exposures) and by galaxy population regime (Tractor type, color).
* Show that ensemble disagreement predicts false positives and low-quality candidates.
Methodology blueprint
1. Define 3–6 domains:
    * Good-seeing / bad-seeing
    * Deep / shallow
    * High exposure count / low exposure count
    * SER/DEV vs EXP/REX (mirrors the type breakdown they report)
2. Train one model per domain (same architecture).
3. Combine with:
    * Simple averaging
    * Stacking
    * And an uncertainty score: entropy, variance across models, or disagreement rate
4. Evaluate:
    * Anchor recall
    * Contaminant FPR
    * Calibration (ECE) and “selective classification” curves: performance vs coverage as you drop high-uncertainty predictions.
Pitfalls
* If positives are Huang-discovered candidates, label circularity will bite. Your defense is to:
    * Separate evaluation on Tier-A confirmed anchors from training positives.
    * Report results both with candidates-in-training and without (ablation).

Option 4 (Higher novelty, higher risk): “Foundation features” as a controlled baseline, not a hype pivot
Paper IV mentions foundations/custom CNNs in the title, and uses a classical CNN ensemble approach. A strong contribution is a careful audit: when do frozen features help, and when do they fail?
Research question
Do frozen self-supervised vision features improve DR10 lens detection robustness (especially to confusers and observing-condition shifts) compared to end-to-end CNNs trained on small positives?
Blueprint
1. Use a frozen encoder (e.g., DINOv2-like) and train only a small head.
2. Compare to ResNet18 end-to-end.
3. Evaluate on:
    * Anchors
    * Contaminants
    * Cross-regime shifts (good seeing vs bad seeing)
4. Report where it helps and where it fails, with interpretability.
Pitfalls
* Reviewer skepticism if it reads like “we tried a trendy model”. Keep it as a controlled baseline with strong negative results allowed.

Ranking summary
1. Selection-function audit separating detectability vs model failure (best novelty-to-effort ratio, strongest “referee-proof” framing)
2. Contaminant taxonomy + prevalence-aware FPR (practical, publishable, complements existing candidate papers)
3. True domain-diversified ensemble with uncertainty (incremental but defensible if you show real robustness gains)
4. Foundation-feature controlled baseline (novel but riskier to execute convincingly in your timeline)

Concrete 4-week plan for Option 1 (recommended)
Week 1: Data + “detectability gate”
* Finalize Tier-A anchors and contaminants.
* Implement detectability metrics (annulus energy, multi-band coherence, background stability).
* Calibrate a conservative detectability threshold using anchors vs random galaxies.
Week 2: Train 2–3 real-data models
* Baseline ResNet18
* One “robustness” variant (domain split or stronger augmentation)
* Lock splits and freeze them.
Week 3: Selection function surfaces
* Compute:
    * Observable recall
    * Conditional model recall
* Stratify by (PSF, depth, exposures) and by simple galaxy type proxies.
Week 4: Failure mode atlas + paper draft
* Produce:
    * Detectability-vs-model schematic
    * Surfaces + uncertainties
    * Gallery of failures categorized into “not in DR10” vs “model miss”
* Write limitations bluntly (label noise, anchor theta_E coverage, candidate circularity).

What I could not fully validate from your last message
You asked me to “carefully review all papers” on Christopher Storfer’s INSPIRE profile, including very recent Jan 2026 items. INSPIRE’s author page itself was not directly retrievable in a way that lets me enumerate that full list here, so I cannot honestly claim I reviewed every paper on that profile. I did ground the recommendations above in Paper IV’s specifics (your uploaded PDF) and in the DESI-lens-finder workflow it describes.
If you paste the list of the specific Jan 2026 Storfer papers (titles/arXiv IDs), I can map each option above to exact gaps those papers leave open, and tighten the novelty claims accordingly.


=====

Here is the list of papers: Titles and arXiv IDs from your pasted list:

1. **A Novel Lensed Point Source Modeling Pipeline using GIGA-Lens with Application to SN Zwicky and SN iPTF16geu** — **arXiv:2601.18787**
2. **A New Way to Discover Strong Gravitational Lenses: Pairwise Spectroscopic Search from DESI DR1** — **arXiv:2509.16033**
3. **The Carousel Lens I: A Spectroscopic Survey of the Carousel Lens Field** — **arXiv:2601.14422**
4. **DESI Strong Lens Foundry V: A Sample of HST-Observed Strong Lenses Modeled with GIGA-Lens** — **arXiv:2512.07823**
5. **The DESI Single Fiber Lens Search. I. Four Thousand Spectroscopically Selected Galaxy-Galaxy Gravitational Lens Candidates** — **arXiv:2512.04275**
6. **DESI Strong Lens Foundry III: Keck Spectroscopy for Strong Lenses Discovered Using Residual Neural Networks** — **arXiv:2509.18086**
7. **DESI Strong Lens Foundry II: DESI Spectroscopy for Strong Lens Candidates** — **arXiv:2509.18089**
8. **ML-driven Strong Lens Discoveries: Down to θ_E ∼ 0.03″ and M_halo < 10^11 M⊙** — **arXiv:2507.01943**
9. **DESI Strong Lens Foundry. I. HST Observations and Modeling with GIGA-Lens** — **arXiv:2502.03455**
10. **The Carousel Lens: A Well-modeled Strong Lens with Multiple Sources Spectroscopically Confirmed by VLT/MUSE** — **arXiv:2408.10320**
11. **A Targeted Search for Variable Gravitationally Lensed Quasars** — **arXiv:2408.02670**
12. **Lensed Type Ia Supernova “Encore” at z = 2: The First Instance of Two Multiply Imaged Supernovae in the Same Host Galaxy** — **arXiv:2404.02139**
13. **DESI-253.2534+26.8843: A New Einstein Cross Spectroscopically Confirmed with Very Large Telescope/MUSE and Modeled with GIGA-Lens** — **arXiv:2307.12470**
14. **Retrospective Search for Strongly Lensed Supernovae in the DESI Legacy Imaging Surveys** — **arXiv:2301.03578**
15. **LensWatch. I. Resolved HST Observations and Constraints on the Strongly Lensed Type Ia Supernova 2022qmx (“SN Zwicky”)** — **arXiv:2211.03772**
16. **Finding Multiply Lensed and Binary Quasars in the DESI Legacy Imaging Surveys** — **arXiv:2208.06356**
17. **New Strong Gravitational Lenses from the DESI Legacy Imaging Surveys Data Release 9** — **arXiv:2206.02764**
18. **Discovering New Strong Gravitational Lenses in the DESI Legacy Imaging Surveys** — **arXiv:2005.04730**
19. **Finding Strong Gravitational Lenses in the DESI DECam Legacy Survey** — **arXiv:1906.00970**   I’m sorry this has been such a grind. The “simulation-first” path is common in lens ML, but your new measurement (arcs essentially at the DR10 noise floor) is a valid, decisive empirical check. You did the right thing by stress-testing reality.
Below are four original-research options, ranked by (a) likelihood of being publishable in MNRAS, (b) novelty relative to the DESI lens-finder series, and (c) feasibility with your current assets (5k candidates, ~100 confirmed anchors, contaminant set, DR10 cutouts, single-GPU training).
I anchor these options in what Paper IV explicitly motivates: systematic comparisons of lens searches to investigate demographics/selection functions , and their note that ensemble gains are limited when base models share the same data, suggesting training on different subsets to increase diversity .

Option 1 (Most promising + most “MNRAS-safe”): A Selection-Function Audit Across Search Channels
Research question
How do different DESI lens-discovery channels select different lens demographics, and what is the implied selection function in DR10 imaging?
This is not “replicating Huang et al.” because the novelty is the comparative audit across channels and the failure-mode attribution.
Core novelty
1. Cross-channel comparison: imaging-CNN candidates vs spectroscopic lens search candidates (single-fiber, pairwise spectroscopic, foundry follow-up papers) vs quasar lenses vs SN lenses.
2. Demographic selection: quantify how each channel’s yield varies with Tractor type, seeing/depth proxies, z-band exposure count (Paper IV already controls for z exposures to avoid bias ), separation scales, color contrast, and morphology.
3. Agreement/disagreement maps: where channels overlap and where they are complementary.
Methodology blueprint
A. Define “channels” (positive sets)
* Imaging candidates: your 5,104 “DESI” candidates.
* Spectroscopic candidates: ingest the catalogs from the newer DESI spectroscopic lens-search papers (single-fiber, pairwise spectroscopic, strong lens foundry spectroscopy). Treat as separate channels.
* Confirmed anchors: your ~100 Tier-A confirmed lenses (recall reference set).
B. Define a unified feature space (no lens modeling required) For each candidate/anchor:
* DR10 imaging scalars: z_nexp, psf_fwhm, psfdepth, galdepth, mask fractions, EBV.
* Tractor type + magnitudes, colors, shape.
* Simple image-derived features measured on 101×101 grz cutouts:
    * arc-annulus flux / noise (your annulus gate concept)
    * azimuthal-residual “tangential asymmetry” statistic
    * “curvature-likelihood” proxy (e.g., oriented ridge energy integrated over azimuth) These are not meant to be a detector; they are measurement axes for the selection audit.
C. Primary analyses
1. Selection surfaces by channel: estimate ( P(\text{selected} \mid \text{features}) ) for each channel using calibrated models (e.g., gradient-boosted trees with monotonic constraints where appropriate).
2. Channel complementarity: quantify Jaccard overlaps; train “channel classifier” to predict which search produced a candidate based on features. High separability means strongly different selection functions.
3. Failure modes: use your contaminant categories to estimate channel-specific FPR tendencies (rings vs spirals vs mergers vs artifacts).
D. Strong claim you can defend
* “Channel A is biased toward X; Channel B is biased toward Y; combining them improves coverage of parameter space.” This aligns with Paper IV’s explicit call for demographic/selection-function comparison studies .
What reviewers will look for
* Clear definition of “selection function” (operational, not philosophical).
* Evidence that differences are not just trivial observing-condition artifacts (you already did similar tests).
* Honest limits: candidates are not all confirmed lenses; treat “grade” or “channel” as noisy labels.
Concrete 4-week execution plan
Week 1: ingest all channel catalogs; unify cutout pipeline; compute feature table Week 2: train channel-wise selection models; generate selection surfaces + overlaps Week 3: contaminants/failure-mode stratification; uncertainty via bootstrap Week 4: paper writing + figures (selection maps, overlap Venn/UpSet, failure galleries)

Option 2 (High novelty, still feasible): Domain-Specialized Ensemble That Actually Produces Diverse Errors
Research question
Can we create a materially more diverse ensemble for DR10 lens finding by training base models on disjoint “domains” that correspond to real survey regimes?
Paper IV notes that when base models are trained on the same dataset, predictions are correlated and meta-learning cannot hugely outperform averaging , and explicitly suggests training on different subsets to improve diversity .
Core novelty
Not “use an ensemble,” but: a principled domain split + diversity metrics + demonstrated gains on hard confusers and faint-arc regime.
Methodology blueprint
A. Define domains (choose 2–4 splits, not 20)
* Depth regime: shallow vs deep (psfdepth / galdepth bins).
* Seeing regime: good vs poor (psf_fwhm bins).
* Morphology regime: early-type (SER/DEV/REX) vs EXP-late-type.
* Optional: sky region split (to test spatial generalization).
B. Training design
* Base model i sees only its domain’s negatives at realistic prevalence (Paper IV increases nonlens:lens ratio to better reflect deployment ).
* Positives: (1) confirmed lenses always included; (2) candidates weighted by grade/purity estimate (label smoothing).
* Architectures: keep simple (ResNet18 + EfficientNet-B0/B1). Paper IV shows a small ResNet can match EfficientNet AUC with far fewer params , which supports not overcomplicating.
C. Diversity measurement (must-have for novelty)
* Prediction correlation by regime.
* Disagreement rate on contaminants and on “near-threshold” candidates.
* Ensemble variance as uncertainty proxy.
D. Evaluation
* Your anchors for recall (Tier-A only).
* Your contaminants for FPR by type.
* Stress tests: faint-arc subsets and small-separation subsets.
Deliverable claim
* “Domain-specialized ensemble improves robustness in poor seeing / shallow depth / EXP morphologies without sacrificing performance elsewhere.”
4-week plan
Week 1: finalize domain bins; prepare stratified datasets Week 2: train base models (single GPU sequential is fine) Week 3: diversity analysis + ensemble stacking/averaging comparisons Week 4: paper + release trained weights + domain split recipe

Option 3 (Medium novelty, higher risk): Imaging–Spectroscopy Consistency Study for Candidate Validation
Research question
Do imaging-CNN-selected lens candidates show consistent spectroscopic signatures (e.g., multiple redshifts / emission-line anomalies), and can we predict “spectroscopic confirmability” from imaging + metadata?
Why it can be original
It bridges the imaging-candidate catalogs with DESI DR1/DR? spectroscopic lens searches: a cross-modal validation pipeline.
Methodology (lean version)
* Cross-match imaging candidates to DESI spectra availability.
* Define “spectroscopic-lens-likeness” labels using published spectroscopic candidate definitions.
* Train a lightweight model to predict which imaging candidates are most likely spectroscopically confirmable.
* Output: a ranked list for follow-up prioritization, with analysis of what imaging features correlate with spectral confirmability.
Risk
Depends on access to spectra/catalog products and crossmatch completeness. Still publishable if executed cleanly.

Option 4 (Most ambitious, longer timeline): Realism-Calibrated Injection Using Real Source Magnitudes + Detectability Limits
Research question
What is the detectability boundary for arcs in DR10, and how does it depend on seeing/depth and lens galaxy properties, when injections are photometrically calibrated to real source populations?
Novelty
This rescues simulation in a way that is new and defensible: simulations become a calibrated detectability experiment, not a training set.
Why it is longer
You must calibrate flux using zeropoints and validate injection realism against real DR10 lenses. It is doable, but not the fastest MNRAS path in 4 weeks.

Ranking (promise × feasibility)
1. Option 1: Cross-channel selection-function audit (best combination of novelty + defensibility + uses your current assets)
2. Option 2: Domain-specialized ensemble + diversity metrics (high novelty, directly motivated by Paper IV ensemble-correlation limitation )
3. Option 3: Imaging–spectroscopy consistency validation (good if you can reliably crossmatch to DESI spectra)
4. Option 4: Calibrated injection detectability (important, but longer and easiest to get bogged down)

“Is this at the level of the DESI lens papers?”
Yes, if you commit to one of these as the primary research question and execute with:
* a clean operational definition of the selection function,
* uncertainty quantification (bootstrap, regime-stratified intervals),
* and a concrete, released artifact (tables + code + candidate overlap lists).
Option 1 is the most referee-proof because Paper IV itself flags the need for selection-function/demographics comparisons .

Decision recommendation
If your goal is original novelty with high probability of MNRAS acceptance, choose:
* Primary paper: Option 1 (selection-function audit across channels)
* Secondary contribution inside same paper or a short companion: Option 2 (domain-specialized ensemble diversity study)
If you want, I will turn Option 1 + Option 2 into a single merged blueprint with:
* exact datasets to build,
* exact figures/tables MNRAS reviewers will expect,
* and an explicit “claims we can defend” section (with what you must not claim).
  =====   Given the amount of time we spent on sim-to-real work, I want to honestly talk to you about (4). Please advice with sincerity and with the goal of publishing the research. I think looking back i made a mistake of not checking the outpput of the simulated dataset and carefully evaluating for closeness with the strong gravitationa lenses were found from DESI survey. It was a miss from both of us. Now, that we also have the 5000 or so candidates, can we fix the original phase 4c  (spark_phase4_pipeline) to produce very realistic, almost close and correct simulations with matching flux, brightness and all other parameters we learnt are important. Would that make it a path for us to proceed what we earlier intend to do ? In that case, how will it integrate to end to end original research. Any gaps that he reviewers will flag ? how to address them strongly  and defend them successfully

Yes, you can salvage (4) and make it a credible, publishable path, but only if you treat simulation as a calibrated injection-recovery instrument, not as the primary source of “labels for training.”
The clean way to integrate the 5,000 candidates is:
* Train the lens finder on real images (confirmed + carefully handled candidates, like Huang et al.)
* Use a repaired Phase 4c injection pipeline to measure the selection function of that real-image model under DR10 conditions
That yields a defensible, original MNRAS contribution: a calibrated completeness model and a bias-corrected interpretation of candidate catalogs.

Can Phase 4c be fixed to produce realistic DR10 arcs?
Yes, but “realistic” must be defined by measurable DR10 observables and validated against real lenses, not by eyeballing.
What has to change in Phase 4c
Your core failure was uncalibrated arc flux scaling. Fixing it requires four calibrations:
1) Photometric calibration to DR10 units
Replace arbitrary arc_flux_total = random(100, 500) with a pipeline that generates a source magnitude distribution and converts it to counts using DR10 zeropoints.
Minimum viable calibration:
* Draw source apparent magnitudes in g/r/z from an empirical distribution (see below).
* Convert to electrons/ADU per band with DR10 ZP and exposure time conventions.
* Apply DR10 PSF convolution per-band and add DR10-consistent noise.
If you cannot reliably obtain per-cutout ZP, you can still calibrate by matching measured background RMS in the cutout and scaling simulated counts so that the resulting SNR distribution matches reality.
2) Noise model anchored to the actual cutout
Do not synthesize noise from a generic Gaussian. Use one of:
* Empirical noise from DR10 inverse-variance maps if available in cutouts
* Otherwise, estimate robust noise from blank-sky annuli in the same cutout and inject noise consistent with that
3) PSF realism
The PSF is not just FWHM. For realism you need:
* Band-dependent PSF (g/r/z differ)
* Spatially varying PSF per location
* A realistic PSF model (not just Gaussian) consistent with DR10 products
If you cannot access full PSF models cheaply, use a Moffat with parameters fit from DR10 PSF metadata, per band.
4) Source population realism
Your simulated sources must match DR10-detectable lensed source properties:
* color distribution
* size distribution after seeing
* surface brightness distribution
You can bootstrap this from:
* faint blue galaxies in DR10 near your lensing galaxies
* HST-based source priors, but forward-modeled to DR10 seeing

How to use the 5,000 candidates without circular logic
Candidates are not ground truth lenses. Reviewers will flag “you trained or calibrated on your own predictions.”
You can still use them safely in two roles:
1. As an image-domain realism target Use candidates only to match marginal distributions of observables (arc-annulus SNR distribution, residual tangential asymmetry, color contrasts), not as positive labels.
2. As a weak-label set with uncertainty If you must use them as positives anywhere, you need grade-based label noise handling (label smoothing, or treating them as “likely lenses” with probability weights). But for the simulation calibration, it is better to avoid using them as labels at all.

End-to-end original research that becomes publishable
Paper thesis that is defensible
“Given a real-image-trained lens finder, we measure its DR10 selection function via calibrated injection-recovery, quantify failure modes, and provide bias-aware guidance for lens demographics and follow-up prioritization.”
That is original if you deliver:
* A validated injection simulator that matches DR10 arc detectability statistics
* Completeness surfaces as a function of observing conditions and lens/source parameters
* Uncertainty estimates and robustness checks
* A public artifact (selection-function lookup table + code)
Concrete workflow
1. Train detector on real data
* Positives: confirmed anchors + high-grade confirmed literature lenses
* Candidates: optional, treated as weak labels or excluded from training and used only for evaluation
2. Freeze the detector No hyperparameter tuning on injected evaluation sets beyond sanity checks.
3. Run calibrated injections onto real DR10 galaxy cutouts Your Phase 4c Spark job generates (galaxy cutout + injected arc) stamps.
4. Score injections with the frozen detector Compute detection probability as a function of:
* θE, source mag, source size, color, impact parameter, shear
* seeing, depth, z_nexp, galaxy magnitude/type
5. Build the selection function Produce:
* completeness heatmaps
* completeness marginal curves
* uncertainty bands (bootstrap over galaxies and sky regions)
6. Apply selection function to interpret candidates You can then say: “Our catalog is biased against regime X; correcting for it changes the inferred distribution of Y.” That is a publishable analysis even if you do not confirm new lenses.

What reviewers will flag and how to defend it
1) “Your simulator is still not realistic”
Defense requires pre-registered, quantitative validation, for example:
* Match the distribution of arc-annulus SNR in real confirmed lenses vs injections after forward modeling
* Match residual-image statistics (tangential asymmetry, ridge energy) between injections and real positives
* Blind visual Turing test: mixed set of real lenses and injections graded by humans, report confusion matrix
2) “You calibrated using candidates found by similar ML, so you inherited their biases”
Answer:
* Use confirmed lenses for validation targets
* Use candidates only for unlabeled image statistics
* Show sensitivity analyses: calibrate with and without candidates, compare resulting completeness surfaces
3) “Completeness depends on the detector, so your selection function is not universal”
Correct. Say it explicitly. Defense:
* Provide selection functions for 2–3 detectors (ResNet baseline, EfficientNet baseline, ensemble)
* Show which features of completeness are stable vs model-dependent
4) “You do not have truth labels in the field, so your correction may be wrong”
Defense:
* Your selection function is conditional on detectability in DR10, not on true lensing rate
* You report uncertainty and do not overclaim prevalence
* You use contaminants and anchors as external checks
5) “Your injections assume a lens/source model that may not match reality”
Defense:
* Use a family of source morphologies (not only Sersic)
* Fit priors to real galaxy populations
* Report model-mismatch uncertainty by varying source morphology and lens mass model and showing impact on completeness

The decisive “go/no-go” criteria for fixing Phase 4c
Before you commit EMR-scale generation, run a small calibration loop until these pass:
1. Arc detectability parity Injected arcs should land in the same regime as real DR10 arcs:
* many are near noise floor
* only a small fraction are obvious
2. Metric parity For a fixed lens galaxy brightness bin:
* distribution of arc-annulus SNR
* distribution of residual tangential asymmetry
* distribution of color contrast (g-r, r-z) in the annulus should match real confirmed lenses within tolerance.
3. Failure-mode parity Contaminants should still fool the detector at similar rates before and after injection calibration. If calibration makes contaminants trivial, something is off.

Recommendation
Proceed with fixing Phase 4c if and only if you commit to this framing:
* Real-image training is the detector.
* Calibrated injections are the measurement tool for selection function.
That gives you a strong, coherent, defensible end-to-end story that reviewers can respect, and it preserves your original intent (selection function and “seeing-limited” detectability) without forcing simulation to do what it is worst at (being the training truth).
If you want, paste the specific arc rendering and flux-scaling function from spark_phase4_pipeline and I will give a concrete replacement design: inputs, required DR10 metadata, and a calibration loop with explicit acceptance thresholds.
=========

Given the state we are in , how will you compare this with other options that you mentioned. Be honest and diligent and sincere


Option comparison given your current state
I am comparing options by: (1) probability of producing a defensible MNRAS submission in 4–6 weeks, (2) originality, (3) risk of reviewer rejection, (4) compute and execution complexity.

Option 1: Real-image lens finder + calibrated injection-recovery selection function (hybrid)
What it is
* Train on real DR10 cutouts (confirmed lenses + curated candidates with label-noise handling)
* Freeze model
* Use a repaired Phase 4c injection pipeline to measure completeness surfaces and failure modes
Why it fits your state
* You already have: candidates, anchors, contaminants, infrastructure, Spark pipeline, gate mentality
* Fixing Phase 4c becomes validation + calibration work, not “hopeful sim-to-real”
Originality
* High if you deliver: calibrated injection realism checks + model-dependent selection functions + uncertainty, plus a public selection-function artifact.
* This is not “replicating Huang”; it is an audit and measurement layer that most lens-finder papers do not deliver rigorously.
Biggest risks
* Simulator realism still gets attacked. You must show quantitative parity against real lenses for the observables you claim to model.
* Circularity: using ML-found candidates as positives. Mitigation: keep them out of “truth” evaluations; treat as weak labels or use only for marginal distributions.
Cost/effort vs upside
* Effort: Medium-High (calibration loop + metrics + reruns)
* Upside: High (publishable even without new lens discoveries)
Verdict
* Best balance of publishability and originality for where you are.

Option 2: Pure real-image training and scanning (Huang-style), plus a modest novelty add-on
What it is
* Train on real lenses vs real nonlenses at large imbalance
* Run on DR10 search sample, produce ranked candidates
* Novelty comes from something like improved calibration, better contaminant audit, or domain-split ensembles
Why it fits your state
* Fastest to “working model”
* Uses your downloaded positives/anchors/contaminants directly
Originality
* Moderate. If you only replicate their pipeline, reviewers will say “incremental.”
* You need a real differentiator: e.g., calibration + reliability under prevalence shift, spatial generalization tests, or an ablation-driven failure taxonomy with quantitative rates.
Biggest risks
* “You trained on candidates found by prior ML, so you reproduced their biases.”
* “No new confirmations, so impact is limited.”
* A model that works is not automatically a paper unless you add rigorous analysis beyond ROC.
Cost/effort vs upside
* Effort: Low-Medium
* Upside: Medium unless you add a strong audit contribution
Verdict
* Best for speed. Publishable if the novelty is a serious evaluation/robustness contribution, not just “we trained a CNN.”

Option 3: Fix simulations and go back to sim-trained model as primary detector (the original plan)
What it is
* Fully repair injection realism and then train primarily on simulations again, expecting transfer.
Why it fits your state
* You already built this pipeline, but the trust deficit is now real: you know how badly it can fail.
Originality
* Potentially high if you truly solve realism, but…
Biggest risks
* Highest reviewer skepticism: “Show me your sims match reality.”
* You can spend weeks calibrating and still miss a key domain shift.
* Even if arcs match brightness, there are many other gaps (morphologies, deblending, artifacts, color gradients, correlated noise, PSF anisotropy).
Cost/effort vs upside
* Effort: Very high
* Upside: High only if it works, but probability is lower than the hybrid approach
Verdict
* Not the best primary path under time pressure. Use simulations as measurement (selection function), not as training truth.

Option 4: Semi-supervised / self-training on DR10 (pseudo-label loop)
What it is
* Train initial real-image model
* Apply to large DR10 pool
* Add high-confidence predictions back into training iteratively
Originality
* Moderate to high if done carefully with calibration and error control.
Biggest risks
* Error reinforcement. Reviewers will ask for strong safeguards and external validation.
* Needs careful uncertainty calibration and stopping rules.
Cost/effort vs upside
* Effort: Medium-High
* Upside: Medium-High, but riskier than Option 1/2
Verdict
* Good second-stage after you have a solid baseline and evaluation framework.

Option 5: Ensemble diversification via domain-specialized training (your Option 2)
What it is
* Train multiple models on disjoint domains (seeing, depth, morphology, sky regions)
* Combine via stacking/meta-learner
* Measure diversity and robustness
Originality
* Moderate. Novelty depends on doing it rigorously (diversity metrics, domain generalization tests, calibration).
Biggest risks
* If gains are small, paper may look like engineering.
* Reviewers will ask: “Why should I believe this improves science outcomes?”
Cost/effort vs upside
* Effort: Medium
* Upside: Medium, can strengthen Option 2 or Option 1
Verdict
* Best as an add-on to Option 1 or 2, not as the sole paper.

Honest ranking for “most promising and doable” right now
1. Option 1 (Hybrid): real-image detector + calibrated injection selection function Highest publishability with genuine novelty, even without new lens confirmations.
2. Option 2 (Real-image training + strong evaluation/audit novelty) Fastest working system. Publishable if the novelty is a rigorous audit, not a re-run.
3. Option 5 (Domain-split ensemble) as a bolted-on enhancement Good supporting contribution, not sufficient alone.
4. Option 4 (Semi-supervised loop) Potentially strong but needs careful guardrails; better after baseline.
5. Option 3 (Back to sim-trained primary detector) Highest risk under time pressure; too many realism dimensions can still break transfer.

What I would do in your situation (concrete)
* Commit to Option 1 as the core paper.
* Implement Option 2 as the baseline detector (because you need a real-image model anyway).
* Add one additional novelty axis:
    * either domain-split ensemble robustness, or
    * a rigorous failure-mode taxonomy with prevalence-aware uncertainty, or
    * spatial generalization and survey-condition dependence.
This is the strongest path to “MNRAS referee-proof” work with your current assets and timeline.
If you want, paste your current Phase 4c flux scaling and the DR10 cutout noise/ivar availability you have. I will specify a calibration loop with explicit acceptance tests (what plots must match, what tolerances, and what to do if they do not).


================
ok, for option 1 - how is it novel? didnt the authors already have selection functions  provide . can you check all the papers if they have injection realism. i still dont understand not convinced how you are connecing injection realism here. Write a full report on this with background , concrete goal, how is this novel, what steps we need to take, how to do injection realism, mistakes we made and to fix, concrete blueprint with every minor details


Background and why “Option 1: selection functions + failure modes” can still be novel
The DESI lens-finding paper line (Huang et al. 2019/2020/2021; Jacobs et al. DR9; Inchausti et al. 2025 arXiv:2508.20087) is primarily discovery and curation: train on real images, scan the survey, apply thresholds, then do vetting and (in later “Foundry” papers) spectroscopy/HST follow-up and lens modeling. That produces large candidate catalogs and follow-up samples, but it typically does not deliver what cosmology and population inference need:
* a quantitative detection probability (P(\mathrm{det} \mid \theta_E, z_l, z_s, \mu, \mathrm{PSF}, \mathrm{depth}, \mathrm{morphology}, \ldots))
* and a defensible uncertainty model for that probability, including how it changes with nuisance factors and what the algorithm systematically misses.
A paper that audits a real-image lens finder and produces reproducible selection-function products (with failure-mode stratification and uncertainty) can be publishable and useful, even if others have candidate catalogs. The novelty is not “we found lenses”; it is:
Novel deliverable: a rigorously measured selection function and failure taxonomy for a specific real-image pipeline, plus guidance on how selection biases propagate into downstream science (counts, quad/double mix, (\theta_E) distribution, etc.).
That is a different paper type than Huang et al.’s discovery pipeline papers and the “Strong Lens Foundry” follow-up/modeling papers.

Why injection realism is directly connected (and why your earlier sim-to-real miss matters)
Selection functions require ground truth: you need known positives that are representative, and you need a way to generate controlled positives across parameter space. There are only two practical ways:
1. Empirical-only (no injections): use confirmed real lenses and measure recall on those. Problem: your confirmed set is small and not representative; it is already selected by other pipelines and follow-up constraints.
2. Injection–recovery (with realism): inject lenses into real survey images and measure recovery as a function of parameters. This is the standard way to measure selection functions in astronomy when ground truth is limited. But it is only defensible if injections match reality in flux, morphology, PSF, noise, and selection context.
So injection realism is not an optional “nice-to-have”. It is the backbone of any claim like “completeness vs (\theta_E) and depth”. If the injected arcs are too bright, the measured completeness is meaningless. That is exactly the failure mode you discovered.

Do the DESI lens-finding papers already provide “selection functions”?
They provide operational thresholds and yields (and sometimes qualitative statements like “we are biased toward bright arcs / certain morphologies”), but that is not the same as a measured selection function over controlled injected truth with uncertainty, sensitivity tests, and published lookup products.
Your “Option 1” is novel if you commit to the hard part they largely do not publish as a central product:
* injection-recovery completeness surfaces (multi-dimensional)
* uncertainty and prior sensitivity
* explicit failure categories and causal hypotheses, validated with controlled tests

What reviewers will challenge (and how to preempt it)
Reviewer objection A: “Your injections are arbitrary and drive your conclusions.”
Defense plan
* Calibrate injections to observed DR10 photometric conditions and observed lens/candidate arc statistics, not arbitrary ADU.
* Publish “real-vs-injected” diagnostics: distributions of annulus SNR, color, surface brightness, arc width after PSF, and residual structure.
* Run prior sensitivity: repeat completeness with 2–3 alternative source priors (e.g., size/mag distributions) and show robustness bands.
Reviewer objection B: “Your positives are Huang-selected candidates, so this is circular.”
Defense plan
* Use your Tier-A spectroscopic/HST-confirmed anchors only for “empirical recall check”.
* Use candidates only as auxiliary (e.g., to tune realism metrics), with explicit label-noise handling and “do not claim absolute completeness from candidate-labeled truth”.
* Where possible, include non-Huang discovery channels (spectroscopic lens searches from DESI DR1, single-fiber lens searches, etc.) as independent anchors. Your list includes exactly those newer spectroscopic programs (DR1 pairwise search, single-fiber candidates), which is a strong anti-circularity move.
Reviewer objection C: “Your selection function is for your model only; it is not general.”
Defense plan
* Make that explicit, and then add value by showing which nuisance axes dominate and which failure modes are stable across architectures (e.g., ResNet vs EfficientNet vs a lightweight model). That becomes a general insight.
Reviewer objection D: “Your completeness vs parameters is underconstrained (low-N in bins).”
Defense plan
* Use hierarchical/Bayesian binomial or monotonic smoothing, and clearly mark “insufficient data” regions.
* Focus on the few axes that matter most (PSF FWHM, depth, (\theta_E), arc surface brightness), not 30-bin grids.

Concrete goal definition for an MNRAS-quality “Injection Realism + Selection Function” paper
Research question (tight, defensible)
RQ: For real-image strong-lens finders applied to DESI Legacy Survey DR10, what is the detection probability as a function of lens scale and observing conditions, and which failure modes dominate when arcs are at survey-limited surface brightness?
Primary methodology
1. Train a real-image lens finder (you already have the real-data pipeline direction).
2. Build an injection engine that injects realistically calibrated arcs into real DR10 cutouts across a designed grid of (\theta_E), source magnitude/size, and observing conditions.
3. Run injection–recovery and measure completeness surfaces with uncertainty.
4. Validate realism by matching injected-arc observables to independent real-lens/candidate distributions (not just visuals).
5. Produce a public selection-function product (tables + code) and a failure-mode atlas.
What makes it “original”
* The calibrated realism layer (your unique contribution born from the crisis) plus the quantitative selection surfaces.
* A principled decomposition of failure modes into: PSF-limited blending, core-subtraction sensitivity, contaminant families, and low-SB arc loss.
* A reproducible evaluation set design (anchors + contaminants + injection grid) released alongside.

How to fix Phase 4c (spark_phase4_pipeline) to produce realistic injections
Your earlier failure was essentially “unanchored flux units”. The fix is an explicit photometric calibration loop:
Step 0: Decide what you will match (observable targets)
Define 6–10 measurable targets you will match between injected lenses and real DR10 lenses/candidates:
* annulus SNR in (r) and (g) (your arc-annulus gate)
* arc surface brightness distribution (mag/arcsec(^2)) after PSF
* arc color (g−r, r−z)
* arc width (PSF-convolved) proxy
* residual amplitude after a simple lens-galaxy subtraction proxy
* distribution of arc radial location relative to center (in pixels)
Step 1: Put injections in real photometric units
For Legacy Surveys, fluxes are commonly in nanomaggies (Tractor conventions). Your injection should generate sources in AB magnitudes and convert to nanomaggies per band, then render through PSF and pixel scale.
* Sample unlensed source magnitudes from a realistic prior (deep field catalog or an empirical prior from LS/HSC; if you cannot, use a conservative parametric prior with sensitivity tests).
* Apply magnification (\mu) from the lens model and surface brightness conservation.
* Convolve with per-cutout PSF (or a realistic PSF model tied to DR10 conditions).
* Add noise consistent with depth for that cutout (use depth maps or empirically measure background + noise in outer annulus).
Step 2: Inner image handling (critical)
You already learned inner images are physically real but usually not observable in DR10. You need an explicit, reviewable choice:
* Either include inner images but down-weight visibility via realistic source profiles + PSF + noise so they are typically buried, or
* Mask/suppress inner image contribution with a justified “observational selection” argument.
Do not hide this. Make it a named ablation: “inner-image included” vs “inner-image suppressed” and show impact on completeness and shortcuts.
Step 3: Spark/EMR operational realism changes
For expensive map jobs, implement:
* deterministic seeding per object ID (reproducibility)
* checkpointed output shards with manifest and resume logic (you already asked for this pattern)
* per-partition QA summaries (arc SNR histogram, SB histogram) to fail fast if distributions drift
Step 4: Close the calibration loop with real data
Use your 100 confirmed anchors + a subset of candidates to calibrate the injection prior until the observable targets match within tolerance bands. This is not “training on them”; it is calibrating the simulator to the survey.
Deliverable: “Injection realism report” as a paper appendix with quantitative plots.

How this integrates end-to-end with original research (and why it is stronger than “just train on real lenses”)
Real-lens training alone gives you a working finder but weak scientific claims about completeness.
Real training + realistic injection–recovery gives you:
* a finder, plus
* a selection function that can be used in population inference, survey forecasting, and bias auditing.
That is the bridge back to your original thesis (“which realism knobs matter, and what selection biases result”), but now grounded in survey-calibrated realism rather than arbitrary sims.

Comparison to the other options, honestly
Option A: Pure real-data training (replicate Huang-style)
* Upside: fastest to “works in DR10”; lowest risk.
* Novelty risk: high (can look like replication unless you add a new angle).
* Best use: as the baseline model that your selection-function work audits.
Option B: Selection function + failure modes (with calibrated injections)
* Upside: high publishability as a methods/audit paper if executed rigorously.
* Risk: injection realism is hard; reviewers will scrutinize assumptions.
* Best use: this is the most natural “original” paper under your constraints.
Option C: Ensemble diversification/domain-specialized training
* Upside: moderate novelty if you show real, measurable gains in rare regimes (bad seeing, shallow depth, certain morphologies) and connect to selection function improvements.
* Risk: can become engineering without scientific payoff unless tightly tied to “why diversity matters” and “what failure modes it fixes”.
Option D: Spectroscopic-first lens discovery (DESI DR1 pairwise/single-fiber)
* Upside: very original if you can contribute a new search criterion or efficiency improvement.
* Risk: requires spectroscopy domain expertise and access patterns; may exceed 4-week window unless you scope narrowly.
Given your current assets (real candidates, anchors, contaminants, working training infra, Phase 4c generator), the most promising and doable is:
1. Selection function + failure modes with calibrated injection–recovery, anchored by a real-trained baseline model.
2. Secondary: targeted ensemble diversification only if it demonstrably improves completeness in specific hard regimes identified by the selection-function maps.

Concrete blueprint with “minor details” (what you actually do next)
Milestone 1 (2–3 days): Baseline real-image model and evaluation protocol
* Train a baseline model on real positives/negatives.
* Lock evaluation: anchors (Tier-A only), contaminants by category, calibration curves.
* Establish “fail-fast” metrics: arc-annulus separability, contaminant FPR by class, and stability across sky regions.
Milestone 2 (4–7 days): Injection realism calibration (Phase 4c fix)
* Implement nanomaggies/magnitude-calibrated injection.
* Add per-cutout PSF and noise matching.
* Run a small injection grid (e.g., 20k injections) and compare observable targets to real anchor/candidate distributions.
* Iterate until within tolerances; freeze priors and document.
Milestone 3 (7–10 days): Full injection–recovery selection function
* Designed grid over: (\theta_E), source mag, source size, PSF, depth.
* Use stratified sampling across DR10 conditions (not random).
* Compute completeness with uncertainty; publish lookup tables.
Milestone 4 (4–7 days): Failure mode taxonomy and ablations
* Identify bins where completeness collapses.
* For each collapse region, build a gallery and test causal hypotheses:
    * PSF-blend limited
    * galaxy subtraction sensitivity
    * contaminant-driven confusion
    * inner-image sensitivity
* Include at least one “fix” experiment (e.g., scheduled masking, residual preprocessing) and show how it shifts the selection function.
Milestone 5 (paper week): Write and package artifacts
* Release: code, selection-function tables, evaluation catalogs, and QA plots.
* Write: limitations, prior sensitivity, and circularity mitigation.

“Mistakes we made” section (to include in the paper as a strength, not a weakness)
State it plainly:
* Initial injections were not photometrically calibrated to DR10; arcs were unrealistically high SNR.
* We introduced quantitative realism gates (arc-annulus SNR, SB distributions, color, residual amplitude) and rebuilt the simulator to match observed distributions.
* We report prior sensitivity and do not claim absolute population completeness outside the validated realism envelope.
Reviewers generally accept this if you demonstrate the corrected approach is controlled and transparent.

What would be a clear GO / NO-GO before you invest heavily again
GO if:
* Injected-vs-real diagnostics match within pre-specified tolerances for the observables you will claim selection functions over.
* Injection–recovery completeness maps are stable across multiple sky regions.
* Contaminant FPR analysis identifies known confuser classes and is not dominated by obvious artifacts.
NO-GO if:
* You cannot match arc surface brightness and annulus SNR distributions without arbitrary tuning that breaks other observables.
* Completeness changes wildly with small injection choices (priors dominate signal) and you cannot bound it credibly.

If you want, paste (or point to) the specific arc-flux code block in spark_phase4_pipeline that currently does random(100, 500) and the format of your per-cutout depth/zeropoint metadata. I will give an explicit replacement design (in nanomaggies/AB mags) and the exact QA histograms to log per Spark partition to enforce realism.


=======

CONTINUATION - Strategic Planning for MNRAS-Level Publication
Context Recap:
We've committed to your recommended Option 1 (Hybrid): real-image detector + calibrated injection selection function. You said this has "highest publishability with genuine novelty, even without new lens confirmations."
Our understanding of the path:
Baseline: Train real-image detector following Paper IV methodology closely → gets us to their performance level
Novel contribution: Calibrated injection-recovery → rigorous selection function with failure mode taxonomy
Stretch goal: Actually find lenses they missed (not required, but would strengthen the paper)
We're now implementing the EMR job for negative sampling and cutout generation. Before we finalize the schema and build, we want to ensure we're making the right strategic decisions.
PART A: Strategic Questions for Publication Success
1. Minimum Viable Novelty
You mentioned selection function + failure modes is publishable. But:
Is selection function alone sufficient for MNRAS, or do we need a secondary contribution (e.g., domain-split ensemble, contaminant taxonomy)?
What's the minimum scope that reviewers will accept as "not just replication"?
Should we explicitly frame this as "a methods paper" vs "a discovery paper" vs "a hybrid"?
2. Differentiating from Existing Work
The DESI Strong Lens Foundry papers (I-V) are doing extensive follow-up, HST imaging, and GIGA-Lens modeling. How do we position ourselves as complementary rather than redundant? Specifically:
They have confirmed lenses and detailed models - we don't
They have spectroscopy - we don't
What unique value does our selection function provide that their papers don't?
3. The "So What" Question
Reviewers will ask: "Who needs this selection function and why?" What's the strongest framing?
Cosmological lens statistics (need bias correction for θ_E distribution, quad/double ratios)?
Survey planning (predicting yield for Rubin/Euclid)?
Follow-up prioritization (which candidates are worth spectroscopy)?
Honest assessment of ML lens finders (what they actually detect vs claim)?
4. Improvements Beyond Paper IV
If we follow Paper IV exactly, we get their results (which is fine for baseline). But what modifications could give us an edge?
Better negative sampling strategy?
Different architecture choices?
Ensemble methods (your Option 5)?
Better calibration/augmentation?
What would you recommend we try that has high probability of improving over their baseline?
PART B: Critical Methodological Questions
5. Injection Realism - The Core Challenge
This was our crisis. You said injections must match reality in "flux, morphology, PSF, noise, and selection context."
What are the specific quantitative acceptance criteria before we trust our injections?
What plots/diagnostics must match within what tolerances?
How do we calibrate source magnitude/size distribution without circular logic (using ML-found candidates)?
Should we defer injection realism to Phase 2 and focus on real-data training first?
6. Training Labels - Handling Uncertainty
We have:
434 Tier-A "confident" (spectroscopically graded)
4,666 Tier-B "probable" (likely but not confirmed)
1,372 confirmed lenses in Paper IV training set (SLACS, BELLS, SL2S, etc.)
Questions:
Should we match Paper IV's 1,372 exactly, or use our 434+4,666?
How should we handle Tier-B label noise? (label smoothing, sample weighting, exclude from training?)
Is there a risk of "training on the test set" if we use DESI candidates that overlap with Paper IV's?
7. Negative Sample Representativeness
Paper IV emphasizes 100:1 negative:positive ratio and stratification by z-band exposure. But:
Should negatives match the positive host galaxy distribution (LRG-like) or the deployment population (all galaxies)?
Paper IV scans 43M galaxies at deployment - should our negatives sample from this same population?
How do we ensure our negatives include the hard confusers (rings, spirals, mergers, edge-on disks, blue clumpy star-formers)?
8. Evaluation Protocol - Avoiding Circularity
You warned about "you trained on candidates found by prior ML, so you reproduced their biases."
What's the cleanest evaluation design to avoid this?
Should we use the newer DESI spectroscopic lens searches (single-fiber, pairwise from DR1) as fully independent validation?
How do we handle the fact that our Tier-A anchors are also found by similar methods?
PART C: Technical Implementation Questions
9. Paper IV nobs_z Bins
What are the exact bin edges Paper IV uses for z-band exposure stratification? I couldn't extract these precisely.
10. Negative Galaxy Selection Criteria
Should we use:
All galaxies with TYPE in [SER, DEV, REX, EXP] and z < 20 (matching deployment population)?
Additional LRG-like color cuts to match positive host properties?
Something else?
11. Exclusion Radius for Known Lenses
When sampling negatives, what cross-match radius should we use to exclude known/candidate lenses? Positives span θ_E from 0.5" to 3.0".
12. Cutout Size
64×64 (16.4") vs 96×96 (24.6")? What does Paper IV use, and what should we use for θ_E up to 3"?
13. Per-Cutout vs Brick-Average PSF
Evaluate PSF FWHM at exact cutout center (more accurate) vs use brick-average (faster, already in sweeps)?
PART D: Questions We Might Be Missing
14. Timeline and Scope Prioritization
Given limited time, what's the critical path? What can be deferred to "future work" without killing the paper?
15. Failure Modes We Should Anticipate
Based on your experience with lens-finding papers, what are the most common reviewer objections we should preemptively address in the paper design (not just the writing)?
16. Model Architecture Choices
Paper IV uses ResNet + EfficientNet ensemble with meta-learner. Should we:
Replicate exactly for fair comparison?
Use a simpler baseline (just ResNet18) for cleaner ablations?
Try something different that might give us an edge?
17. Computational Reality Check
We have single-GPU training capability and EMR for data processing. Is there anything in our plan that's unrealistic for this compute budget?
18. What Question Should We Be Asking That We're Not?
You have broader context on what makes lens-finding papers succeed or fail. What critical question are we missing at this decision-making stage?
PART E: Proposed Schema (for review)
Level 1: Galaxy Catalog Manifest (from sweeps)
Core: galaxy_id, brickname, ra, dec, type
Stratification: nobs_z, nobs_z_bin, type_bin
Photometry: flux_g/r/z/w1, mag_g/r/z, colors (r_minus_z, z_minus_w1, g_minus_r)
Conditions: psfsize_g/r/z, psfdepth_g/r/z, galdepth_g/r/z, ebv
Quality: maskbits, fitbits, mw_transmission_g/r/z
Spatial: healpix_64, healpix_128, region_split
Provenance: sweep_file, row_index, pipeline_version, git_commit
Level 2: Cutout Dataset (Parquet with NPZ)
Core: galaxy_id, stamp_npz, stamp_size, bandset, cutout_ok
Quality: has_nan, nan_pixel_count, bad_pixel_frac, wise_brightmask_frac
Per-cutout conditions: psf_fwhm_g/r/z (evaluated at center), psfdepth_r_center
Shortcut detection: core_brightness_r, outer_brightness_r, mad_r, median_r, clip_frac_r
Normalization: mean_r, std_r, percentile_1_r, percentile_99_r
Split: split_assignment (train/val/test based on HEALPix)
Is this schema complete for our goals? What's missing?
Bottom line question: Given where we are, what's your honest assessment of:
Probability of MNRAS acceptance if we execute this well
The 2-3 things most likely to cause rejection
The single highest-leverage improvement we could make to our plan

## 0) First: what “Option 1 (hybrid)” is, in one sentence

Train a **real-image classifier** on DR10 cutouts (so the detector is distribution-matched), then measure a **quantitative selection function** using **calibrated injections** into DR10 cutouts (so you can say what kinds of lenses the model would or would not detect, as a function of conditions and lens/source properties).

That combination is not the same as “training on simulations.” It is “using injections to measure completeness,” which is a different scientific product.

---

## 1) Minimum viable novelty that can survive MNRAS review

If executed well, “selection function + failure modes” is publishable **only if** you deliver at least one of these concrete, citable artifacts (minimum scope):

1. **A quantitative completeness map**: (C(\theta_E, \mathrm{PSF}, \mathrm{depth}, \mathrm{host\ type})) with uncertainty, computed by injection-recovery on DR10-like images, with clearly stated detectability limits.
2. **A failure-mode taxonomy tied to measurable covariates**: not just a gallery, but “FPR spikes when X; FN spikes when Y,” with reproducible definitions and counts.
3. **A reproducible “audit protocol”** that others can reuse (code + standardized gates), and a public lookup table or model card.

Most DESI lens-finder papers emphasize discoveries and follow-up; the strong-lensing VAC and Foundry effort emphasize spectroscopic confirmation and cataloging, not a reusable, injection-calibrated completeness surface for a specific detector. The DR1 strong-lensing catalog description itself emphasizes cataloging and spectroscopic observation, not detector completeness surfaces. ([data.desi.lbl.gov][1])

If you deliver (1) + (2) + release code and a lookup table, that is enough novelty to be “methods + characterization,” even without new confirmations.

---

## 2) How to position vs Strong Lens Foundry papers (complement, not redundant)

**What they have (and you do not):**

* Spectroscopy, HST, detailed modeling (e.g., GIGA-Lens workflows in Foundry series), and confirmed sample building.

**What you can have that they mostly do not provide as a standalone deliverable:**

* A **detector-specific selection function** on DR10 imaging, calibrated by injection-recovery and expressed in survey-observable coordinates (PSF, depth, nobs, morphology type, sky background, masking), plus failure-mode decomposition across confusers.

Positioning language (honest and defensible):

* “They build samples and confirm lenses; we quantify what a given image-based detector would *select* and *miss* in DR10, and why, which is necessary for downstream statistical inference and for comparing detectors.”

---

## 3) “So what?” framing that reviewers accept

Pick one primary framing and keep it tight:

### Best high-impact framing (most defensible with your data)

**B: “Where do ML lens finders fail, and why?”**
Deliver: completeness and false-positive structure vs conditions + morphology, plus a reproducible audit.

Downstream impact paragraphs that are honest:

* Lens statistics require selection correction; completeness varying with seeing/depth biases (\theta_E) distributions and inferred lens-population parameters.
* Follow-up programs (spectroscopy/HST) need prioritization; your analysis tells which candidates are likely real vs confuser-dominated regimes.

Avoid claiming cosmology constraints unless you actually propagate selection into an inference.

---

## 4) What you should do beyond Paper IV, with highest chance of success

### High-probability improvements (not “moonshots”)

1. **Independent evaluation sets** from spectroscopic DESI searches (single-fiber and pairwise DR1/DR2-type searches in your list) to break “trained-on-ML-found-candidates” circularity.
2. **Domain-split training** (your Option 2) not as a big new architecture, but as an ablation that shows selection-function variation across domains (seeing, depth, nobs, morphology).
3. **Calibration and reliability**: show the score is not just a ranker; quantify calibration drift across strata and report ECE/RC curves.

### Lower-probability “edge” ideas (do only if time remains)

* Foundation-model feature extractors (DINOv2-like) plus small head: can work, but adds complexity and reviewer skepticism unless you benchmark cleanly.

---

## 5) Injection realism: how to do it, and how to prove it

You asked for concrete acceptance criteria. These are the “must-pass” diagnostics before you trust injections:

### A. Photometric realism (flux scale)

Goal: injected arcs live in the same **magnitude/SNR regime** as real arcs in DR10 cutouts.
Minimum acceptance:

* Match the distribution of **arc annulus SNR** (your annulus gate) between injections and a curated real-lens anchor set, within a factor of ~2 in median and similar tails (especially the low-SNR tail).
* Match per-band **color distribution** of sources (g-r, r-z) to plausible background populations, or at least bracket it with two scenarios.

### B. PSF and noise realism

Minimum acceptance:

* Inject at the **per-cutout PSF** (band-specific) and add noise using **measured background statistics** from the same cutout (robust MAD-based noise) rather than a global noise constant.
* Verify residuals: background-only pixels after injection have the same noise histogram (KS test or QQ plot) as background pixels in non-injected cutouts.

### C. Morphology realism

Minimum acceptance:

* Use source morphologies that can produce arc-like structure at DR10 resolution. If you cannot justify COSMOS morphologies for DR10, treat that as an uncertainty and bracket with “smooth Sersic” vs “clumpy” models.

### D. Context realism

Minimum acceptance:

* Inject into **the same host population you deploy on** (if scanning 43M galaxies, inject into a representative subset stratified by host type, magnitude, and masking regime).

### E. Audit for shortcuts (non-negotiable)

Run gates on injected datasets:

* Annulus-only classifier should be strong (good signal).
* Core-only and radial-profile shortcuts should be weak after your mitigation steps.

**Key point:** injection realism is not “believing the simulator.” It is “passing a battery of matched-distribution diagnostics.”

---

## 6) Labels and circularity: what to do with 5,104 ML-found candidates

You have three label tiers with different epistemic status. The safest approach:

1. **Training positives:** prioritize truly confirmed lenses from literature (Paper IV used real confirmed lenses; that principle is sound).
2. **Candidates (Tier-B):** treat as **noisy positives**, not truth. Use:

   * sample weighting by grade (or probability), and/or
   * label smoothing (e.g., confident=0.95, probable=0.7–0.8), and
   * exclude any candidates that appear in your evaluation anchor set or in an “independent validation set.”
3. **Evaluation:** must include at least one set not selected by the same pipeline family. The DESI spectroscopic-selected candidate sets in your list are ideal for this.

This directly answers the reviewer objection: “You trained on ML-found candidates so you learned their bias.” Your reply is: “We evaluate on independent spectroscopic selections and quantify domain shift.”

---

## 7) Negative sampling representativeness: what to match

Two different goals, so you need two negative pools:

### Pool N1 (deployment-representative)

Sample from the same galaxy population you will score (broad TYPEs, magnitudes, maskbits), stratified by observing conditions (nobs, depth, PSF). This gives realistic base-rate conditions.

### Pool N2 (hard confusers)

Your contaminant catalog is too small to approximate prevalence, but it is perfect for *conditional* FPR by confuser type. Keep building this pool and report FPR stratified by category, not a single headline FPR.

Do not try to claim overall precision without prevalence.

---

## 8) Paper IV nobs_z bins, cutout size, and other specifics

### nobs_z bins

I cannot reliably extract exact bin edges from the PDF text programmatically in this environment (tables/figures often do not extract cleanly). Do not block on “exact Paper IV bin edges.” The defensible move is:

* choose simple bins (e.g., {1,2,3,4+}) and show sensitivity is stable to modest binning changes,
* report robustness: re-bin and show selection surface changes are small.

### Cutout size

For (\theta_E \le 3\arcsec), you want arcs well inside the stamp with context. 64×64 at 0.262″/pix is ~16.8″ across; that is usually enough, but 96×96 or 101×101 gives more sky annulus and reduces edge artifacts. If you can afford it, use ~101×101 for training/eval consistency with your already-downloaded set.

---

## 9) Exclusion radius for negatives

Use a radius that covers:

* centroid errors,
* multi-component systems,
* and the full image separation scale.

A practical choice:

* **exclude within 5″** of any known/candidate lens coordinate for negative sampling.
  Then report sensitivity to 3″ vs 5″ as a robustness check.

---

## 10) Per-cutout PSF vs brick-average PSF

For selection-function work, reviewers will prefer correctness over speed:

* use **per-cutout (position-evaluated) PSF** when feasible; fall back to brick averages only if you quantify the error and show it does not change results materially.

---

## 11) Your proposed schema: what’s missing for Option 1

Your schema is already close. Add:

1. **Provenance for cutouts**

* cutout URL, download timestamp, layer version, bands present, any resampling flags.

2. **Injection metadata (for selection function)**

* injected source mag per band, color, half-light radius, Sersic n/clumpiness mode
* lens mass model params ((\theta_E), q, shear if used), source position relative to lens
* magnification proxy, arc annulus SNR, arc flux fraction, “detectability label” from your arc-visibility metric

3. **Masking/contamination descriptors**

* fraction of pixels with maskbits above thresholds
* nearby bright star distance and Gaia magnitude (for spikes/ghost likelihood)

4. **Spatial split keys**

* healpix at multiple nside plus “region_id” to support spatial CV and avoid leakage.

---

## 12) Critical path (what must happen first) vs defer

### Must-do in the next 1–2 weeks

1. Train real-image baseline detector with clean splits (spatially disjoint).
2. Build independent evaluation set (spectroscopic-selected candidates).
3. Implement injection realism diagnostics and iterate until they pass.

### Can be deferred to “future work” without killing the paper

* foundation models
* complex meta-learners
* full-survey inference at 43M scale (you can demonstrate on a large subset)

---

## 13) What reviewers will most likely reject you for (top 2–3)

1. **Circular evaluation** (training on ML-found candidates and evaluating on the same family of candidates).
   Fix: independent spectroscopic-selected validation, spatial CV, and a clear statement of label uncertainty.

2. **Unvalidated injections** (claims of completeness based on injections that are not demonstrably DR10-like).
   Fix: publish the acceptance diagnostics and bracket uncertainties.

3. **Overclaiming precision/selection** without prevalence and without enough confirmed anchors in each stratum.
   Fix: report conditional metrics, bootstrap/Bayesian binomial intervals, and clearly label “insufficient data” bins.

---

## 14) Probability of MNRAS acceptance if executed well

* If you deliver: real-image baseline + independent validation + injection-calibrated selection surface + failure-mode breakdown + released code/data products: **moderate-to-high**.
* If you skip independent validation or do not validate injections quantitatively: **low**.

Single highest-leverage improvement: **independent validation set** (spectroscopic-selected candidates) plus **spatial CV**.

---

## 15) Answer to the core strategic question: should you fix Phase 4c simulations and return to the original plan?

### Honest comparison of options (publishability vs risk)

#### Option 1 (hybrid: real detector + calibrated injections for selection)

* **Publishability:** highest
* **Risk:** moderate
* **Why:** detector is real-data matched, injections are used only for completeness measurement with explicit validation.

#### “Fix Phase 4c and train on simulations again”

* **Publishability:** lower unless you can *prove* realism across many axes
* **Risk:** high (reviewers will attack domain shift and calibration assumptions)
* **When it’s worth doing:** only as a **secondary component** to Option 1, used to explore parameter space after you have a real-data baseline.

#### Train only on real candidates (replicate Huang et al.)

* **Publishability:** low unless you add a new independent contribution
* **Risk:** moderate (circularity, label noise)
* **Good for:** baseline and sanity checks.

**Recommendation:** keep “Phase 4c realism repair” as a *selection-function engine* and stress-test tool, not the primary training source. That gives you scientific value without betting the paper on simulations.

---

## 16) Concrete blueprint for Option 1 (hybrid) with injection realism integrated

### Milestone M1: Real-image baseline (Week 1–2)

* Data: confirmed lenses + carefully sampled negatives (N1), strict spatial splits.
* Output: baseline detector with calibrated scoring curves by stratum.

### Milestone M2: Independent validation (Week 2)

* Data: spectroscopic-selected lens candidates (single-fiber / pairwise) as “silver positives” for qualitative validation + subset with confirmations where available.
* Output: domain shift analysis.

### Milestone M3: Injection realism calibration (Week 2–3)

* Implement calibrated injection pipeline:

  * magnitude-based flux scaling using DR10 zeropoints
  * per-cutout PSF and local noise estimation
  * acceptance diagnostics (SNR distributions, noise histograms, annulus gate behavior)
* Output: “injection realism report” as a paper appendix + code.

### Milestone M4: Selection function surfaces (Week 3)

* Run injection-recovery across a design grid:

  * (\theta_E), PSF, depth/nobs, host type, mask regime
* Output: completeness surfaces with uncertainty; show where the detector is blind.

### Milestone M5: Failure modes and contaminant analysis (Week 3–4)

* Score contaminant categories; quantify FPR by type and conditions; produce interpretable examples.
* Output: failure taxonomy + prioritization rules for follow-up.

### Milestone M6: Paper assembly (Week 4)

* Methods, datasets, audit protocol, results, limitations, and released artifacts.

---

## 17) Code bundles you already have

* [Download DR10 real-data pipeline + training/eval code](sandbox:/mnt/data/dhs_dr10_realdata_code.zip)
* [Download scheduled masking module](sandbox:/mnt/data/scheduled_masking_module.zip)

If you want, I can map each file in the zip to the milestones above and specify the exact command sequence to reproduce every table/figure in the draft.

---

## 18) Directly addressing “injection realism vs selection function novelty” (why it is connected)

The novelty claim is not “we injected arcs.” The novelty claim is:

* “We provide a **validated injection-recovery completeness** for a real-data-trained detector, as a function of DR10 observing conditions and morphology, and we show exactly which confusers dominate FPR and in which regimes.”

That is what makes selection function meaningful and scientifically usable.

If you do not validate injection realism quantitatively, you cannot claim a selection function. That is why injection realism is not optional and why it is central to Option 1.

---

[1]: https://data.desi.lbl.gov/doc/releases/dr1/vac/strong-lensing/ "DESI Strong Lensing Catalog - DESI Data"
========================



CONTINUATION - Complete Implementation Clarifications Before Execution
We've reviewed your comprehensive blueprint and are ready to execute. However, after auditing our existing codebase against your recommendations, we found several conflicts and gaps that need clarification before we write code. We want to implement correctly the first time to avoid costly EMR reruns.
SECTION A: Negative Sampling Strategy (7 questions)
A1. Two-Pool Design
You recommend two pools (N1: deployment-representative, N2: hard confusers). Our current code uses a single is_control pool.
What ratio should N1:N2 be in the final training set?
For the 100:1 overall negative:positive ratio per (nobs_z, type) bin, how does this break down between N1 and N2?
Should control samples (same galaxy, no injection) be a separate third pool, part of N1, or deprecated?
For N2 (hard confusers), should we curate manually, sample automatically via morphology/color cuts, or both?
A2. Defining Hard Confusers
What specific Tractor catalog criteria define each confuser category (rings, spirals, mergers, edge-on disks, blue clumpy star-formers) for automatic sampling?
A3. Exclusion and Selection
For 5" exclusion around known lenses - is this from centroid, or should we add θ_E?
What maskbit values should exclude a galaxy from being a negative?
SECTION B: Spatial Splits (4 questions)
Our code uses hash-based splitting. You recommend HEALPix.
What nside should we use (64 gives ~0.84 deg² cells, 128 gives ~0.21 deg²)?
Should we stratify HEALPix cell allocation by observing conditions to ensure balanced PSF/depth per split?
For 70/15/15 split, approximately how many HEALPix cells per split for statistical power?
How do we ensure positives are in correct spatial splits?
SECTION C: Label Handling (8 questions)
C1. Implementation
Should tier-based labels be implemented as label smoothing (target = weight) or sample weights in loss function?
Exact values: Tier-A = 0.95, Tier-B = 0.75, Literature confirmed = 1.0?
C2. SLACS/BELLS Problem
Our lessons_learned notes SLACS/BELLS have low DR10 visibility (arcs often invisible).
Should SLACS/BELLS be: included with full weight, down-weighted, excluded from training (evaluation only), or stress-test only?
C3. Paper IV Alignment
Paper IV used 1,372 confirmed lenses. Do we have access to this exact list? If not, how to reconstruct?
C4. Separation
What's the cleanest way to ensure no overlap between training positives and evaluation anchors? Spatial separation? Explicit ID exclusion? Both?
SECTION D: DESI Spectroscopic Catalogs (6 questions)
You strongly recommend using spectroscopic-selected candidates as independent validation.
Are DESI DR1 spectroscopic lens catalogs (Single Fiber Search arXiv:2512.04275, Pairwise Search arXiv:2509.16033) publicly available?
What's the data access URL/format?
Do we need DESI collaboration membership?
How many have spectroscopic confirmation vs just "spectroscopically selected"?
How should we use them: independent validation only, silver positives for training, or something else?
How do we handle overlap with our imaging candidates?
SECTION E: Injection Realism (14 questions)
E1. Source Properties
What distribution should we sample source magnitudes from? What magnitude range?
What source morphology model: smooth Sersic, clumpy, COSMOS? Should we bracket with multiple as sensitivity test?
E2. PSF and Noise
Moffat PSF - what beta parameter? From DR10 metadata or fixed?
For MAD-based noise estimation, measure from outer annulus (r > 20 pixels)? What if outer annulus has sources?
E3. Inner Images
For inner image handling, which should be primary: include with realistic visibility, explicitly suppress, or run both as ablation?
E4. Acceptance Thresholds (Critical)
You specified diagnostics but not exact thresholds:
Arc Annulus SNR matching: "Within factor of ~2 in median" - is this 0.5x to 2x range?
"Similar tails" - how quantified? KS test p-value? 10th percentile within X%?
Noise histogram KS test p-value threshold: p > 0.05? p > 0.1?
Color distribution tolerance on (g-r, r-z): ±0.1 mag? ±0.2 mag?
GO/NO-GO decision: all diagnostics must pass, or weighted score?
SECTION F: Cutouts (4 questions)
Our Phase 4c uses 64×64 (16.4"). You suggested 101×101. Should negatives match existing positives (64×64) or use larger?
If we change to 101×101, must we regenerate all positive cutouts for consistency?
Should we require all three bands (g, r, z) or allow missing bands?
How handle cutouts where one band has NaN but others OK?
SECTION G: Training Configuration (6 questions)
ResNet18 vs EfficientNet-B0 - which as primary baseline?
For fair Paper IV comparison, should we implement their exact ensemble, or is single architecture cleaner?
Minimum epochs for "defensible" baseline?
Batch size for 24GB GPU VRAM with 64×64 3-channel images?
What augmentations are appropriate? Risk of introducing shortcuts with brightness/scaling augmentations?
SECTION H: Selection Function (7 questions)
What axes and resolution for selection function grid?
θ_E: 0.5-3.0" in what steps?
PSF: what range and resolution?
Depth: what binning?
Minimum injection points per grid cell for statistical power?
How handle cells with very few injections: hierarchical smoothing, mark insufficient, merge?
Bootstrap or Bayesian binomial for uncertainty?
How many bootstrap iterations?
What confidence level for error bars?
SECTION I: Milestone Sequencing (2 questions)
Your milestones suggest training baseline in Week 1-2. But training needs negatives (EMR job), proper splits (HEALPix), label handling. Should we:
A) Build EMR job first → Generate negatives → Then train baseline
B) Use existing Phase 4c controls as temporary negatives → Train quick baseline → Then build proper EMR job
What's the critical path? What can run in parallel?
SECTION J: Paper Strategy (5 questions)
What figures are "must-have" for MNRAS acceptance?
What claims should we explicitly AVOID?
How exactly should we frame "following Paper IV methodology" as not being replication?
One-sentence novelty statement for abstract?
MNRAS vs ApJ vs AAS Journals preference for this contribution type?
SECTION K: Computational Constraints (2 questions)
Single-GPU (24GB) + EMR - is anything in the plan unrealistic?
Approximate EMR cost for 500K negative cutouts + injection grid?
SECTION L: Gap Check (3 questions)
What critical question are we NOT asking that would cause reviewer rejection?
Single highest-risk failure mode in our plan?
If scope must be cut, what defer to "future work" vs "must have"?
Total: 59 implementation-critical questions across 12 sections.
Please provide as much detail as possible. We want to implement correctly the first time.
## SECTION A: Negative sampling strategy

### A1. Two-pool design (N1 deployment negatives, N2 hard confusers)

**Recommended training mix (practical and defensible):**

* **N1 : N2 = 85 : 15** within the negative class.

  * Rationale: N1 sets the score distribution and base-rate behavior for the deployment population. N2 prevents “easy-negative” training and improves specificity on known confusers without dominating gradients.
* Maintain your **overall 100:1 negative:positive** ratio in the *training stream*, but implement N2 as an **oversampled stream** rather than trying to match its true prevalence.

**How to implement with 100:1 per (nobs_z, type) bin:**

* For each stratum (nobs_z_bin × type_bin):

  * draw **~85 negatives** from N1 (deployment representative within that stratum)
  * draw **~15 negatives** from N2 (confusers, not necessarily within that exact stratum; but record their stratum so you can report FPR by stratum later)

**Controls (same galaxy “no injection”)**

* For real-image training, **deprecate paired controls as a primary negative source**.
* Keep them as a **diagnostic pool** only (shortcut gates and sanity checks), not part of the main negative training distribution.
* If you must use them temporarily (see Section I), treat them as part of **N1**, but phase them out as soon as N1 is built.

**How to source N2**

* **Both**: automatic high-recall mining + small manual curation.

  * Automatic yields scale.
  * Manual curation ensures you include weird artifacts (sat trails, ghosts) that catalogs miss.

### A2. Tractor criteria for hard confusers (automatic sampling)

Tractor catalogs are not built to label “ring galaxy” directly, so you cannot get high purity from Tractor-only rules. Use a two-step approach:

**Step 1: high-recall candidate mining (cheap, Tractor-only)**

* **Edge-on disks**: TYPE in {EXP, DEV} plus high ellipticity proxy from shape parameters (if available), or axis ratio cuts if Tractor provides them.
* **Blue clumpy star-formers**: use colors and size:

  * low (g-r), low (r-z), relatively extended in g/r (if size proxies exist), and not masked.
* **Spiral/ring/merger proxies** from Tractor alone are weak. Do not pretend they are clean.

**Step 2: confirm confuser morphology via image-based filtering**

* Run a lightweight image model (even your baseline CNN with “non-lens” target) or a simple embedding + clustering, then manually label a few hundred.
* Practical: create N2 by scoring a large pool and selecting high-scoring negatives that are not known lenses, then manually categorize.

Net: Tractor helps you avoid garbage; images determine confuser categories.

### A3. Exclusion and maskbits

**Exclusion radius**

* Use **5 arcsec from catalog position** as the base exclusion.
* Also exclude if within **(5″ + 2×θE_max)** of any known lens center when θE is available.

  * With θE_max = 3″, this becomes ~11″. This reduces “nearby arc” leakage.

**Maskbits**
Exclude negatives if any of these are present in a central region (e.g., r < 8 px):

* bright star mask / saturation
* bleeding trails
* large contiguous masked fraction
  Also exclude if **masked pixel fraction** > 1–2% in the stamp center, and track mask fraction as a covariate.

Do not hardcode maskbits without checking DR10 documentation for meaning. Treat maskbits as a feature for analysis even if you filter.

---

## SECTION B: Spatial splits (HEALPix)

### B1. nside choice

Use **nside=128** for split assignment.

* Smaller cells reduce leakage via survey systematics and local background.
* nside=64 is acceptable, but 128 is safer for “independent regions” claims.

### B2. Stratify HEALPix by conditions?

Do **not** hard-stratify the split by PSF/depth. It can accidentally entangle selection with footprint boundaries.
Instead:

* assign splits by HEALPix deterministically
* then **report** and **verify** balance: distribution of PSF/depth/nobs per split.

### B3. 70/15/15 and number of cells

You do not need a target number of cells, you need:

* enough positives per split
* enough spatial diversity
  With nside=128 you will have many cells; the split is stable.

### B4. Ensuring positives in correct splits

Use the same deterministic function:

* healpix_index = ang2pix(nside, ra, dec)
* split = hash(healpix_index) → train/val/test
  Apply this to positives and negatives identically.
  Then explicitly verify:
* no healpix cell appears in multiple splits.

---

## SECTION C: Label handling

### C1. Smoothing vs sample weights

Use **sample weights in the loss** as the primary mechanism, not only label smoothing.

* label smoothing changes the implied target distribution
* sample weights preserve the meaning of probability while downweighting noisy labels

Recommended:

* Literature confirmed: label=1.0, weight=1.0
* Tier-A spectroscopic/confident: label=1.0, weight=0.9–1.0
* Tier-B probable: label=1.0, weight=0.3–0.6 depending on estimated purity
  Optionally apply mild label smoothing (1.0 → 0.95) for Tier-B only.

### C2. SLACS/BELLS low visibility

Do **not** use “invisible in DR10” positives for training as positives.

* Put SLACS/BELLS into:

  * evaluation Tier-B or stress-test set if their arc SNR is below threshold
  * training only if they pass your arc-visibility selection function

### C3. Reconstructing Paper IV list

You likely cannot reconstruct the exact 1,372 set unless they published the list (table or data release).
Treat alignment as:

* match their broad source categories and selection criteria
* do not claim exact replication

### C4. Overlap prevention

Use **explicit ID exclusion plus spatial split**.

* Build a canonical ID for each lens candidate (ra,dec rounded to 0.1″ plus source catalog name).
* Remove any overlap between training positives and evaluation anchors by ID.
  Spatial separation alone is not enough because lenses cluster and coordinates can be close.

---

## SECTION D: DESI spectroscopic catalogs access

You must verify availability and formats. Use web queries before implementation. The likely outcome:

* some are public via DESI data releases or associated tables
* some may require collaboration access
  Plan defensively:
* treat them as **independent validation** if public
* otherwise, use other independent sets (non-Huang) that are public (e.g., SLACS/BELLS/SL2S confirmed, and other published tables)

Operational rule:

* if a catalog is “spectroscopically selected” but not confirmed, treat as “silver positives” for qualitative validation, not ground truth recall.

---

## SECTION E: Injection realism (critical)

### E1. Source magnitude distribution

Do not pick uniform flux. Use a magnitude distribution tied to real DR10 detectability:

* start with **r-band source mag 22–26** (unlensed)
* draw magnification proxy μ from a broad prior (e.g., 5–30)
* compute lensed apparent magnitude and enforce that **annulus SNR** spans roughly 0–5 for the majority, with a tail to ~10

### E1 morphology model

Bracket with two source families:

1. **smooth Sersic** (baseline)
2. **clumpy** (parametric clumps on Sersic or a small library)
   If you can justify COSMOS cutouts, treat as a third optional bracket, not the only model.

### E2 PSF and noise

* Use per-cutout PSF proxy from DR10 when possible. If you must use Moffat:

  * fixed beta is acceptable only if you show it approximates DR10 PSF wings in stacked stars; otherwise bracket beta (e.g., 2.5, 3.5, 4.5).
* Noise estimation:

  * outer annulus is fine, but first mask sources using sigma-clipping or segmentation.
  * use MAD not std.

### E3 Inner images

Primary path:

* **include inner images** but calibrate their visibility by enforcing realistic contrast relative to the lens light and noise.
  Ablation:
* also run a “suppressed inner image” variant to bound sensitivity.
  Do not pick one without bracketing. Reviewers will ask.

### E4 Acceptance thresholds (GO/NO-GO)

Use a rule-based GO/NO-GO. Suggested thresholds:

1. **Annulus SNR distribution match** (injections vs real Tier-A anchors):

* median within **0.5× to 2×**
* 10th percentile within **0.5× to 2×**
* 90th percentile within **0.5× to 2×**

2. **Noise-only pixels KS test**:

* p-value > **0.05** for background pixel distributions in each band (after masking sources)

3. **Color distributions**:

* median (g-r) and (r-z) within **±0.2 mag**

4. **Visual sanity panel**:

* blinded 50-image panel: injections should not look systematically “cleaner” than real.

All must pass for GO. If one fails, you fix and rerun a small validation batch, not the full EMR run.

---

## SECTION F: Cutouts

### F1. 64×64 vs 101×101

Pick one and keep consistency. Given you already have 101×101 for many real candidates, I would standardize on **101×101** and train with center-crop augmentation down to 64 if needed.

### F2. Regenerate positives?

Yes, if you change canonical size for training. For evaluation, you can run inference at the native size if the model supports it, but for clean methodology, keep one size.

### F3. Require all bands?

Require **g,r,z present** for training baseline if you want parity with Huang-style pipelines. If missing:

* either exclude or fill with a sentinel and add a “band_missing” feature.
  For first pass, exclude missing-band cutouts to avoid hidden artifacts.

### F4. NaNs in one band

Exclude if NaNs are in the central region. If NaNs only in outer rim and you crop/augment away, you can keep but track it.

---

## SECTION G: Training configuration

### G1. Primary baseline

Use **ResNet18** first. It is fast, stable, and good for ablations.
Then add EfficientNet-B0 only if:

* you can demonstrate incremental gain on independent validation.

### G2. Paper IV ensemble replication?

Do not start with full replication. Start with:

* single ResNet18 baseline + strong data protocol
  Then:
* add a second architecture for diversity if time allows.

### G3. Minimum epochs

For a defensible baseline:

* train until validation metrics plateau and calibration stabilizes, typically **20–40 epochs** with cosine schedule.
  State in paper: early stopping on spatial-val set.

### G4. Batch size (24GB)

For 64×64×3:

* batch 256 is often possible; for 101×101, batch 128–192.
  Use AMP.

### G5. Augmentations

Safe:

* rotations/flips
* small translations
* mild additive noise consistent with DR10
* mild PSF blur jitter within observed range
  Risky:
* aggressive brightness/contrast jitter (can create shortcuts)
* heavy normalization changes
  If you use brightness jitter, constrain it and show gates do not activate.

---

## SECTION H: Selection function grid

### H1. Axes and resolution (minimum viable)

Use 3 primary axes first:

* θE: 0.5–3.0 in **0.25″ steps**
* PSF FWHM: 0.9–1.8 in **0.15″ steps**
* depth (psfdepth_r or galdepth_r): 22.5–24.5 in **0.5 mag steps**
  Optional fourth axis:
* host type (Tractor TYPE bins)

### H2. Injections per cell

Minimum **200 injections per cell** for stable binomial error bars.
If you cannot afford that, reduce grid resolution.

### H3. Sparse cells

Do not smooth unless you can justify a model.
Preferred:

* mark “insufficient” below Nmin
* merge adjacent bins

### H4. Uncertainty

Use **Bayesian binomial** intervals per cell (Beta posterior) as default.
Bootstrap is fine for aggregated metrics; for per-cell, Bayesian is cleaner.
Report 68% intervals (and optionally 95% in appendix).

---

## SECTION I: Milestone sequencing

### I1. EMR first vs quick baseline

Critical path is:

* you need negatives to train properly, but you can start de-risking immediately.

Recommended parallel plan:

* **Path A (fast):** Train a quick baseline using existing negatives (temporary) only to validate training code, splits, and evaluation pipeline.
* **Path B (correct):** Build EMR negative sampling + cutouts in parallel; switch training to proper N1+N2 once ready.

Do not interpret quick-baseline performance as scientific.

---

## SECTION J: Paper strategy

### Must-have figures

1. Data and split schematic (spatial split + strata)
2. Score distributions by stratum (PSF/depth/nobs/type)
3. Selection function heatmaps (C(\theta_E,\mathrm{PSF})) at fixed depth slices
4. Failure mode gallery with quantitative counts by category and conditions
5. Independent validation performance table (even if labels are weak)

### Claims to avoid

* overall precision in the survey
* cosmology constraints
* “complete” lens sample
* claims of outperforming Huang unless you have a matched evaluation protocol

### One-sentence novelty statement

“We provide a detector-audit framework for DR10 strong-lens searches, including injection-calibrated completeness surfaces and a condition- and confuser-resolved false-positive taxonomy, enabling bias-aware use of ML lens catalogs.”

### Journal choice

MNRAS is reasonable for methods + selection bias work if you keep claims disciplined. ApJ is also plausible. Do not optimize for venue now; optimize for correctness.

---

## SECTION K: Computational constraints

Single GPU + EMR is sufficient for:

* training 1–3 models
* injection grid on a reduced host sample
  Risk is cost blow-up from too fine a selection grid or too many cutouts.

EMR cost depends entirely on instance types and IO; treat as variable and keep:

* 500k cutouts is feasible
* injection grid: start with a small host subset per stratum and scale only after diagnostics pass

---

## SECTION L: Gap check

### Critical question you are not asking

“How will we demonstrate independence from prior ML candidate selection?”
You need an explicit section and experiment for this.

### Single highest-risk failure mode

Injections that pass visual checks but fail quantitative distribution matching, leading to a selection function that reviewers reject as “not validated.”

### If scope must be cut

Cut:

* full ensemble meta-learner
* full-survey inference
  Keep:
* real-image baseline with strict spatial split
* independent validation set
* validated injection-recovery selection function
* failure-mode taxonomy

---

If you want this turned into an execution checklist (commands, expected artifacts, GO/NO-GO gates at each step) aligned to your schema and EMR job stages, say so and I will write it as a single runbook document.


===============



