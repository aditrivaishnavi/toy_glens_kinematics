THere is a lot of technical papers relevant to Strong gravitational lenses in DESI . I searched for the author Christopher Storfer and found this list : https://inspirehep.net/authors/2091894 some of them is very latest in Jan 2026. Please carefully review all the papers and strongly look at how we can set ourselves for success in original research with clear concrete goal definition and steps for achieving them . Note clearly specify the research question and methodology. Please validate your answer by look at other relevant support. As we are expecting original research and not replicating them for MNRAS, we need a good amount of original novelty, focus on that . provide multiple options here and rank them by what looks most promising and doable. I believe we can find very good suggestions and approaches here and we can get it published. Provide full concrete plan which should be scientifically meaningful and pass the question : is this research at the level the folks in the papers are doing ?  Below are four original-research directions that are incremental relative to Huang et al. Papers I–IV, but still plausibly MNRAS-level if executed cleanly with strong controls, ablations, and honest limitations. I’m ranking by a mix of novelty, doability in 4–8 weeks, and likelihood reviewers accept the contribution as “new” rather than a replication.
I am grounding the “what they did” claims in Paper IV (your uploaded PDF), including their deployment population, candidate triage, and ensemble/meta-learner specifics.

Option 1 (Most promising + doable): A selection-function audit that separates “detectability” from “model bias”
Research question
For DR10 lens searches, what fraction of “known lenses” are non-detections because the lens is intrinsically not visible in DR10, versus non-detections due to model failure?
This is the key gap in most ML lens papers: they report recall, but don’t decompose why recall fails.
Core novelty
A two-stage selection function:
1. Observability / detectability gate (data-limited): “Would a human or a generic arc-energy detector see any arc-like signal in DR10 at all?”
2. Model gate (algorithm-limited): “Given it is observable, does the model recover it?”
This creates a defensible separation between “not in the data” and “in the data but missed”.
Methodology blueprint
1. Define detectability metrics computed from DR10 cutouts (no ML needed):
    * Arc-annulus energy after subtracting an azimuthal model (you already have this concept).
    * Multi-band consistency: arc-energy must cohere in g/r/z (or at least r/z).
    * A “background stability” metric: noise stationarity in the annulus.
2. Calibrate detectability thresholds using:
    * Your Tier-A anchors (confirmed).
    * A matched random galaxy sample (negatives).
3. Compute two recalls:
    * Observable recall: fraction of anchors passing detectability gate.
    * Conditional model recall: fraction of those observable anchors recovered by the CNN.
4. Report completeness surfaces:
    * completeness vs (PSF FWHM, depth, z_nexp), and any lens proxy you have (theta_E for anchors).
5. Failure mode atlas:
    * For false negatives, label the failure as:
        * “Not detectable in DR10” vs “Detectable but missed”.
    * Provide representative cutouts.
Why reviewers will accept this as original
It directly addresses the fairness critique: “you evaluated on lenses that are not visible at DR10 resolution.” Your paper would formalize that as a measurable selection function instead of hand-waving.
Success criteria (paper-level)
* A figure that cleanly shows detectability vs model performance, and how they vary with observing conditions.
* A reproducible procedure: anyone can apply the detectability gate and reproduce the selection function.
Key pitfalls (and how to preempt)
* Pitfall: Detectability gate becomes “another classifier” in disguise. Preempt: Keep it deliberately simple and interpretable (energy in annulus, multi-band coherence).
* Pitfall: Small-N anchors cause noisy surfaces. Preempt: Use coarse binning + Bayesian/binomial intervals and clearly mark bins with insufficient support.

Option 2: Contaminant taxonomy + prevalence-aware FPR for DR10 lens searches
Research question
What contaminant morphologies dominate the top-scored candidates in DR10, and how does FPR change by object type and observing conditions?
Paper IV reports candidate counts and breaks down by Tractor type in DR10. A strong follow-on is to make false positives measurable and actionable.
Core novelty
A structured contaminant set plus category-conditional FPR that can be plugged into search planning:
* “If you scan 43M galaxies (their deployment scale), what fraction of the top 0.01% are rings vs spirals vs mergers vs artifacts?”
Methodology blueprint
1. Expand your contaminant set to include the categories that actually dominate top candidates:
    * rings, spirals, mergers, edge-on disks, bright-star artifacts, plus blue clumpy star-formers (often the sleeper confuser).
2. For each category:
    * Measure FPR at multiple thresholds (top-K, top p>t).
    * Stratify by (PSF, depth, z_nexp).
3. Add a prevalence model (even if rough):
    * Use a large random sample of DR10 galaxies and run a lightweight morphology tagger or Tractor-type + color cuts as a proxy.
    * Convert category-conditional FPR into an estimated overall FPR for a realistic search population.
4. Deliverable that reviewers like:
    * A table: “Expected confuser composition of top candidates for a DR10-like search.”
Pitfalls
* Prevalence estimation can be attacked. Make it explicitly “scenario-based” with sensitivity tests.

Option 3: Ensemble diversification that is truly “different data”, not just “different architecture”
Paper IV uses ResNet + EfficientNet and stacks them with a small meta-learner. They also emphasize deployment-scale scanning and top-recommendation selection.
Research question
Can we improve reliability (calibration + robustness to confusers) by training domain-specialized models on disjoint DR10 regimes, and combining them with an uncertainty-aware ensemble?
Core novelty
Not “new backbone”, but new training regime:
* Split by observing condition regime (seeing/depth/exposures) and by galaxy population regime (Tractor type, color).
* Show that ensemble disagreement predicts false positives and low-quality candidates.
Methodology blueprint
1. Define 3–6 domains:
    * Good-seeing / bad-seeing
    * Deep / shallow
    * High exposure count / low exposure count
    * SER/DEV vs EXP/REX (mirrors the type breakdown they report)
2. Train one model per domain (same architecture).
3. Combine with:
    * Simple averaging
    * Stacking
    * And an uncertainty score: entropy, variance across models, or disagreement rate
4. Evaluate:
    * Anchor recall
    * Contaminant FPR
    * Calibration (ECE) and “selective classification” curves: performance vs coverage as you drop high-uncertainty predictions.
Pitfalls
* If positives are Huang-discovered candidates, label circularity will bite. Your defense is to:
    * Separate evaluation on Tier-A confirmed anchors from training positives.
    * Report results both with candidates-in-training and without (ablation).

Option 4 (Higher novelty, higher risk): “Foundation features” as a controlled baseline, not a hype pivot
Paper IV mentions foundations/custom CNNs in the title, and uses a classical CNN ensemble approach. A strong contribution is a careful audit: when do frozen features help, and when do they fail?
Research question
Do frozen self-supervised vision features improve DR10 lens detection robustness (especially to confusers and observing-condition shifts) compared to end-to-end CNNs trained on small positives?
Blueprint
1. Use a frozen encoder (e.g., DINOv2-like) and train only a small head.
2. Compare to ResNet18 end-to-end.
3. Evaluate on:
    * Anchors
    * Contaminants
    * Cross-regime shifts (good seeing vs bad seeing)
4. Report where it helps and where it fails, with interpretability.
Pitfalls
* Reviewer skepticism if it reads like “we tried a trendy model”. Keep it as a controlled baseline with strong negative results allowed.

Ranking summary
1. Selection-function audit separating detectability vs model failure (best novelty-to-effort ratio, strongest “referee-proof” framing)
2. Contaminant taxonomy + prevalence-aware FPR (practical, publishable, complements existing candidate papers)
3. True domain-diversified ensemble with uncertainty (incremental but defensible if you show real robustness gains)
4. Foundation-feature controlled baseline (novel but riskier to execute convincingly in your timeline)

Concrete 4-week plan for Option 1 (recommended)
Week 1: Data + “detectability gate”
* Finalize Tier-A anchors and contaminants.
* Implement detectability metrics (annulus energy, multi-band coherence, background stability).
* Calibrate a conservative detectability threshold using anchors vs random galaxies.
Week 2: Train 2–3 real-data models
* Baseline ResNet18
* One “robustness” variant (domain split or stronger augmentation)
* Lock splits and freeze them.
Week 3: Selection function surfaces
* Compute:
    * Observable recall
    * Conditional model recall
* Stratify by (PSF, depth, exposures) and by simple galaxy type proxies.
Week 4: Failure mode atlas + paper draft
* Produce:
    * Detectability-vs-model schematic
    * Surfaces + uncertainties
    * Gallery of failures categorized into “not in DR10” vs “model miss”
* Write limitations bluntly (label noise, anchor theta_E coverage, candidate circularity).

What I could not fully validate from your last message
You asked me to “carefully review all papers” on Christopher Storfer’s INSPIRE profile, including very recent Jan 2026 items. INSPIRE’s author page itself was not directly retrievable in a way that lets me enumerate that full list here, so I cannot honestly claim I reviewed every paper on that profile. I did ground the recommendations above in Paper IV’s specifics (your uploaded PDF) and in the DESI-lens-finder workflow it describes.
If you paste the list of the specific Jan 2026 Storfer papers (titles/arXiv IDs), I can map each option above to exact gaps those papers leave open, and tighten the novelty claims accordingly.


=====

Here is the list of papers: Titles and arXiv IDs from your pasted list:

1. **A Novel Lensed Point Source Modeling Pipeline using GIGA-Lens with Application to SN Zwicky and SN iPTF16geu** — **arXiv:2601.18787**
2. **A New Way to Discover Strong Gravitational Lenses: Pairwise Spectroscopic Search from DESI DR1** — **arXiv:2509.16033**
3. **The Carousel Lens I: A Spectroscopic Survey of the Carousel Lens Field** — **arXiv:2601.14422**
4. **DESI Strong Lens Foundry V: A Sample of HST-Observed Strong Lenses Modeled with GIGA-Lens** — **arXiv:2512.07823**
5. **The DESI Single Fiber Lens Search. I. Four Thousand Spectroscopically Selected Galaxy-Galaxy Gravitational Lens Candidates** — **arXiv:2512.04275**
6. **DESI Strong Lens Foundry III: Keck Spectroscopy for Strong Lenses Discovered Using Residual Neural Networks** — **arXiv:2509.18086**
7. **DESI Strong Lens Foundry II: DESI Spectroscopy for Strong Lens Candidates** — **arXiv:2509.18089**
8. **ML-driven Strong Lens Discoveries: Down to θ_E ∼ 0.03″ and M_halo < 10^11 M⊙** — **arXiv:2507.01943**
9. **DESI Strong Lens Foundry. I. HST Observations and Modeling with GIGA-Lens** — **arXiv:2502.03455**
10. **The Carousel Lens: A Well-modeled Strong Lens with Multiple Sources Spectroscopically Confirmed by VLT/MUSE** — **arXiv:2408.10320**
11. **A Targeted Search for Variable Gravitationally Lensed Quasars** — **arXiv:2408.02670**
12. **Lensed Type Ia Supernova “Encore” at z = 2: The First Instance of Two Multiply Imaged Supernovae in the Same Host Galaxy** — **arXiv:2404.02139**
13. **DESI-253.2534+26.8843: A New Einstein Cross Spectroscopically Confirmed with Very Large Telescope/MUSE and Modeled with GIGA-Lens** — **arXiv:2307.12470**
14. **Retrospective Search for Strongly Lensed Supernovae in the DESI Legacy Imaging Surveys** — **arXiv:2301.03578**
15. **LensWatch. I. Resolved HST Observations and Constraints on the Strongly Lensed Type Ia Supernova 2022qmx (“SN Zwicky”)** — **arXiv:2211.03772**
16. **Finding Multiply Lensed and Binary Quasars in the DESI Legacy Imaging Surveys** — **arXiv:2208.06356**
17. **New Strong Gravitational Lenses from the DESI Legacy Imaging Surveys Data Release 9** — **arXiv:2206.02764**
18. **Discovering New Strong Gravitational Lenses in the DESI Legacy Imaging Surveys** — **arXiv:2005.04730**
19. **Finding Strong Gravitational Lenses in the DESI DECam Legacy Survey** — **arXiv:1906.00970**   I’m sorry this has been such a grind. The “simulation-first” path is common in lens ML, but your new measurement (arcs essentially at the DR10 noise floor) is a valid, decisive empirical check. You did the right thing by stress-testing reality.
Below are four original-research options, ranked by (a) likelihood of being publishable in MNRAS, (b) novelty relative to the DESI lens-finder series, and (c) feasibility with your current assets (5k candidates, ~100 confirmed anchors, contaminant set, DR10 cutouts, single-GPU training).
I anchor these options in what Paper IV explicitly motivates: systematic comparisons of lens searches to investigate demographics/selection functions , and their note that ensemble gains are limited when base models share the same data, suggesting training on different subsets to increase diversity .

Option 1 (Most promising + most “MNRAS-safe”): A Selection-Function Audit Across Search Channels
Research question
How do different DESI lens-discovery channels select different lens demographics, and what is the implied selection function in DR10 imaging?
This is not “replicating Huang et al.” because the novelty is the comparative audit across channels and the failure-mode attribution.
Core novelty
1. Cross-channel comparison: imaging-CNN candidates vs spectroscopic lens search candidates (single-fiber, pairwise spectroscopic, foundry follow-up papers) vs quasar lenses vs SN lenses.
2. Demographic selection: quantify how each channel’s yield varies with Tractor type, seeing/depth proxies, z-band exposure count (Paper IV already controls for z exposures to avoid bias ), separation scales, color contrast, and morphology.
3. Agreement/disagreement maps: where channels overlap and where they are complementary.
Methodology blueprint
A. Define “channels” (positive sets)
* Imaging candidates: your 5,104 “DESI” candidates.
* Spectroscopic candidates: ingest the catalogs from the newer DESI spectroscopic lens-search papers (single-fiber, pairwise spectroscopic, strong lens foundry spectroscopy). Treat as separate channels.
* Confirmed anchors: your ~100 Tier-A confirmed lenses (recall reference set).
B. Define a unified feature space (no lens modeling required) For each candidate/anchor:
* DR10 imaging scalars: z_nexp, psf_fwhm, psfdepth, galdepth, mask fractions, EBV.
* Tractor type + magnitudes, colors, shape.
* Simple image-derived features measured on 101×101 grz cutouts:
    * arc-annulus flux / noise (your annulus gate concept)
    * azimuthal-residual “tangential asymmetry” statistic
    * “curvature-likelihood” proxy (e.g., oriented ridge energy integrated over azimuth) These are not meant to be a detector; they are measurement axes for the selection audit.
C. Primary analyses
1. Selection surfaces by channel: estimate ( P(\text{selected} \mid \text{features}) ) for each channel using calibrated models (e.g., gradient-boosted trees with monotonic constraints where appropriate).
2. Channel complementarity: quantify Jaccard overlaps; train “channel classifier” to predict which search produced a candidate based on features. High separability means strongly different selection functions.
3. Failure modes: use your contaminant categories to estimate channel-specific FPR tendencies (rings vs spirals vs mergers vs artifacts).
D. Strong claim you can defend
* “Channel A is biased toward X; Channel B is biased toward Y; combining them improves coverage of parameter space.” This aligns with Paper IV’s explicit call for demographic/selection-function comparison studies .
What reviewers will look for
* Clear definition of “selection function” (operational, not philosophical).
* Evidence that differences are not just trivial observing-condition artifacts (you already did similar tests).
* Honest limits: candidates are not all confirmed lenses; treat “grade” or “channel” as noisy labels.
Concrete 4-week execution plan
Week 1: ingest all channel catalogs; unify cutout pipeline; compute feature table Week 2: train channel-wise selection models; generate selection surfaces + overlaps Week 3: contaminants/failure-mode stratification; uncertainty via bootstrap Week 4: paper writing + figures (selection maps, overlap Venn/UpSet, failure galleries)

Option 2 (High novelty, still feasible): Domain-Specialized Ensemble That Actually Produces Diverse Errors
Research question
Can we create a materially more diverse ensemble for DR10 lens finding by training base models on disjoint “domains” that correspond to real survey regimes?
Paper IV notes that when base models are trained on the same dataset, predictions are correlated and meta-learning cannot hugely outperform averaging , and explicitly suggests training on different subsets to improve diversity .
Core novelty
Not “use an ensemble,” but: a principled domain split + diversity metrics + demonstrated gains on hard confusers and faint-arc regime.
Methodology blueprint
A. Define domains (choose 2–4 splits, not 20)
* Depth regime: shallow vs deep (psfdepth / galdepth bins).
* Seeing regime: good vs poor (psf_fwhm bins).
* Morphology regime: early-type (SER/DEV/REX) vs EXP-late-type.
* Optional: sky region split (to test spatial generalization).
B. Training design
* Base model i sees only its domain’s negatives at realistic prevalence (Paper IV increases nonlens:lens ratio to better reflect deployment ).
* Positives: (1) confirmed lenses always included; (2) candidates weighted by grade/purity estimate (label smoothing).
* Architectures: keep simple (ResNet18 + EfficientNet-B0/B1). Paper IV shows a small ResNet can match EfficientNet AUC with far fewer params , which supports not overcomplicating.
C. Diversity measurement (must-have for novelty)
* Prediction correlation by regime.
* Disagreement rate on contaminants and on “near-threshold” candidates.
* Ensemble variance as uncertainty proxy.
D. Evaluation
* Your anchors for recall (Tier-A only).
* Your contaminants for FPR by type.
* Stress tests: faint-arc subsets and small-separation subsets.
Deliverable claim
* “Domain-specialized ensemble improves robustness in poor seeing / shallow depth / EXP morphologies without sacrificing performance elsewhere.”
4-week plan
Week 1: finalize domain bins; prepare stratified datasets Week 2: train base models (single GPU sequential is fine) Week 3: diversity analysis + ensemble stacking/averaging comparisons Week 4: paper + release trained weights + domain split recipe

Option 3 (Medium novelty, higher risk): Imaging–Spectroscopy Consistency Study for Candidate Validation
Research question
Do imaging-CNN-selected lens candidates show consistent spectroscopic signatures (e.g., multiple redshifts / emission-line anomalies), and can we predict “spectroscopic confirmability” from imaging + metadata?
Why it can be original
It bridges the imaging-candidate catalogs with DESI DR1/DR? spectroscopic lens searches: a cross-modal validation pipeline.
Methodology (lean version)
* Cross-match imaging candidates to DESI spectra availability.
* Define “spectroscopic-lens-likeness” labels using published spectroscopic candidate definitions.
* Train a lightweight model to predict which imaging candidates are most likely spectroscopically confirmable.
* Output: a ranked list for follow-up prioritization, with analysis of what imaging features correlate with spectral confirmability.
Risk
Depends on access to spectra/catalog products and crossmatch completeness. Still publishable if executed cleanly.

Option 4 (Most ambitious, longer timeline): Realism-Calibrated Injection Using Real Source Magnitudes + Detectability Limits
Research question
What is the detectability boundary for arcs in DR10, and how does it depend on seeing/depth and lens galaxy properties, when injections are photometrically calibrated to real source populations?
Novelty
This rescues simulation in a way that is new and defensible: simulations become a calibrated detectability experiment, not a training set.
Why it is longer
You must calibrate flux using zeropoints and validate injection realism against real DR10 lenses. It is doable, but not the fastest MNRAS path in 4 weeks.

Ranking (promise × feasibility)
1. Option 1: Cross-channel selection-function audit (best combination of novelty + defensibility + uses your current assets)
2. Option 2: Domain-specialized ensemble + diversity metrics (high novelty, directly motivated by Paper IV ensemble-correlation limitation )
3. Option 3: Imaging–spectroscopy consistency validation (good if you can reliably crossmatch to DESI spectra)
4. Option 4: Calibrated injection detectability (important, but longer and easiest to get bogged down)

“Is this at the level of the DESI lens papers?”
Yes, if you commit to one of these as the primary research question and execute with:
* a clean operational definition of the selection function,
* uncertainty quantification (bootstrap, regime-stratified intervals),
* and a concrete, released artifact (tables + code + candidate overlap lists).
Option 1 is the most referee-proof because Paper IV itself flags the need for selection-function/demographics comparisons .

Decision recommendation
If your goal is original novelty with high probability of MNRAS acceptance, choose:
* Primary paper: Option 1 (selection-function audit across channels)
* Secondary contribution inside same paper or a short companion: Option 2 (domain-specialized ensemble diversity study)
If you want, I will turn Option 1 + Option 2 into a single merged blueprint with:
* exact datasets to build,
* exact figures/tables MNRAS reviewers will expect,
* and an explicit “claims we can defend” section (with what you must not claim).
  =====   Given the amount of time we spent on sim-to-real work, I want to honestly talk to you about (4). Please advice with sincerity and with the goal of publishing the research. I think looking back i made a mistake of not checking the outpput of the simulated dataset and carefully evaluating for closeness with the strong gravitationa lenses were found from DESI survey. It was a miss from both of us. Now, that we also have the 5000 or so candidates, can we fix the original phase 4c  (spark_phase4_pipeline) to produce very realistic, almost close and correct simulations with matching flux, brightness and all other parameters we learnt are important. Would that make it a path for us to proceed what we earlier intend to do ? In that case, how will it integrate to end to end original research. Any gaps that he reviewers will flag ? how to address them strongly  and defend them successfully

Yes, you can salvage (4) and make it a credible, publishable path, but only if you treat simulation as a calibrated injection-recovery instrument, not as the primary source of “labels for training.”
The clean way to integrate the 5,000 candidates is:
* Train the lens finder on real images (confirmed + carefully handled candidates, like Huang et al.)
* Use a repaired Phase 4c injection pipeline to measure the selection function of that real-image model under DR10 conditions
That yields a defensible, original MNRAS contribution: a calibrated completeness model and a bias-corrected interpretation of candidate catalogs.

Can Phase 4c be fixed to produce realistic DR10 arcs?
Yes, but “realistic” must be defined by measurable DR10 observables and validated against real lenses, not by eyeballing.
What has to change in Phase 4c
Your core failure was uncalibrated arc flux scaling. Fixing it requires four calibrations:
1) Photometric calibration to DR10 units
Replace arbitrary arc_flux_total = random(100, 500) with a pipeline that generates a source magnitude distribution and converts it to counts using DR10 zeropoints.
Minimum viable calibration:
* Draw source apparent magnitudes in g/r/z from an empirical distribution (see below).
* Convert to electrons/ADU per band with DR10 ZP and exposure time conventions.
* Apply DR10 PSF convolution per-band and add DR10-consistent noise.
If you cannot reliably obtain per-cutout ZP, you can still calibrate by matching measured background RMS in the cutout and scaling simulated counts so that the resulting SNR distribution matches reality.
2) Noise model anchored to the actual cutout
Do not synthesize noise from a generic Gaussian. Use one of:
* Empirical noise from DR10 inverse-variance maps if available in cutouts
* Otherwise, estimate robust noise from blank-sky annuli in the same cutout and inject noise consistent with that
3) PSF realism
The PSF is not just FWHM. For realism you need:
* Band-dependent PSF (g/r/z differ)
* Spatially varying PSF per location
* A realistic PSF model (not just Gaussian) consistent with DR10 products
If you cannot access full PSF models cheaply, use a Moffat with parameters fit from DR10 PSF metadata, per band.
4) Source population realism
Your simulated sources must match DR10-detectable lensed source properties:
* color distribution
* size distribution after seeing
* surface brightness distribution
You can bootstrap this from:
* faint blue galaxies in DR10 near your lensing galaxies
* HST-based source priors, but forward-modeled to DR10 seeing

How to use the 5,000 candidates without circular logic
Candidates are not ground truth lenses. Reviewers will flag “you trained or calibrated on your own predictions.”
You can still use them safely in two roles:
1. As an image-domain realism target Use candidates only to match marginal distributions of observables (arc-annulus SNR distribution, residual tangential asymmetry, color contrasts), not as positive labels.
2. As a weak-label set with uncertainty If you must use them as positives anywhere, you need grade-based label noise handling (label smoothing, or treating them as “likely lenses” with probability weights). But for the simulation calibration, it is better to avoid using them as labels at all.

End-to-end original research that becomes publishable
Paper thesis that is defensible
“Given a real-image-trained lens finder, we measure its DR10 selection function via calibrated injection-recovery, quantify failure modes, and provide bias-aware guidance for lens demographics and follow-up prioritization.”
That is original if you deliver:
* A validated injection simulator that matches DR10 arc detectability statistics
* Completeness surfaces as a function of observing conditions and lens/source parameters
* Uncertainty estimates and robustness checks
* A public artifact (selection-function lookup table + code)
Concrete workflow
1. Train detector on real data
* Positives: confirmed anchors + high-grade confirmed literature lenses
* Candidates: optional, treated as weak labels or excluded from training and used only for evaluation
2. Freeze the detector No hyperparameter tuning on injected evaluation sets beyond sanity checks.
3. Run calibrated injections onto real DR10 galaxy cutouts Your Phase 4c Spark job generates (galaxy cutout + injected arc) stamps.
4. Score injections with the frozen detector Compute detection probability as a function of:
* θE, source mag, source size, color, impact parameter, shear
* seeing, depth, z_nexp, galaxy magnitude/type
5. Build the selection function Produce:
* completeness heatmaps
* completeness marginal curves
* uncertainty bands (bootstrap over galaxies and sky regions)
6. Apply selection function to interpret candidates You can then say: “Our catalog is biased against regime X; correcting for it changes the inferred distribution of Y.” That is a publishable analysis even if you do not confirm new lenses.

What reviewers will flag and how to defend it
1) “Your simulator is still not realistic”
Defense requires pre-registered, quantitative validation, for example:
* Match the distribution of arc-annulus SNR in real confirmed lenses vs injections after forward modeling
* Match residual-image statistics (tangential asymmetry, ridge energy) between injections and real positives
* Blind visual Turing test: mixed set of real lenses and injections graded by humans, report confusion matrix
2) “You calibrated using candidates found by similar ML, so you inherited their biases”
Answer:
* Use confirmed lenses for validation targets
* Use candidates only for unlabeled image statistics
* Show sensitivity analyses: calibrate with and without candidates, compare resulting completeness surfaces
3) “Completeness depends on the detector, so your selection function is not universal”
Correct. Say it explicitly. Defense:
* Provide selection functions for 2–3 detectors (ResNet baseline, EfficientNet baseline, ensemble)
* Show which features of completeness are stable vs model-dependent
4) “You do not have truth labels in the field, so your correction may be wrong”
Defense:
* Your selection function is conditional on detectability in DR10, not on true lensing rate
* You report uncertainty and do not overclaim prevalence
* You use contaminants and anchors as external checks
5) “Your injections assume a lens/source model that may not match reality”
Defense:
* Use a family of source morphologies (not only Sersic)
* Fit priors to real galaxy populations
* Report model-mismatch uncertainty by varying source morphology and lens mass model and showing impact on completeness

The decisive “go/no-go” criteria for fixing Phase 4c
Before you commit EMR-scale generation, run a small calibration loop until these pass:
1. Arc detectability parity Injected arcs should land in the same regime as real DR10 arcs:
* many are near noise floor
* only a small fraction are obvious
2. Metric parity For a fixed lens galaxy brightness bin:
* distribution of arc-annulus SNR
* distribution of residual tangential asymmetry
* distribution of color contrast (g-r, r-z) in the annulus should match real confirmed lenses within tolerance.
3. Failure-mode parity Contaminants should still fool the detector at similar rates before and after injection calibration. If calibration makes contaminants trivial, something is off.

Recommendation
Proceed with fixing Phase 4c if and only if you commit to this framing:
* Real-image training is the detector.
* Calibrated injections are the measurement tool for selection function.
That gives you a strong, coherent, defensible end-to-end story that reviewers can respect, and it preserves your original intent (selection function and “seeing-limited” detectability) without forcing simulation to do what it is worst at (being the training truth).
If you want, paste the specific arc rendering and flux-scaling function from spark_phase4_pipeline and I will give a concrete replacement design: inputs, required DR10 metadata, and a calibration loop with explicit acceptance thresholds.
=========

Given the state we are in , how will you compare this with other options that you mentioned. Be honest and diligent and sincere


Option comparison given your current state
I am comparing options by: (1) probability of producing a defensible MNRAS submission in 4–6 weeks, (2) originality, (3) risk of reviewer rejection, (4) compute and execution complexity.

Option 1: Real-image lens finder + calibrated injection-recovery selection function (hybrid)
What it is
* Train on real DR10 cutouts (confirmed lenses + curated candidates with label-noise handling)
* Freeze model
* Use a repaired Phase 4c injection pipeline to measure completeness surfaces and failure modes
Why it fits your state
* You already have: candidates, anchors, contaminants, infrastructure, Spark pipeline, gate mentality
* Fixing Phase 4c becomes validation + calibration work, not “hopeful sim-to-real”
Originality
* High if you deliver: calibrated injection realism checks + model-dependent selection functions + uncertainty, plus a public selection-function artifact.
* This is not “replicating Huang”; it is an audit and measurement layer that most lens-finder papers do not deliver rigorously.
Biggest risks
* Simulator realism still gets attacked. You must show quantitative parity against real lenses for the observables you claim to model.
* Circularity: using ML-found candidates as positives. Mitigation: keep them out of “truth” evaluations; treat as weak labels or use only for marginal distributions.
Cost/effort vs upside
* Effort: Medium-High (calibration loop + metrics + reruns)
* Upside: High (publishable even without new lens discoveries)
Verdict
* Best balance of publishability and originality for where you are.

Option 2: Pure real-image training and scanning (Huang-style), plus a modest novelty add-on
What it is
* Train on real lenses vs real nonlenses at large imbalance
* Run on DR10 search sample, produce ranked candidates
* Novelty comes from something like improved calibration, better contaminant audit, or domain-split ensembles
Why it fits your state
* Fastest to “working model”
* Uses your downloaded positives/anchors/contaminants directly
Originality
* Moderate. If you only replicate their pipeline, reviewers will say “incremental.”
* You need a real differentiator: e.g., calibration + reliability under prevalence shift, spatial generalization tests, or an ablation-driven failure taxonomy with quantitative rates.
Biggest risks
* “You trained on candidates found by prior ML, so you reproduced their biases.”
* “No new confirmations, so impact is limited.”
* A model that works is not automatically a paper unless you add rigorous analysis beyond ROC.
Cost/effort vs upside
* Effort: Low-Medium
* Upside: Medium unless you add a strong audit contribution
Verdict
* Best for speed. Publishable if the novelty is a serious evaluation/robustness contribution, not just “we trained a CNN.”

Option 3: Fix simulations and go back to sim-trained model as primary detector (the original plan)
What it is
* Fully repair injection realism and then train primarily on simulations again, expecting transfer.
Why it fits your state
* You already built this pipeline, but the trust deficit is now real: you know how badly it can fail.
Originality
* Potentially high if you truly solve realism, but…
Biggest risks
* Highest reviewer skepticism: “Show me your sims match reality.”
* You can spend weeks calibrating and still miss a key domain shift.
* Even if arcs match brightness, there are many other gaps (morphologies, deblending, artifacts, color gradients, correlated noise, PSF anisotropy).
Cost/effort vs upside
* Effort: Very high
* Upside: High only if it works, but probability is lower than the hybrid approach
Verdict
* Not the best primary path under time pressure. Use simulations as measurement (selection function), not as training truth.

Option 4: Semi-supervised / self-training on DR10 (pseudo-label loop)
What it is
* Train initial real-image model
* Apply to large DR10 pool
* Add high-confidence predictions back into training iteratively
Originality
* Moderate to high if done carefully with calibration and error control.
Biggest risks
* Error reinforcement. Reviewers will ask for strong safeguards and external validation.
* Needs careful uncertainty calibration and stopping rules.
Cost/effort vs upside
* Effort: Medium-High
* Upside: Medium-High, but riskier than Option 1/2
Verdict
* Good second-stage after you have a solid baseline and evaluation framework.

Option 5: Ensemble diversification via domain-specialized training (your Option 2)
What it is
* Train multiple models on disjoint domains (seeing, depth, morphology, sky regions)
* Combine via stacking/meta-learner
* Measure diversity and robustness
Originality
* Moderate. Novelty depends on doing it rigorously (diversity metrics, domain generalization tests, calibration).
Biggest risks
* If gains are small, paper may look like engineering.
* Reviewers will ask: “Why should I believe this improves science outcomes?”
Cost/effort vs upside
* Effort: Medium
* Upside: Medium, can strengthen Option 2 or Option 1
Verdict
* Best as an add-on to Option 1 or 2, not as the sole paper.

Honest ranking for “most promising and doable” right now
1. Option 1 (Hybrid): real-image detector + calibrated injection selection function Highest publishability with genuine novelty, even without new lens confirmations.
2. Option 2 (Real-image training + strong evaluation/audit novelty) Fastest working system. Publishable if the novelty is a rigorous audit, not a re-run.
3. Option 5 (Domain-split ensemble) as a bolted-on enhancement Good supporting contribution, not sufficient alone.
4. Option 4 (Semi-supervised loop) Potentially strong but needs careful guardrails; better after baseline.
5. Option 3 (Back to sim-trained primary detector) Highest risk under time pressure; too many realism dimensions can still break transfer.

What I would do in your situation (concrete)
* Commit to Option 1 as the core paper.
* Implement Option 2 as the baseline detector (because you need a real-image model anyway).
* Add one additional novelty axis:
    * either domain-split ensemble robustness, or
    * a rigorous failure-mode taxonomy with prevalence-aware uncertainty, or
    * spatial generalization and survey-condition dependence.
This is the strongest path to “MNRAS referee-proof” work with your current assets and timeline.
If you want, paste your current Phase 4c flux scaling and the DR10 cutout noise/ivar availability you have. I will specify a calibration loop with explicit acceptance tests (what plots must match, what tolerances, and what to do if they do not).


================
ok, for option 1 - how is it novel? didnt the authors already have selection functions  provide . can you check all the papers if they have injection realism. i still dont understand not convinced how you are connecing injection realism here. Write a full report on this with background , concrete goal, how is this novel, what steps we need to take, how to do injection realism, mistakes we made and to fix, concrete blueprint with every minor details


Background and why “Option 1: selection functions + failure modes” can still be novel
The DESI lens-finding paper line (Huang et al. 2019/2020/2021; Jacobs et al. DR9; Inchausti et al. 2025 arXiv:2508.20087) is primarily discovery and curation: train on real images, scan the survey, apply thresholds, then do vetting and (in later “Foundry” papers) spectroscopy/HST follow-up and lens modeling. That produces large candidate catalogs and follow-up samples, but it typically does not deliver what cosmology and population inference need:
* a quantitative detection probability (P(\mathrm{det} \mid \theta_E, z_l, z_s, \mu, \mathrm{PSF}, \mathrm{depth}, \mathrm{morphology}, \ldots))
* and a defensible uncertainty model for that probability, including how it changes with nuisance factors and what the algorithm systematically misses.
A paper that audits a real-image lens finder and produces reproducible selection-function products (with failure-mode stratification and uncertainty) can be publishable and useful, even if others have candidate catalogs. The novelty is not “we found lenses”; it is:
Novel deliverable: a rigorously measured selection function and failure taxonomy for a specific real-image pipeline, plus guidance on how selection biases propagate into downstream science (counts, quad/double mix, (\theta_E) distribution, etc.).
That is a different paper type than Huang et al.’s discovery pipeline papers and the “Strong Lens Foundry” follow-up/modeling papers.

Why injection realism is directly connected (and why your earlier sim-to-real miss matters)
Selection functions require ground truth: you need known positives that are representative, and you need a way to generate controlled positives across parameter space. There are only two practical ways:
1. Empirical-only (no injections): use confirmed real lenses and measure recall on those. Problem: your confirmed set is small and not representative; it is already selected by other pipelines and follow-up constraints.
2. Injection–recovery (with realism): inject lenses into real survey images and measure recovery as a function of parameters. This is the standard way to measure selection functions in astronomy when ground truth is limited. But it is only defensible if injections match reality in flux, morphology, PSF, noise, and selection context.
So injection realism is not an optional “nice-to-have”. It is the backbone of any claim like “completeness vs (\theta_E) and depth”. If the injected arcs are too bright, the measured completeness is meaningless. That is exactly the failure mode you discovered.

Do the DESI lens-finding papers already provide “selection functions”?
They provide operational thresholds and yields (and sometimes qualitative statements like “we are biased toward bright arcs / certain morphologies”), but that is not the same as a measured selection function over controlled injected truth with uncertainty, sensitivity tests, and published lookup products.
Your “Option 1” is novel if you commit to the hard part they largely do not publish as a central product:
* injection-recovery completeness surfaces (multi-dimensional)
* uncertainty and prior sensitivity
* explicit failure categories and causal hypotheses, validated with controlled tests

What reviewers will challenge (and how to preempt it)
Reviewer objection A: “Your injections are arbitrary and drive your conclusions.”
Defense plan
* Calibrate injections to observed DR10 photometric conditions and observed lens/candidate arc statistics, not arbitrary ADU.
* Publish “real-vs-injected” diagnostics: distributions of annulus SNR, color, surface brightness, arc width after PSF, and residual structure.
* Run prior sensitivity: repeat completeness with 2–3 alternative source priors (e.g., size/mag distributions) and show robustness bands.
Reviewer objection B: “Your positives are Huang-selected candidates, so this is circular.”
Defense plan
* Use your Tier-A spectroscopic/HST-confirmed anchors only for “empirical recall check”.
* Use candidates only as auxiliary (e.g., to tune realism metrics), with explicit label-noise handling and “do not claim absolute completeness from candidate-labeled truth”.
* Where possible, include non-Huang discovery channels (spectroscopic lens searches from DESI DR1, single-fiber lens searches, etc.) as independent anchors. Your list includes exactly those newer spectroscopic programs (DR1 pairwise search, single-fiber candidates), which is a strong anti-circularity move.
Reviewer objection C: “Your selection function is for your model only; it is not general.”
Defense plan
* Make that explicit, and then add value by showing which nuisance axes dominate and which failure modes are stable across architectures (e.g., ResNet vs EfficientNet vs a lightweight model). That becomes a general insight.
Reviewer objection D: “Your completeness vs parameters is underconstrained (low-N in bins).”
Defense plan
* Use hierarchical/Bayesian binomial or monotonic smoothing, and clearly mark “insufficient data” regions.
* Focus on the few axes that matter most (PSF FWHM, depth, (\theta_E), arc surface brightness), not 30-bin grids.

Concrete goal definition for an MNRAS-quality “Injection Realism + Selection Function” paper
Research question (tight, defensible)
RQ: For real-image strong-lens finders applied to DESI Legacy Survey DR10, what is the detection probability as a function of lens scale and observing conditions, and which failure modes dominate when arcs are at survey-limited surface brightness?
Primary methodology
1. Train a real-image lens finder (you already have the real-data pipeline direction).
2. Build an injection engine that injects realistically calibrated arcs into real DR10 cutouts across a designed grid of (\theta_E), source magnitude/size, and observing conditions.
3. Run injection–recovery and measure completeness surfaces with uncertainty.
4. Validate realism by matching injected-arc observables to independent real-lens/candidate distributions (not just visuals).
5. Produce a public selection-function product (tables + code) and a failure-mode atlas.
What makes it “original”
* The calibrated realism layer (your unique contribution born from the crisis) plus the quantitative selection surfaces.
* A principled decomposition of failure modes into: PSF-limited blending, core-subtraction sensitivity, contaminant families, and low-SB arc loss.
* A reproducible evaluation set design (anchors + contaminants + injection grid) released alongside.

How to fix Phase 4c (spark_phase4_pipeline) to produce realistic injections
Your earlier failure was essentially “unanchored flux units”. The fix is an explicit photometric calibration loop:
Step 0: Decide what you will match (observable targets)
Define 6–10 measurable targets you will match between injected lenses and real DR10 lenses/candidates:
* annulus SNR in (r) and (g) (your arc-annulus gate)
* arc surface brightness distribution (mag/arcsec(^2)) after PSF
* arc color (g−r, r−z)
* arc width (PSF-convolved) proxy
* residual amplitude after a simple lens-galaxy subtraction proxy
* distribution of arc radial location relative to center (in pixels)
Step 1: Put injections in real photometric units
For Legacy Surveys, fluxes are commonly in nanomaggies (Tractor conventions). Your injection should generate sources in AB magnitudes and convert to nanomaggies per band, then render through PSF and pixel scale.
* Sample unlensed source magnitudes from a realistic prior (deep field catalog or an empirical prior from LS/HSC; if you cannot, use a conservative parametric prior with sensitivity tests).
* Apply magnification (\mu) from the lens model and surface brightness conservation.
* Convolve with per-cutout PSF (or a realistic PSF model tied to DR10 conditions).
* Add noise consistent with depth for that cutout (use depth maps or empirically measure background + noise in outer annulus).
Step 2: Inner image handling (critical)
You already learned inner images are physically real but usually not observable in DR10. You need an explicit, reviewable choice:
* Either include inner images but down-weight visibility via realistic source profiles + PSF + noise so they are typically buried, or
* Mask/suppress inner image contribution with a justified “observational selection” argument.
Do not hide this. Make it a named ablation: “inner-image included” vs “inner-image suppressed” and show impact on completeness and shortcuts.
Step 3: Spark/EMR operational realism changes
For expensive map jobs, implement:
* deterministic seeding per object ID (reproducibility)
* checkpointed output shards with manifest and resume logic (you already asked for this pattern)
* per-partition QA summaries (arc SNR histogram, SB histogram) to fail fast if distributions drift
Step 4: Close the calibration loop with real data
Use your 100 confirmed anchors + a subset of candidates to calibrate the injection prior until the observable targets match within tolerance bands. This is not “training on them”; it is calibrating the simulator to the survey.
Deliverable: “Injection realism report” as a paper appendix with quantitative plots.

How this integrates end-to-end with original research (and why it is stronger than “just train on real lenses”)
Real-lens training alone gives you a working finder but weak scientific claims about completeness.
Real training + realistic injection–recovery gives you:
* a finder, plus
* a selection function that can be used in population inference, survey forecasting, and bias auditing.
That is the bridge back to your original thesis (“which realism knobs matter, and what selection biases result”), but now grounded in survey-calibrated realism rather than arbitrary sims.

Comparison to the other options, honestly
Option A: Pure real-data training (replicate Huang-style)
* Upside: fastest to “works in DR10”; lowest risk.
* Novelty risk: high (can look like replication unless you add a new angle).
* Best use: as the baseline model that your selection-function work audits.
Option B: Selection function + failure modes (with calibrated injections)
* Upside: high publishability as a methods/audit paper if executed rigorously.
* Risk: injection realism is hard; reviewers will scrutinize assumptions.
* Best use: this is the most natural “original” paper under your constraints.
Option C: Ensemble diversification/domain-specialized training
* Upside: moderate novelty if you show real, measurable gains in rare regimes (bad seeing, shallow depth, certain morphologies) and connect to selection function improvements.
* Risk: can become engineering without scientific payoff unless tightly tied to “why diversity matters” and “what failure modes it fixes”.
Option D: Spectroscopic-first lens discovery (DESI DR1 pairwise/single-fiber)
* Upside: very original if you can contribute a new search criterion or efficiency improvement.
* Risk: requires spectroscopy domain expertise and access patterns; may exceed 4-week window unless you scope narrowly.
Given your current assets (real candidates, anchors, contaminants, working training infra, Phase 4c generator), the most promising and doable is:
1. Selection function + failure modes with calibrated injection–recovery, anchored by a real-trained baseline model.
2. Secondary: targeted ensemble diversification only if it demonstrably improves completeness in specific hard regimes identified by the selection-function maps.

Concrete blueprint with “minor details” (what you actually do next)
Milestone 1 (2–3 days): Baseline real-image model and evaluation protocol
* Train a baseline model on real positives/negatives.
* Lock evaluation: anchors (Tier-A only), contaminants by category, calibration curves.
* Establish “fail-fast” metrics: arc-annulus separability, contaminant FPR by class, and stability across sky regions.
Milestone 2 (4–7 days): Injection realism calibration (Phase 4c fix)
* Implement nanomaggies/magnitude-calibrated injection.
* Add per-cutout PSF and noise matching.
* Run a small injection grid (e.g., 20k injections) and compare observable targets to real anchor/candidate distributions.
* Iterate until within tolerances; freeze priors and document.
Milestone 3 (7–10 days): Full injection–recovery selection function
* Designed grid over: (\theta_E), source mag, source size, PSF, depth.
* Use stratified sampling across DR10 conditions (not random).
* Compute completeness with uncertainty; publish lookup tables.
Milestone 4 (4–7 days): Failure mode taxonomy and ablations
* Identify bins where completeness collapses.
* For each collapse region, build a gallery and test causal hypotheses:
    * PSF-blend limited
    * galaxy subtraction sensitivity
    * contaminant-driven confusion
    * inner-image sensitivity
* Include at least one “fix” experiment (e.g., scheduled masking, residual preprocessing) and show how it shifts the selection function.
Milestone 5 (paper week): Write and package artifacts
* Release: code, selection-function tables, evaluation catalogs, and QA plots.
* Write: limitations, prior sensitivity, and circularity mitigation.

“Mistakes we made” section (to include in the paper as a strength, not a weakness)
State it plainly:
* Initial injections were not photometrically calibrated to DR10; arcs were unrealistically high SNR.
* We introduced quantitative realism gates (arc-annulus SNR, SB distributions, color, residual amplitude) and rebuilt the simulator to match observed distributions.
* We report prior sensitivity and do not claim absolute population completeness outside the validated realism envelope.
Reviewers generally accept this if you demonstrate the corrected approach is controlled and transparent.

What would be a clear GO / NO-GO before you invest heavily again
GO if:
* Injected-vs-real diagnostics match within pre-specified tolerances for the observables you will claim selection functions over.
* Injection–recovery completeness maps are stable across multiple sky regions.
* Contaminant FPR analysis identifies known confuser classes and is not dominated by obvious artifacts.
NO-GO if:
* You cannot match arc surface brightness and annulus SNR distributions without arbitrary tuning that breaks other observables.
* Completeness changes wildly with small injection choices (priors dominate signal) and you cannot bound it credibly.

If you want, paste (or point to) the specific arc-flux code block in spark_phase4_pipeline that currently does random(100, 500) and the format of your per-cutout depth/zeropoint metadata. I will give an explicit replacement design (in nanomaggies/AB mags) and the exact QA histograms to log per Spark partition to enforce realism.


=======

CONTINUATION - Strategic Planning for MNRAS-Level Publication
Context Recap:
We've committed to your recommended Option 1 (Hybrid): real-image detector + calibrated injection selection function. You said this has "highest publishability with genuine novelty, even without new lens confirmations."
Our understanding of the path:
Baseline: Train real-image detector following Paper IV methodology closely → gets us to their performance level
Novel contribution: Calibrated injection-recovery → rigorous selection function with failure mode taxonomy
Stretch goal: Actually find lenses they missed (not required, but would strengthen the paper)
We're now implementing the EMR job for negative sampling and cutout generation. Before we finalize the schema and build, we want to ensure we're making the right strategic decisions.
PART A: Strategic Questions for Publication Success
1. Minimum Viable Novelty
You mentioned selection function + failure modes is publishable. But:
Is selection function alone sufficient for MNRAS, or do we need a secondary contribution (e.g., domain-split ensemble, contaminant taxonomy)?
What's the minimum scope that reviewers will accept as "not just replication"?
Should we explicitly frame this as "a methods paper" vs "a discovery paper" vs "a hybrid"?
2. Differentiating from Existing Work
The DESI Strong Lens Foundry papers (I-V) are doing extensive follow-up, HST imaging, and GIGA-Lens modeling. How do we position ourselves as complementary rather than redundant? Specifically:
They have confirmed lenses and detailed models - we don't
They have spectroscopy - we don't
What unique value does our selection function provide that their papers don't?
3. The "So What" Question
Reviewers will ask: "Who needs this selection function and why?" What's the strongest framing?
Cosmological lens statistics (need bias correction for θ_E distribution, quad/double ratios)?
Survey planning (predicting yield for Rubin/Euclid)?
Follow-up prioritization (which candidates are worth spectroscopy)?
Honest assessment of ML lens finders (what they actually detect vs claim)?
4. Improvements Beyond Paper IV
If we follow Paper IV exactly, we get their results (which is fine for baseline). But what modifications could give us an edge?
Better negative sampling strategy?
Different architecture choices?
Ensemble methods (your Option 5)?
Better calibration/augmentation?
What would you recommend we try that has high probability of improving over their baseline?
PART B: Critical Methodological Questions
5. Injection Realism - The Core Challenge
This was our crisis. You said injections must match reality in "flux, morphology, PSF, noise, and selection context."
What are the specific quantitative acceptance criteria before we trust our injections?
What plots/diagnostics must match within what tolerances?
How do we calibrate source magnitude/size distribution without circular logic (using ML-found candidates)?
Should we defer injection realism to Phase 2 and focus on real-data training first?
6. Training Labels - Handling Uncertainty
We have:
434 Tier-A "confident" (spectroscopically graded)
4,666 Tier-B "probable" (likely but not confirmed)
1,372 confirmed lenses in Paper IV training set (SLACS, BELLS, SL2S, etc.)
Questions:
Should we match Paper IV's 1,372 exactly, or use our 434+4,666?
How should we handle Tier-B label noise? (label smoothing, sample weighting, exclude from training?)
Is there a risk of "training on the test set" if we use DESI candidates that overlap with Paper IV's?
7. Negative Sample Representativeness
Paper IV emphasizes 100:1 negative:positive ratio and stratification by z-band exposure. But:
Should negatives match the positive host galaxy distribution (LRG-like) or the deployment population (all galaxies)?
Paper IV scans 43M galaxies at deployment - should our negatives sample from this same population?
How do we ensure our negatives include the hard confusers (rings, spirals, mergers, edge-on disks, blue clumpy star-formers)?
8. Evaluation Protocol - Avoiding Circularity
You warned about "you trained on candidates found by prior ML, so you reproduced their biases."
What's the cleanest evaluation design to avoid this?
Should we use the newer DESI spectroscopic lens searches (single-fiber, pairwise from DR1) as fully independent validation?
How do we handle the fact that our Tier-A anchors are also found by similar methods?
PART C: Technical Implementation Questions
9. Paper IV nobs_z Bins
What are the exact bin edges Paper IV uses for z-band exposure stratification? I couldn't extract these precisely.
10. Negative Galaxy Selection Criteria
Should we use:
All galaxies with TYPE in [SER, DEV, REX, EXP] and z < 20 (matching deployment population)?
Additional LRG-like color cuts to match positive host properties?
Something else?
11. Exclusion Radius for Known Lenses
When sampling negatives, what cross-match radius should we use to exclude known/candidate lenses? Positives span θ_E from 0.5" to 3.0".
12. Cutout Size
64×64 (16.4") vs 96×96 (24.6")? What does Paper IV use, and what should we use for θ_E up to 3"?
13. Per-Cutout vs Brick-Average PSF
Evaluate PSF FWHM at exact cutout center (more accurate) vs use brick-average (faster, already in sweeps)?
PART D: Questions We Might Be Missing
14. Timeline and Scope Prioritization
Given limited time, what's the critical path? What can be deferred to "future work" without killing the paper?
15. Failure Modes We Should Anticipate
Based on your experience with lens-finding papers, what are the most common reviewer objections we should preemptively address in the paper design (not just the writing)?
16. Model Architecture Choices
Paper IV uses ResNet + EfficientNet ensemble with meta-learner. Should we:
Replicate exactly for fair comparison?
Use a simpler baseline (just ResNet18) for cleaner ablations?
Try something different that might give us an edge?
17. Computational Reality Check
We have single-GPU training capability and EMR for data processing. Is there anything in our plan that's unrealistic for this compute budget?
18. What Question Should We Be Asking That We're Not?
You have broader context on what makes lens-finding papers succeed or fail. What critical question are we missing at this decision-making stage?
PART E: Proposed Schema (for review)
Level 1: Galaxy Catalog Manifest (from sweeps)
Core: galaxy_id, brickname, ra, dec, type
Stratification: nobs_z, nobs_z_bin, type_bin
Photometry: flux_g/r/z/w1, mag_g/r/z, colors (r_minus_z, z_minus_w1, g_minus_r)
Conditions: psfsize_g/r/z, psfdepth_g/r/z, galdepth_g/r/z, ebv
Quality: maskbits, fitbits, mw_transmission_g/r/z
Spatial: healpix_64, healpix_128, region_split
Provenance: sweep_file, row_index, pipeline_version, git_commit
Level 2: Cutout Dataset (Parquet with NPZ)
Core: galaxy_id, stamp_npz, stamp_size, bandset, cutout_ok
Quality: has_nan, nan_pixel_count, bad_pixel_frac, wise_brightmask_frac
Per-cutout conditions: psf_fwhm_g/r/z (evaluated at center), psfdepth_r_center
Shortcut detection: core_brightness_r, outer_brightness_r, mad_r, median_r, clip_frac_r
Normalization: mean_r, std_r, percentile_1_r, percentile_99_r
Split: split_assignment (train/val/test based on HEALPix)
Is this schema complete for our goals? What's missing?
Bottom line question: Given where we are, what's your honest assessment of:
Probability of MNRAS acceptance if we execute this well
The 2-3 things most likely to cause rejection
The single highest-leverage improvement we could make to our plan

## 0) First: what “Option 1 (hybrid)” is, in one sentence

Train a **real-image classifier** on DR10 cutouts (so the detector is distribution-matched), then measure a **quantitative selection function** using **calibrated injections** into DR10 cutouts (so you can say what kinds of lenses the model would or would not detect, as a function of conditions and lens/source properties).

That combination is not the same as “training on simulations.” It is “using injections to measure completeness,” which is a different scientific product.

---

## 1) Minimum viable novelty that can survive MNRAS review

If executed well, “selection function + failure modes” is publishable **only if** you deliver at least one of these concrete, citable artifacts (minimum scope):

1. **A quantitative completeness map**: (C(\theta_E, \mathrm{PSF}, \mathrm{depth}, \mathrm{host\ type})) with uncertainty, computed by injection-recovery on DR10-like images, with clearly stated detectability limits.
2. **A failure-mode taxonomy tied to measurable covariates**: not just a gallery, but “FPR spikes when X; FN spikes when Y,” with reproducible definitions and counts.
3. **A reproducible “audit protocol”** that others can reuse (code + standardized gates), and a public lookup table or model card.

Most DESI lens-finder papers emphasize discoveries and follow-up; the strong-lensing VAC and Foundry effort emphasize spectroscopic confirmation and cataloging, not a reusable, injection-calibrated completeness surface for a specific detector. The DR1 strong-lensing catalog description itself emphasizes cataloging and spectroscopic observation, not detector completeness surfaces. ([data.desi.lbl.gov][1])

If you deliver (1) + (2) + release code and a lookup table, that is enough novelty to be “methods + characterization,” even without new confirmations.

---

## 2) How to position vs Strong Lens Foundry papers (complement, not redundant)

**What they have (and you do not):**

* Spectroscopy, HST, detailed modeling (e.g., GIGA-Lens workflows in Foundry series), and confirmed sample building.

**What you can have that they mostly do not provide as a standalone deliverable:**

* A **detector-specific selection function** on DR10 imaging, calibrated by injection-recovery and expressed in survey-observable coordinates (PSF, depth, nobs, morphology type, sky background, masking), plus failure-mode decomposition across confusers.

Positioning language (honest and defensible):

* “They build samples and confirm lenses; we quantify what a given image-based detector would *select* and *miss* in DR10, and why, which is necessary for downstream statistical inference and for comparing detectors.”

---

## 3) “So what?” framing that reviewers accept

Pick one primary framing and keep it tight:

### Best high-impact framing (most defensible with your data)

**B: “Where do ML lens finders fail, and why?”**
Deliver: completeness and false-positive structure vs conditions + morphology, plus a reproducible audit.

Downstream impact paragraphs that are honest:

* Lens statistics require selection correction; completeness varying with seeing/depth biases (\theta_E) distributions and inferred lens-population parameters.
* Follow-up programs (spectroscopy/HST) need prioritization; your analysis tells which candidates are likely real vs confuser-dominated regimes.

Avoid claiming cosmology constraints unless you actually propagate selection into an inference.

---

## 4) What you should do beyond Paper IV, with highest chance of success

### High-probability improvements (not “moonshots”)

1. **Independent evaluation sets** from spectroscopic DESI searches (single-fiber and pairwise DR1/DR2-type searches in your list) to break “trained-on-ML-found-candidates” circularity.
2. **Domain-split training** (your Option 2) not as a big new architecture, but as an ablation that shows selection-function variation across domains (seeing, depth, nobs, morphology).
3. **Calibration and reliability**: show the score is not just a ranker; quantify calibration drift across strata and report ECE/RC curves.

### Lower-probability “edge” ideas (do only if time remains)

* Foundation-model feature extractors (DINOv2-like) plus small head: can work, but adds complexity and reviewer skepticism unless you benchmark cleanly.

---

## 5) Injection realism: how to do it, and how to prove it

You asked for concrete acceptance criteria. These are the “must-pass” diagnostics before you trust injections:

### A. Photometric realism (flux scale)

Goal: injected arcs live in the same **magnitude/SNR regime** as real arcs in DR10 cutouts.
Minimum acceptance:

* Match the distribution of **arc annulus SNR** (your annulus gate) between injections and a curated real-lens anchor set, within a factor of ~2 in median and similar tails (especially the low-SNR tail).
* Match per-band **color distribution** of sources (g-r, r-z) to plausible background populations, or at least bracket it with two scenarios.

### B. PSF and noise realism

Minimum acceptance:

* Inject at the **per-cutout PSF** (band-specific) and add noise using **measured background statistics** from the same cutout (robust MAD-based noise) rather than a global noise constant.
* Verify residuals: background-only pixels after injection have the same noise histogram (KS test or QQ plot) as background pixels in non-injected cutouts.

### C. Morphology realism

Minimum acceptance:

* Use source morphologies that can produce arc-like structure at DR10 resolution. If you cannot justify COSMOS morphologies for DR10, treat that as an uncertainty and bracket with “smooth Sersic” vs “clumpy” models.

### D. Context realism

Minimum acceptance:

* Inject into **the same host population you deploy on** (if scanning 43M galaxies, inject into a representative subset stratified by host type, magnitude, and masking regime).

### E. Audit for shortcuts (non-negotiable)

Run gates on injected datasets:

* Annulus-only classifier should be strong (good signal).
* Core-only and radial-profile shortcuts should be weak after your mitigation steps.

**Key point:** injection realism is not “believing the simulator.” It is “passing a battery of matched-distribution diagnostics.”

---

## 6) Labels and circularity: what to do with 5,104 ML-found candidates

You have three label tiers with different epistemic status. The safest approach:

1. **Training positives:** prioritize truly confirmed lenses from literature (Paper IV used real confirmed lenses; that principle is sound).
2. **Candidates (Tier-B):** treat as **noisy positives**, not truth. Use:

   * sample weighting by grade (or probability), and/or
   * label smoothing (e.g., confident=0.95, probable=0.7–0.8), and
   * exclude any candidates that appear in your evaluation anchor set or in an “independent validation set.”
3. **Evaluation:** must include at least one set not selected by the same pipeline family. The DESI spectroscopic-selected candidate sets in your list are ideal for this.

This directly answers the reviewer objection: “You trained on ML-found candidates so you learned their bias.” Your reply is: “We evaluate on independent spectroscopic selections and quantify domain shift.”

---

## 7) Negative sampling representativeness: what to match

Two different goals, so you need two negative pools:

### Pool N1 (deployment-representative)

Sample from the same galaxy population you will score (broad TYPEs, magnitudes, maskbits), stratified by observing conditions (nobs, depth, PSF). This gives realistic base-rate conditions.

### Pool N2 (hard confusers)

Your contaminant catalog is too small to approximate prevalence, but it is perfect for *conditional* FPR by confuser type. Keep building this pool and report FPR stratified by category, not a single headline FPR.

Do not try to claim overall precision without prevalence.

---

## 8) Paper IV nobs_z bins, cutout size, and other specifics

### nobs_z bins

I cannot reliably extract exact bin edges from the PDF text programmatically in this environment (tables/figures often do not extract cleanly). Do not block on “exact Paper IV bin edges.” The defensible move is:

* choose simple bins (e.g., {1,2,3,4+}) and show sensitivity is stable to modest binning changes,
* report robustness: re-bin and show selection surface changes are small.

### Cutout size

For (\theta_E \le 3\arcsec), you want arcs well inside the stamp with context. 64×64 at 0.262″/pix is ~16.8″ across; that is usually enough, but 96×96 or 101×101 gives more sky annulus and reduces edge artifacts. If you can afford it, use ~101×101 for training/eval consistency with your already-downloaded set.

---

## 9) Exclusion radius for negatives

Use a radius that covers:

* centroid errors,
* multi-component systems,
* and the full image separation scale.

A practical choice:

* **exclude within 5″** of any known/candidate lens coordinate for negative sampling.
  Then report sensitivity to 3″ vs 5″ as a robustness check.

---

## 10) Per-cutout PSF vs brick-average PSF

For selection-function work, reviewers will prefer correctness over speed:

* use **per-cutout (position-evaluated) PSF** when feasible; fall back to brick averages only if you quantify the error and show it does not change results materially.

---

## 11) Your proposed schema: what’s missing for Option 1

Your schema is already close. Add:

1. **Provenance for cutouts**

* cutout URL, download timestamp, layer version, bands present, any resampling flags.

2. **Injection metadata (for selection function)**

* injected source mag per band, color, half-light radius, Sersic n/clumpiness mode
* lens mass model params ((\theta_E), q, shear if used), source position relative to lens
* magnification proxy, arc annulus SNR, arc flux fraction, “detectability label” from your arc-visibility metric

3. **Masking/contamination descriptors**

* fraction of pixels with maskbits above thresholds
* nearby bright star distance and Gaia magnitude (for spikes/ghost likelihood)

4. **Spatial split keys**

* healpix at multiple nside plus “region_id” to support spatial CV and avoid leakage.

---

## 12) Critical path (what must happen first) vs defer

### Must-do in the next 1–2 weeks

1. Train real-image baseline detector with clean splits (spatially disjoint).
2. Build independent evaluation set (spectroscopic-selected candidates).
3. Implement injection realism diagnostics and iterate until they pass.

### Can be deferred to “future work” without killing the paper

* foundation models
* complex meta-learners
* full-survey inference at 43M scale (you can demonstrate on a large subset)

---

## 13) What reviewers will most likely reject you for (top 2–3)

1. **Circular evaluation** (training on ML-found candidates and evaluating on the same family of candidates).
   Fix: independent spectroscopic-selected validation, spatial CV, and a clear statement of label uncertainty.

2. **Unvalidated injections** (claims of completeness based on injections that are not demonstrably DR10-like).
   Fix: publish the acceptance diagnostics and bracket uncertainties.

3. **Overclaiming precision/selection** without prevalence and without enough confirmed anchors in each stratum.
   Fix: report conditional metrics, bootstrap/Bayesian binomial intervals, and clearly label “insufficient data” bins.

---

## 14) Probability of MNRAS acceptance if executed well

* If you deliver: real-image baseline + independent validation + injection-calibrated selection surface + failure-mode breakdown + released code/data products: **moderate-to-high**.
* If you skip independent validation or do not validate injections quantitatively: **low**.

Single highest-leverage improvement: **independent validation set** (spectroscopic-selected candidates) plus **spatial CV**.

---

## 15) Answer to the core strategic question: should you fix Phase 4c simulations and return to the original plan?

### Honest comparison of options (publishability vs risk)

#### Option 1 (hybrid: real detector + calibrated injections for selection)

* **Publishability:** highest
* **Risk:** moderate
* **Why:** detector is real-data matched, injections are used only for completeness measurement with explicit validation.

#### “Fix Phase 4c and train on simulations again”

* **Publishability:** lower unless you can *prove* realism across many axes
* **Risk:** high (reviewers will attack domain shift and calibration assumptions)
* **When it’s worth doing:** only as a **secondary component** to Option 1, used to explore parameter space after you have a real-data baseline.

#### Train only on real candidates (replicate Huang et al.)

* **Publishability:** low unless you add a new independent contribution
* **Risk:** moderate (circularity, label noise)
* **Good for:** baseline and sanity checks.

**Recommendation:** keep “Phase 4c realism repair” as a *selection-function engine* and stress-test tool, not the primary training source. That gives you scientific value without betting the paper on simulations.

---

## 16) Concrete blueprint for Option 1 (hybrid) with injection realism integrated

### Milestone M1: Real-image baseline (Week 1–2)

* Data: confirmed lenses + carefully sampled negatives (N1), strict spatial splits.
* Output: baseline detector with calibrated scoring curves by stratum.

### Milestone M2: Independent validation (Week 2)

* Data: spectroscopic-selected lens candidates (single-fiber / pairwise) as “silver positives” for qualitative validation + subset with confirmations where available.
* Output: domain shift analysis.

### Milestone M3: Injection realism calibration (Week 2–3)

* Implement calibrated injection pipeline:

  * magnitude-based flux scaling using DR10 zeropoints
  * per-cutout PSF and local noise estimation
  * acceptance diagnostics (SNR distributions, noise histograms, annulus gate behavior)
* Output: “injection realism report” as a paper appendix + code.

### Milestone M4: Selection function surfaces (Week 3)

* Run injection-recovery across a design grid:

  * (\theta_E), PSF, depth/nobs, host type, mask regime
* Output: completeness surfaces with uncertainty; show where the detector is blind.

### Milestone M5: Failure modes and contaminant analysis (Week 3–4)

* Score contaminant categories; quantify FPR by type and conditions; produce interpretable examples.
* Output: failure taxonomy + prioritization rules for follow-up.

### Milestone M6: Paper assembly (Week 4)

* Methods, datasets, audit protocol, results, limitations, and released artifacts.

---

## 17) Code bundles you already have

* [Download DR10 real-data pipeline + training/eval code](sandbox:/mnt/data/dhs_dr10_realdata_code.zip)
* [Download scheduled masking module](sandbox:/mnt/data/scheduled_masking_module.zip)

If you want, I can map each file in the zip to the milestones above and specify the exact command sequence to reproduce every table/figure in the draft.

---

## 18) Directly addressing “injection realism vs selection function novelty” (why it is connected)

The novelty claim is not “we injected arcs.” The novelty claim is:

* “We provide a **validated injection-recovery completeness** for a real-data-trained detector, as a function of DR10 observing conditions and morphology, and we show exactly which confusers dominate FPR and in which regimes.”

That is what makes selection function meaningful and scientifically usable.

If you do not validate injection realism quantitatively, you cannot claim a selection function. That is why injection realism is not optional and why it is central to Option 1.

---

[1]: https://data.desi.lbl.gov/doc/releases/dr1/vac/strong-lensing/ "DESI Strong Lensing Catalog - DESI Data"
========================



CONTINUATION - Complete Implementation Clarifications Before Execution
We've reviewed your comprehensive blueprint and are ready to execute. However, after auditing our existing codebase against your recommendations, we found several conflicts and gaps that need clarification before we write code. We want to implement correctly the first time to avoid costly EMR reruns.
SECTION A: Negative Sampling Strategy (7 questions)
A1. Two-Pool Design
You recommend two pools (N1: deployment-representative, N2: hard confusers). Our current code uses a single is_control pool.
What ratio should N1:N2 be in the final training set?
For the 100:1 overall negative:positive ratio per (nobs_z, type) bin, how does this break down between N1 and N2?
Should control samples (same galaxy, no injection) be a separate third pool, part of N1, or deprecated?
For N2 (hard confusers), should we curate manually, sample automatically via morphology/color cuts, or both?
A2. Defining Hard Confusers
What specific Tractor catalog criteria define each confuser category (rings, spirals, mergers, edge-on disks, blue clumpy star-formers) for automatic sampling?
A3. Exclusion and Selection
For 5" exclusion around known lenses - is this from centroid, or should we add θ_E?
What maskbit values should exclude a galaxy from being a negative?
SECTION B: Spatial Splits (4 questions)
Our code uses hash-based splitting. You recommend HEALPix.
What nside should we use (64 gives ~0.84 deg² cells, 128 gives ~0.21 deg²)?
Should we stratify HEALPix cell allocation by observing conditions to ensure balanced PSF/depth per split?
For 70/15/15 split, approximately how many HEALPix cells per split for statistical power?
How do we ensure positives are in correct spatial splits?
SECTION C: Label Handling (8 questions)
C1. Implementation
Should tier-based labels be implemented as label smoothing (target = weight) or sample weights in loss function?
Exact values: Tier-A = 0.95, Tier-B = 0.75, Literature confirmed = 1.0?
C2. SLACS/BELLS Problem
Our lessons_learned notes SLACS/BELLS have low DR10 visibility (arcs often invisible).
Should SLACS/BELLS be: included with full weight, down-weighted, excluded from training (evaluation only), or stress-test only?
C3. Paper IV Alignment
Paper IV used 1,372 confirmed lenses. Do we have access to this exact list? If not, how to reconstruct?
C4. Separation
What's the cleanest way to ensure no overlap between training positives and evaluation anchors? Spatial separation? Explicit ID exclusion? Both?
SECTION D: DESI Spectroscopic Catalogs (6 questions)
You strongly recommend using spectroscopic-selected candidates as independent validation.
Are DESI DR1 spectroscopic lens catalogs (Single Fiber Search arXiv:2512.04275, Pairwise Search arXiv:2509.16033) publicly available?
What's the data access URL/format?
Do we need DESI collaboration membership?
How many have spectroscopic confirmation vs just "spectroscopically selected"?
How should we use them: independent validation only, silver positives for training, or something else?
How do we handle overlap with our imaging candidates?
SECTION E: Injection Realism (14 questions)
E1. Source Properties
What distribution should we sample source magnitudes from? What magnitude range?
What source morphology model: smooth Sersic, clumpy, COSMOS? Should we bracket with multiple as sensitivity test?
E2. PSF and Noise
Moffat PSF - what beta parameter? From DR10 metadata or fixed?
For MAD-based noise estimation, measure from outer annulus (r > 20 pixels)? What if outer annulus has sources?
E3. Inner Images
For inner image handling, which should be primary: include with realistic visibility, explicitly suppress, or run both as ablation?
E4. Acceptance Thresholds (Critical)
You specified diagnostics but not exact thresholds:
Arc Annulus SNR matching: "Within factor of ~2 in median" - is this 0.5x to 2x range?
"Similar tails" - how quantified? KS test p-value? 10th percentile within X%?
Noise histogram KS test p-value threshold: p > 0.05? p > 0.1?
Color distribution tolerance on (g-r, r-z): ±0.1 mag? ±0.2 mag?
GO/NO-GO decision: all diagnostics must pass, or weighted score?
SECTION F: Cutouts (4 questions)
Our Phase 4c uses 64×64 (16.4"). You suggested 101×101. Should negatives match existing positives (64×64) or use larger?
If we change to 101×101, must we regenerate all positive cutouts for consistency?
Should we require all three bands (g, r, z) or allow missing bands?
How handle cutouts where one band has NaN but others OK?
SECTION G: Training Configuration (6 questions)
ResNet18 vs EfficientNet-B0 - which as primary baseline?
For fair Paper IV comparison, should we implement their exact ensemble, or is single architecture cleaner?
Minimum epochs for "defensible" baseline?
Batch size for 24GB GPU VRAM with 64×64 3-channel images?
What augmentations are appropriate? Risk of introducing shortcuts with brightness/scaling augmentations?
SECTION H: Selection Function (7 questions)
What axes and resolution for selection function grid?
θ_E: 0.5-3.0" in what steps?
PSF: what range and resolution?
Depth: what binning?
Minimum injection points per grid cell for statistical power?
How handle cells with very few injections: hierarchical smoothing, mark insufficient, merge?
Bootstrap or Bayesian binomial for uncertainty?
How many bootstrap iterations?
What confidence level for error bars?
SECTION I: Milestone Sequencing (2 questions)
Your milestones suggest training baseline in Week 1-2. But training needs negatives (EMR job), proper splits (HEALPix), label handling. Should we:
A) Build EMR job first → Generate negatives → Then train baseline
B) Use existing Phase 4c controls as temporary negatives → Train quick baseline → Then build proper EMR job
What's the critical path? What can run in parallel?
SECTION J: Paper Strategy (5 questions)
What figures are "must-have" for MNRAS acceptance?
What claims should we explicitly AVOID?
How exactly should we frame "following Paper IV methodology" as not being replication?
One-sentence novelty statement for abstract?
MNRAS vs ApJ vs AAS Journals preference for this contribution type?
SECTION K: Computational Constraints (2 questions)
Single-GPU (24GB) + EMR - is anything in the plan unrealistic?
Approximate EMR cost for 500K negative cutouts + injection grid?
SECTION L: Gap Check (3 questions)
What critical question are we NOT asking that would cause reviewer rejection?
Single highest-risk failure mode in our plan?
If scope must be cut, what defer to "future work" vs "must have"?
Total: 59 implementation-critical questions across 12 sections.
Please provide as much detail as possible. We want to implement correctly the first time.
## SECTION A: Negative sampling strategy

### A1. Two-pool design (N1 deployment negatives, N2 hard confusers)

**Recommended training mix (practical and defensible):**

* **N1 : N2 = 85 : 15** within the negative class.

  * Rationale: N1 sets the score distribution and base-rate behavior for the deployment population. N2 prevents “easy-negative” training and improves specificity on known confusers without dominating gradients.
* Maintain your **overall 100:1 negative:positive** ratio in the *training stream*, but implement N2 as an **oversampled stream** rather than trying to match its true prevalence.

**How to implement with 100:1 per (nobs_z, type) bin:**

* For each stratum (nobs_z_bin × type_bin):

  * draw **~85 negatives** from N1 (deployment representative within that stratum)
  * draw **~15 negatives** from N2 (confusers, not necessarily within that exact stratum; but record their stratum so you can report FPR by stratum later)

**Controls (same galaxy “no injection”)**

* For real-image training, **deprecate paired controls as a primary negative source**.
* Keep them as a **diagnostic pool** only (shortcut gates and sanity checks), not part of the main negative training distribution.
* If you must use them temporarily (see Section I), treat them as part of **N1**, but phase them out as soon as N1 is built.

**How to source N2**

* **Both**: automatic high-recall mining + small manual curation.

  * Automatic yields scale.
  * Manual curation ensures you include weird artifacts (sat trails, ghosts) that catalogs miss.

### A2. Tractor criteria for hard confusers (automatic sampling)

Tractor catalogs are not built to label “ring galaxy” directly, so you cannot get high purity from Tractor-only rules. Use a two-step approach:

**Step 1: high-recall candidate mining (cheap, Tractor-only)**

* **Edge-on disks**: TYPE in {EXP, DEV} plus high ellipticity proxy from shape parameters (if available), or axis ratio cuts if Tractor provides them.
* **Blue clumpy star-formers**: use colors and size:

  * low (g-r), low (r-z), relatively extended in g/r (if size proxies exist), and not masked.
* **Spiral/ring/merger proxies** from Tractor alone are weak. Do not pretend they are clean.

**Step 2: confirm confuser morphology via image-based filtering**

* Run a lightweight image model (even your baseline CNN with “non-lens” target) or a simple embedding + clustering, then manually label a few hundred.
* Practical: create N2 by scoring a large pool and selecting high-scoring negatives that are not known lenses, then manually categorize.

Net: Tractor helps you avoid garbage; images determine confuser categories.

### A3. Exclusion and maskbits

**Exclusion radius**

* Use **5 arcsec from catalog position** as the base exclusion.
* Also exclude if within **(5″ + 2×θE_max)** of any known lens center when θE is available.

  * With θE_max = 3″, this becomes ~11″. This reduces “nearby arc” leakage.

**Maskbits**
Exclude negatives if any of these are present in a central region (e.g., r < 8 px):

* bright star mask / saturation
* bleeding trails
* large contiguous masked fraction
  Also exclude if **masked pixel fraction** > 1–2% in the stamp center, and track mask fraction as a covariate.

Do not hardcode maskbits without checking DR10 documentation for meaning. Treat maskbits as a feature for analysis even if you filter.

---

## SECTION B: Spatial splits (HEALPix)

### B1. nside choice

Use **nside=128** for split assignment.

* Smaller cells reduce leakage via survey systematics and local background.
* nside=64 is acceptable, but 128 is safer for “independent regions” claims.

### B2. Stratify HEALPix by conditions?

Do **not** hard-stratify the split by PSF/depth. It can accidentally entangle selection with footprint boundaries.
Instead:

* assign splits by HEALPix deterministically
* then **report** and **verify** balance: distribution of PSF/depth/nobs per split.

### B3. 70/15/15 and number of cells

You do not need a target number of cells, you need:

* enough positives per split
* enough spatial diversity
  With nside=128 you will have many cells; the split is stable.

### B4. Ensuring positives in correct splits

Use the same deterministic function:

* healpix_index = ang2pix(nside, ra, dec)
* split = hash(healpix_index) → train/val/test
  Apply this to positives and negatives identically.
  Then explicitly verify:
* no healpix cell appears in multiple splits.

---

## SECTION C: Label handling

### C1. Smoothing vs sample weights

Use **sample weights in the loss** as the primary mechanism, not only label smoothing.

* label smoothing changes the implied target distribution
* sample weights preserve the meaning of probability while downweighting noisy labels

Recommended:

* Literature confirmed: label=1.0, weight=1.0
* Tier-A spectroscopic/confident: label=1.0, weight=0.9–1.0
* Tier-B probable: label=1.0, weight=0.3–0.6 depending on estimated purity
  Optionally apply mild label smoothing (1.0 → 0.95) for Tier-B only.

### C2. SLACS/BELLS low visibility

Do **not** use “invisible in DR10” positives for training as positives.

* Put SLACS/BELLS into:

  * evaluation Tier-B or stress-test set if their arc SNR is below threshold
  * training only if they pass your arc-visibility selection function

### C3. Reconstructing Paper IV list

You likely cannot reconstruct the exact 1,372 set unless they published the list (table or data release).
Treat alignment as:

* match their broad source categories and selection criteria
* do not claim exact replication

### C4. Overlap prevention

Use **explicit ID exclusion plus spatial split**.

* Build a canonical ID for each lens candidate (ra,dec rounded to 0.1″ plus source catalog name).
* Remove any overlap between training positives and evaluation anchors by ID.
  Spatial separation alone is not enough because lenses cluster and coordinates can be close.

---

## SECTION D: DESI spectroscopic catalogs access

You must verify availability and formats. Use web queries before implementation. The likely outcome:

* some are public via DESI data releases or associated tables
* some may require collaboration access
  Plan defensively:
* treat them as **independent validation** if public
* otherwise, use other independent sets (non-Huang) that are public (e.g., SLACS/BELLS/SL2S confirmed, and other published tables)

Operational rule:

* if a catalog is “spectroscopically selected” but not confirmed, treat as “silver positives” for qualitative validation, not ground truth recall.

---

## SECTION E: Injection realism (critical)

### E1. Source magnitude distribution

Do not pick uniform flux. Use a magnitude distribution tied to real DR10 detectability:

* start with **r-band source mag 22–26** (unlensed)
* draw magnification proxy μ from a broad prior (e.g., 5–30)
* compute lensed apparent magnitude and enforce that **annulus SNR** spans roughly 0–5 for the majority, with a tail to ~10

### E1 morphology model

Bracket with two source families:

1. **smooth Sersic** (baseline)
2. **clumpy** (parametric clumps on Sersic or a small library)
   If you can justify COSMOS cutouts, treat as a third optional bracket, not the only model.

### E2 PSF and noise

* Use per-cutout PSF proxy from DR10 when possible. If you must use Moffat:

  * fixed beta is acceptable only if you show it approximates DR10 PSF wings in stacked stars; otherwise bracket beta (e.g., 2.5, 3.5, 4.5).
* Noise estimation:

  * outer annulus is fine, but first mask sources using sigma-clipping or segmentation.
  * use MAD not std.

### E3 Inner images

Primary path:

* **include inner images** but calibrate their visibility by enforcing realistic contrast relative to the lens light and noise.
  Ablation:
* also run a “suppressed inner image” variant to bound sensitivity.
  Do not pick one without bracketing. Reviewers will ask.

### E4 Acceptance thresholds (GO/NO-GO)

Use a rule-based GO/NO-GO. Suggested thresholds:

1. **Annulus SNR distribution match** (injections vs real Tier-A anchors):

* median within **0.5× to 2×**
* 10th percentile within **0.5× to 2×**
* 90th percentile within **0.5× to 2×**

2. **Noise-only pixels KS test**:

* p-value > **0.05** for background pixel distributions in each band (after masking sources)

3. **Color distributions**:

* median (g-r) and (r-z) within **±0.2 mag**

4. **Visual sanity panel**:

* blinded 50-image panel: injections should not look systematically “cleaner” than real.

All must pass for GO. If one fails, you fix and rerun a small validation batch, not the full EMR run.

---

## SECTION F: Cutouts

### F1. 64×64 vs 101×101

Pick one and keep consistency. Given you already have 101×101 for many real candidates, I would standardize on **101×101** and train with center-crop augmentation down to 64 if needed.

### F2. Regenerate positives?

Yes, if you change canonical size for training. For evaluation, you can run inference at the native size if the model supports it, but for clean methodology, keep one size.

### F3. Require all bands?

Require **g,r,z present** for training baseline if you want parity with Huang-style pipelines. If missing:

* either exclude or fill with a sentinel and add a “band_missing” feature.
  For first pass, exclude missing-band cutouts to avoid hidden artifacts.

### F4. NaNs in one band

Exclude if NaNs are in the central region. If NaNs only in outer rim and you crop/augment away, you can keep but track it.

---

## SECTION G: Training configuration

### G1. Primary baseline

Use **ResNet18** first. It is fast, stable, and good for ablations.
Then add EfficientNet-B0 only if:

* you can demonstrate incremental gain on independent validation.

### G2. Paper IV ensemble replication?

Do not start with full replication. Start with:

* single ResNet18 baseline + strong data protocol
  Then:
* add a second architecture for diversity if time allows.

### G3. Minimum epochs

For a defensible baseline:

* train until validation metrics plateau and calibration stabilizes, typically **20–40 epochs** with cosine schedule.
  State in paper: early stopping on spatial-val set.

### G4. Batch size (24GB)

For 64×64×3:

* batch 256 is often possible; for 101×101, batch 128–192.
  Use AMP.

### G5. Augmentations

Safe:

* rotations/flips
* small translations
* mild additive noise consistent with DR10
* mild PSF blur jitter within observed range
  Risky:
* aggressive brightness/contrast jitter (can create shortcuts)
* heavy normalization changes
  If you use brightness jitter, constrain it and show gates do not activate.

---

## SECTION H: Selection function grid

### H1. Axes and resolution (minimum viable)

Use 3 primary axes first:

* θE: 0.5–3.0 in **0.25″ steps**
* PSF FWHM: 0.9–1.8 in **0.15″ steps**
* depth (psfdepth_r or galdepth_r): 22.5–24.5 in **0.5 mag steps**
  Optional fourth axis:
* host type (Tractor TYPE bins)

### H2. Injections per cell

Minimum **200 injections per cell** for stable binomial error bars.
If you cannot afford that, reduce grid resolution.

### H3. Sparse cells

Do not smooth unless you can justify a model.
Preferred:

* mark “insufficient” below Nmin
* merge adjacent bins

### H4. Uncertainty

Use **Bayesian binomial** intervals per cell (Beta posterior) as default.
Bootstrap is fine for aggregated metrics; for per-cell, Bayesian is cleaner.
Report 68% intervals (and optionally 95% in appendix).

---

## SECTION I: Milestone sequencing

### I1. EMR first vs quick baseline

Critical path is:

* you need negatives to train properly, but you can start de-risking immediately.

Recommended parallel plan:

* **Path A (fast):** Train a quick baseline using existing negatives (temporary) only to validate training code, splits, and evaluation pipeline.
* **Path B (correct):** Build EMR negative sampling + cutouts in parallel; switch training to proper N1+N2 once ready.

Do not interpret quick-baseline performance as scientific.

---

## SECTION J: Paper strategy

### Must-have figures

1. Data and split schematic (spatial split + strata)
2. Score distributions by stratum (PSF/depth/nobs/type)
3. Selection function heatmaps (C(\theta_E,\mathrm{PSF})) at fixed depth slices
4. Failure mode gallery with quantitative counts by category and conditions
5. Independent validation performance table (even if labels are weak)

### Claims to avoid

* overall precision in the survey
* cosmology constraints
* “complete” lens sample
* claims of outperforming Huang unless you have a matched evaluation protocol

### One-sentence novelty statement

“We provide a detector-audit framework for DR10 strong-lens searches, including injection-calibrated completeness surfaces and a condition- and confuser-resolved false-positive taxonomy, enabling bias-aware use of ML lens catalogs.”

### Journal choice

MNRAS is reasonable for methods + selection bias work if you keep claims disciplined. ApJ is also plausible. Do not optimize for venue now; optimize for correctness.

---

## SECTION K: Computational constraints

Single GPU + EMR is sufficient for:

* training 1–3 models
* injection grid on a reduced host sample
  Risk is cost blow-up from too fine a selection grid or too many cutouts.

EMR cost depends entirely on instance types and IO; treat as variable and keep:

* 500k cutouts is feasible
* injection grid: start with a small host subset per stratum and scale only after diagnostics pass

---

## SECTION L: Gap check

### Critical question you are not asking

“How will we demonstrate independence from prior ML candidate selection?”
You need an explicit section and experiment for this.

### Single highest-risk failure mode

Injections that pass visual checks but fail quantitative distribution matching, leading to a selection function that reviewers reject as “not validated.”

### If scope must be cut

Cut:

* full ensemble meta-learner
* full-survey inference
  Keep:
* real-image baseline with strict spatial split
* independent validation set
* validated injection-recovery selection function
* failure-mode taxonomy

---

If you want this turned into an execution checklist (commands, expected artifacts, GO/NO-GO gates at each step) aligned to your schema and EMR job stages, say so and I will write it as a single runbook document.


===============


## Follow-up: Code Audit Against Paper IV - Discrepancy Resolution

After auditing our implementation against "Strong Lens Discoveries in DESI Legacy Imaging Surveys DR10 with Two Deep Learning Architectures" (Inchausti et al. 2025, aka Paper IV), we found and fixed two critical discrepancies in our negative sampling pipeline (`spark_negative_sampling.py`).

### Discrepancy 1: Missing Minimum Exposure Filter

**Paper IV states:**
> "requiring at least three exposures in the g, r, and z bands"

**Our original code:**
Only used `nobs_z` for stratification binning, but did NOT enforce a minimum exposure filter for any band.

**Fix applied (spark_negative_sampling.py, around line 438):**
```python
# 5. Minimum exposures per band (Paper IV: ≥3 in each of g, r, z)
# "at least three exposures in the g, r, and z bands" - Inchausti et al. 2025
MIN_EXPOSURES = 3
nobs_ok = (nobs_g_all >= MIN_EXPOSURES) & (nobs_r_all >= MIN_EXPOSURES) & (nobs_z_all >= MIN_EXPOSURES)
skip_reasons["insufficient_exposures"] = int(np.sum(valid_coords & in_decals & valid_types & maskbit_ok & ~nobs_ok))
```

### Discrepancy 2: Missing DECaLS Footprint Cut

**Paper IV states:**
> "The footprint of DECaLS in DR10 in the range −18◦ < δ < 32◦"

**Our original code:**
No declination filter was applied.

**Fix applied (spark_negative_sampling.py, around line 421):**
```python
# 2. DECaLS footprint (Paper IV: δ < +32°)
# "DECaLS in DR10 in the range −18 ◦ < δ < 32◦" - Inchausti et al. 2025
DECALS_DEC_MAX = 32.0
in_decals = dec_all < DECALS_DEC_MAX
skip_reasons["outside_decals"] = int(np.sum(valid_coords & ~in_decals))
```

### Questions for Review

1. **Is the exposure filter correctly applied?** We now require `nobs_g >= 3 AND nobs_r >= 3 AND nobs_z >= 3` before a source enters the negative pool. This is applied in the vectorized filtering stage before spatial exclusion.

2. **Should we also enforce δ > -18°?** Paper IV mentions "−18◦ < δ < 32◦", but the southern limit is typically covered by DECam anyway. Should we add an explicit lower bound?

3. **Any other filters we missed?** The paper mentions "Extragalactic sources" but we interpret this as the morphological type filter (PSF, REX, EXP, DEV, SER) already applied.

Please confirm these fixes are correct and identify any remaining discrepancies.

LLM: 

1. Exposure filter

Your interpretation matches the Paper IV sentence and the logic is correct in principle: require **nobs_g ≥ 3 AND nobs_r ≥ 3 AND nobs_z ≥ 3** before entering the negative pool.

Two concrete audit points to confirm in your code (these are common pitfalls):

* **Are `nobs_*_all` the correct DR10 “number of exposures” fields?** In Legacy Surveys products there are multiple related quantities (per-band nobs, per-band nobs per camera, etc.). Paper IV’s intent is the per-band exposure counts used for coadds. Make sure you are using the **same “nobs” definition** that Paper IV used (almost certainly the `nobs_g`, `nobs_r`, `nobs_z` columns in the sweeps/Tractor catalogs for the DR you are sampling from).
* **Make the skip reason conditional consistent:** your skip count currently uses `valid_coords & in_decals & valid_types & maskbit_ok & ~nobs_ok`. Ensure `in_decals` reflects the full DECaLS footprint (see below) so you are not undercounting “insufficient_exposures” by excluding many sources earlier.

Recommendation: log and persist a small table of marginal counts:

* count failing each individual band: `nobs_g<3`, `nobs_r<3`, `nobs_z<3`
* count failing any band
  This helps spot if you accidentally used the wrong column (you will see weirdly high failures in a band that should not dominate).

2. Declination lower bound (δ > −18°)

Yes, enforce the lower bound explicitly if you are trying to follow Paper IV “as written.” The statement is a footprint definition, not a casual remark, and applying only δ < 32° is incomplete relative to that definition.

Add:

```python
DECALS_DEC_MIN = -18.0
in_decals = (dec_all > DECALS_DEC_MIN) & (dec_all < DECALS_DEC_MAX)
```

Why this matters even if “DECam covers the south anyway”:

* DR10 includes imaging beyond DECaLS (BASS/MzLS, other regions). The lower bound is part of isolating the DECaLS region. If you do not apply it, you can leak in non-DECaLS regimes with different systematics.
* Your negative distribution will shift and can break “fair comparison” claims versus Paper IV.

3. Other likely missing filters/discrepancies to check

Based on what you have described, here are the highest-probability remaining mismatches that actually change the negative pool in meaningful ways:

A) Extragalactic selection is not just morphology type
“Extragalactic sources” typically means excluding stars and stellar-contaminated entries. In Legacy Surveys catalogs, morphology type helps, but you can still pick up problematic star-like objects depending on TYPE handling (REX can be compact). Common additional filters include:

* remove objects with star-like morphology if you intended “galaxies only”
* or enforce a flux/magnitude cut consistent with the scan population used in Paper IV (they likely have magnitude limits for feasibility)

Action: confirm whether Paper IV used a magnitude cut for the scanned sample and apply the same for negatives.

B) Maskbits / quality cuts details
Paper IV likely uses specific maskbits exclusions (and sometimes fitbits). If you only have a generic `maskbit_ok`, you should verify:

* which maskbits are excluded
* whether they exclude WISE brightmask regions, saturation, bleed trails, etc.

Action: match their exact maskbits list if the paper specifies it. If not specified, document your choices.

C) Galactic plane / high extinction cut
“Extragalactic” often implies a Milky Way avoidance criterion:

* EBV threshold (or |b| cut)
  If Paper IV applied any of this and you did not, your negatives will include crowded/extincted fields with very different artifacts.

Action: check for EBV or |b| cuts and replicate if present.

D) North vs South imaging regimes
If Paper IV restricted to DECaLS (DECam) explicitly, that is partly to avoid mixing BASS/MzLS characteristics. Your δ bounds are one step; you also need to ensure your `in_decals` definition truly isolates DECaLS footprint in DR10 (dec bounds are a proxy, but there can be edge/overlap regions).

Action: prefer a footprint flag if available in the sweeps metadata; otherwise keep the dec bounds and also log camera/survey provenance if available.

E) Spatial exclusion radius and lens exclusion catalog
Make sure you are excluding known lenses/candidates from negatives using the same or stricter radius than your positives’ typical θ_E plus astrometric uncertainty. If you are comparing to Paper IV, they probably excluded known lenses and also excluded their own training positives.

Action: keep your lens exclusion radius conservative and document it.

Minimal recommended next audit step (fast, decisive)

Produce a “Paper IV parity report” from your Spark job that prints, for a fixed sweeps shard:

* starting count
* after each filter: coords valid, decals footprint, type filter, maskbits, nobs_g/r/z>=3, any magnitude cut, any EBV cut, any star/galaxy separation cut, spatial exclusion
* final count per stratum bin (nobs_z bin and type)

If your per-stratum counts look wildly different from what Paper IV implies (or your earlier pipeline), you will spot it immediately.

Summary answers

1. Exposure filter: conceptually correct; verify the `nobs_*` columns are the intended ones and log per-band failure counts.
2. Declination lower bound: yes, add δ > −18° to match Paper IV’s footprint definition.
3. Likely remaining discrepancies: extragalactic selection details (not just TYPE), maskbits specifics, any EBV/|b| cut, any magnitude/flux cut defining the scanned population, and provenance/footprint isolation beyond a single dec bound.

If you upload the exact paragraph(s) from Paper IV where they define the negative pool and scan population (or point me to the section and page in your PDF), I can turn this into an exact checklist of filters to match line-by-line.

===============


# Full Code Review Request: Strong Lens Pipeline for Paper IV Parity

## Context

We are building a pipeline to generate training data for strong lens detection in DESI Legacy Imaging Surveys DR10. Our goal is to match the methodology in **"Strong Lens Discoveries in DESI Legacy Imaging Surveys DR10 with Two Deep Learning Architectures"** (Inchausti et al. 2025, hereafter Paper IV).

The attached `code_review_package.zip` contains all relevant code. Please review it for correctness and Paper IV compliance.

---

## Pipeline Overview

Our pipeline has 5 main stages:

1. **Positive Crossmatch** (`spark_crossmatch_positives_v2.py`): Match known lens catalogs against DR10 sweeps
2. **Negative Sampling** (`spark_negative_sampling.py`): Sample non-lens galaxies from DR10 sweeps
3. **Stratified Sampling** (`spark_stratified_sample.py`): Balance negatives to match positive distribution
4. **Cutout Generation** (`spark_generate_cutouts.py`): Download 101×101 pixel g,r,z cutouts
5. **Cutout Validation** (`spark_validate_cutouts.py`): Quality checks and shortcut detection

---

## Critical Code Sections for Review

### 1. Negative Sampling Filters (spark_negative_sampling.py, lines 417-462)

```python
# 1. Valid coordinates
valid_coords = np.isfinite(ra_all) & np.isfinite(dec_all)
skip_reasons["invalid_coords"] = int(np.sum(~valid_coords))

# 2. DECaLS footprint (Paper IV: −18° < δ < +32°)
# "DECaLS in DR10 in the range −18 ◦ < δ < 32◦" - Inchausti et al. 2025
DECALS_DEC_MIN = -18.0
DECALS_DEC_MAX = 32.0
in_decals = (dec_all > DECALS_DEC_MIN) & (dec_all < DECALS_DEC_MAX)
skip_reasons["outside_decals"] = int(np.sum(valid_coords & ~in_decals))

# 3. Valid galaxy types (N1 pool)
valid_types = np.isin(types_all, list(VALID_TYPES_N1))
skip_reasons["not_valid_type"] = int(np.sum(valid_coords & in_decals & ~valid_types))

# 4. Maskbit exclusions (vectorized bitwise check)
exclude_mask_combined = 0
for bit in exclude_maskbits:
    exclude_mask_combined |= (1 << bit)
maskbit_ok = (maskbits_all & exclude_mask_combined) == 0
skip_reasons["maskbit"] = int(np.sum(valid_coords & in_decals & valid_types & ~maskbit_ok))

# 5. Minimum exposures per band (Paper IV: ≥3 in each of g, r, z)
# "at least three exposures in the g, r, and z bands" - Inchausti et al. 2025
MIN_EXPOSURES = 3
nobs_g_ok = nobs_g_all >= MIN_EXPOSURES
nobs_r_ok = nobs_r_all >= MIN_EXPOSURES
nobs_z_ok = nobs_z_all >= MIN_EXPOSURES
nobs_ok = nobs_g_ok & nobs_r_ok & nobs_z_ok

# Per-band failure counts for diagnostics
pre_nobs_mask = valid_coords & in_decals & valid_types & maskbit_ok
skip_reasons["nobs_g_lt_3"] = int(np.sum(pre_nobs_mask & ~nobs_g_ok))
skip_reasons["nobs_r_lt_3"] = int(np.sum(pre_nobs_mask & ~nobs_r_ok))
skip_reasons["nobs_z_lt_3"] = int(np.sum(pre_nobs_mask & ~nobs_z_ok))
skip_reasons["insufficient_exposures"] = int(np.sum(pre_nobs_mask & ~nobs_ok))

# 6. Z-band magnitude limit
with np.errstate(divide='ignore', invalid='ignore'):
    mag_z_all = np.where(flux_z_all > 0, 22.5 - 2.5 * np.log10(flux_z_all), np.nan)
mag_z_ok = np.isnan(mag_z_all) | (mag_z_all < z_mag_limit)
skip_reasons["mag_z_faint"] = int(np.sum(valid_coords & in_decals & valid_types & maskbit_ok & nobs_ok & ~mag_z_ok))

# Combined mask before spatial query
pre_spatial_mask = valid_coords & in_decals & valid_types & maskbit_ok & nobs_ok & mag_z_ok
```

### 2. Galaxy Type Constants (sampling_utils.py)

```python
# Galaxy types for Pool N1
VALID_TYPES_N1 = {"SER", "DEV", "REX", "EXP"}

# nobs_z bin edges: [1-2], [3-5], [6-10], [11+]
NOBS_Z_BINS = [(1, 2), (3, 5), (6, 10), (11, 999)]

# Maskbits to exclude (from config, but hardcoded defaults)
DEFAULT_EXCLUDE_MASKBITS = {1, 5, 6, 7, 11, 12, 13}
```

### 3. N2 Confuser Classification (sampling_utils.py, lines 177-210)

```python
def classify_pool_n2(
    galaxy_type: str,
    flux_r: Optional[float],
    shape_r: Optional[float],
    g_minus_r: Optional[float],
    mag_r: Optional[float],
    config: Dict[str, Any]
) -> Optional[str]:
    """
    Classify galaxy into N2 confuser categories based on Tractor properties.
    """
    n2_config = config.get("negative_pools", {}).get("pool_n2", {}).get("tractor_criteria", {})
    
    # Ring proxy: DEV with bright flux
    ring_cfg = n2_config.get("ring_proxy", {})
    if galaxy_type == "DEV":
        if flux_r is not None and flux_r >= ring_cfg.get("flux_r_min", 10):
            return "ring_proxy"
    
    # Edge-on proxy: EXP with large half-light radius
    edge_on_cfg = n2_config.get("edge_on_proxy", {})
    if galaxy_type == "EXP":
        if shape_r is not None and shape_r >= edge_on_cfg.get("shape_r_min", 2.0):
            return "edge_on_proxy"
    
    # Blue clumpy proxy: blue color
    blue_cfg = n2_config.get("blue_clumpy_proxy", {})
    if g_minus_r is not None and g_minus_r <= blue_cfg.get("g_minus_r_max", 0.4):
        if mag_r is not None and mag_r <= blue_cfg.get("r_mag_max", 19.0):
            return "blue_clumpy"
    
    return None
```

### 4. Cutout Generation (spark_generate_cutouts.py)

```python
# Cutout parameters
CUTOUT_SIZE = 101  # pixels
PIXEL_SCALE = 0.262  # arcsec/pixel
CUTOUT_ARCSEC = CUTOUT_SIZE * PIXEL_SCALE  # ~26.5 arcsec

# Legacy Survey cutout service
CUTOUT_URL_TEMPLATE = (
    "https://www.legacysurvey.org/viewer/fits-cutout"
    "?ra={ra}&dec={dec}&size={size}&layer=ls-dr10&pixscale={pixscale}&bands={bands}"
)

def download_cutout(...) -> Tuple[Optional[np.ndarray], Dict]:
    # ... 
    with fits.open(buffer) as hdul:
        data = hdul[0].data  # Shape: (n_bands, height, width)
        cutout = np.transpose(data, (1, 2, 0)).astype(np.float32)  # -> (H, W, 3)
```

### 5. Configuration File (negative_sampling_v1.yaml)

```yaml
negative_pools:
  pool_n1:
    valid_types: ["SER", "DEV", "REX", "EXP"]
    z_mag_limit: 21.0
    exclusion_radius_arcsec: 10.0
    nobs_z_bins: [[1, 2], [3, 5], [6, 10], [11, null]]
  pool_n2:
    tractor_criteria:
      ring_proxy:
        flux_r_min: 10.0
      edge_on_proxy:
        shape_r_min: 2.0
      blue_clumpy_proxy:
        g_minus_r_max: 0.4
        r_mag_max: 19.0
```

---

## Direct Questions for Review

### A. Footprint and Selection Filters

1. **DECaLS footprint bounds**: We use `-18° < δ < 32°`. Is this the exact range Paper IV used? Should it be `≤` instead of `<` on either bound?

2. **Galactic plane / extinction cut**: Paper IV mentions "extragalactic sources". Did they apply any E(B-V) threshold or |b| (Galactic latitude) cut to exclude the Galactic plane? We currently have no such filter.

3. **Magnitude cut consistency**: We use `z_mag < 21.0` for the faint limit. Does Paper IV specify a magnitude limit for their scanned sample? If so, which band and what value?

4. **NOBS definition**: We use `NOBS_G`, `NOBS_R`, `NOBS_Z` columns from DR10 sweeps. Are these the correct "number of exposures" fields that Paper IV refers to, or is there a different column (e.g., per-camera counts)?

### B. Galaxy Type Selection

5. **TYPE values**: We accept `{"SER", "DEV", "REX", "EXP"}` as valid galaxy types. Does Paper IV include or exclude any of these? Does Paper IV also include `PSF` type sources for any reason?

6. **Star-galaxy separation**: Beyond TYPE filtering, did Paper IV apply any additional star-galaxy separation (e.g., using `DCHISQ` or `FRACFLUX` columns)?

### C. Maskbit Handling

7. **Maskbit list**: We exclude bits `{1, 5, 6, 7, 11, 12, 13}`. These correspond to:
   - 1: BRIGHT (near bright source)
   - 5: ALLMASK_G
   - 6: ALLMASK_R  
   - 7: ALLMASK_Z
   - 11: MEDIUM (medium-bright star)
   - 12: GALAXY (near large galaxy)
   - 13: CLUSTER (near globular cluster)
   
   Does Paper IV specify their maskbit exclusions? Are we missing any critical bits (e.g., WISE-related bits)?

### D. N2 Confuser Categories

8. **Ring proxy criteria**: We classify `DEV` galaxies with `flux_r >= 10 nanomaggies` as ring proxies. Does Paper IV define ring galaxies differently? Should we also check ellipticity or Sersic index?

9. **Edge-on proxy criteria**: We classify `EXP` galaxies with `shape_r >= 2.0 arcsec` as edge-on proxies. Does Paper IV use axis ratio (b/a) or ellipticity instead?

10. **Blue clumpy criteria**: We use `g-r <= 0.4` and `r_mag <= 19.0`. What are Paper IV's thresholds for blue star-forming galaxies that can mimic arcs?

11. **Missing confuser categories**: Does Paper IV identify other confuser types we should include (e.g., mergers, tidal tails, bright star artifacts)?

### E. Cutout Generation

12. **Cutout size**: We use 101×101 pixels at 0.262 arcsec/pixel (~26.5 arcsec). Does Paper IV use the same size and pixel scale?

13. **Band ordering**: We request bands as "grz" and the FITS returns shape `(3, H, W)`. We transpose to `(H, W, 3)` with channel order [g, r, z]. Is this the correct channel ordering for Paper IV's models?

14. **Normalization**: Paper IV likely applies some normalization before feeding to CNNs. What normalization scheme do they use? (asinh, percentile clipping, per-channel standardization?)

### F. Spatial Exclusion

15. **Exclusion radius**: We exclude negatives within 10 arcsec of known lenses. Does Paper IV use a different radius? Should it scale with Einstein radius?

16. **Known lens catalog**: Which lens catalogs should be used for exclusion? (Master Lens Database, SuGOHI, our own training positives?)

### G. Train/Val/Test Split

17. **Spatial split method**: We use HEALPix-based spatial splitting with NSIDE=128. Does Paper IV use spatial splits or random splits?

18. **Split proportions**: We use 70/15/15 for train/val/test. What proportions does Paper IV use?

### H. End-to-End Consistency

19. **Sample sizes**: Paper IV scanned ~43M galaxies and found ~4,000 high-confidence candidates. Our negative pool targets ~500K total. Is this ratio reasonable for a training set?

20. **N1:N2 ratio**: We target 85% N1 (representative) and 15% N2 (confusers). Does Paper IV specify a similar ratio, or should we adjust?

21. **Stratification**: We stratify by `(nobs_z_bin, type_bin)`. Does Paper IV stratify by additional variables (e.g., seeing, depth, magnitude)?

---

## Actual Pipeline Run Results (2026-02-08)

We completed a full negative sampling run on DR10 sweeps. Here are the actual results:

### Output Summary
- **Total output size**: 3.3 GiB (60 parquet partitions)
- **Rows per partition**: ~470K
- **Estimated total rows**: ~28M negative candidates

### Pool Distribution (from sample partition)
| Pool | Count | Percentage |
|------|-------|------------|
| N1 (representative) | 442,359 | 94.2% |
| N2 (confusers) | 27,427 | 5.8% |

### N2 Confuser Category Breakdown
| Category | Count | Percentage of N2 |
|----------|-------|------------------|
| ring_proxy | 18,318 | 66.8% |
| edge_on_proxy | 7,840 | 28.6% |
| blue_clumpy | 1,269 | 4.6% |

### Output Schema (48 columns)
```
galaxy_id, brickname, objid, ra, dec, type, nobs_z, nobs_z_bin, type_bin,
flux_g, flux_r, flux_z, flux_w1, mag_g, mag_r, mag_z, 
g_minus_r, r_minus_z, z_minus_w1,
psfsize_g, psfsize_r, psfsize_z, psfdepth_g, psfdepth_r, psfdepth_z,
galdepth_g, galdepth_r, galdepth_z, ebv, maskbits, fitbits,
mw_transmission_g, mw_transmission_r, mw_transmission_z,
shape_r, shape_e1, shape_e2, sersic, healpix_64, healpix_128, split,
pool, confuser_category, sweep_file, row_index, 
pipeline_version, git_commit, extraction_timestamp
```

---

## Critical Observations Requiring Clarification

### Observation 1: N2 ratio is 5.8%, not 15%
Our N2 pool is only 5.8% of total, not the 15% we targeted. This suggests either:
- (a) Our confuser criteria are too strict
- (b) True confusers are genuinely rare in the population
- (c) We're missing confuser categories

**Question**: Is 5.8% N2 acceptable, or should we relax thresholds to reach 15%?

### Observation 2: Ring proxies dominate N2
66.8% of N2 are ring_proxy (DEV with flux_r >= 10). Only 4.6% are blue_clumpy.

**Question**: Is this distribution expected? Should we adjust thresholds to balance categories?

### Observation 3: Blue clumpy threshold may be too strict
Only 1,269 blue_clumpy objects found (g-r <= 0.4, r_mag <= 19.0).

**Question**: What g-r threshold does Paper IV use for "blue star-forming" confusers? Is 0.4 too blue?

---

## Summary of Known Discrepancies Already Fixed

1. ✅ Added `nobs_g >= 3 AND nobs_r >= 3 AND nobs_z >= 3` filter (was missing)
2. ✅ Added `dec > -18°` lower bound (was only `dec < 32°`)
3. ✅ Fixed FITS parsing to correctly read `hdul[0].data` with shape `(n_bands, H, W)`
4. ✅ Added per-band nobs failure logging for diagnostics

---

## Additional Questions Based on Run Results

### I. N2 Classification Thresholds

22. **Ring proxy flux threshold**: We use `flux_r >= 10 nanomaggies` (~19.5 mag). Is this the right threshold for identifying galaxies with ring-like structure? Should we also require high ellipticity or specific Sersic index?

23. **Edge-on half-light radius**: We use `shape_r >= 2.0 arcsec`. Paper IV likely uses axis ratio (b/a < 0.3?) instead. What is the correct criterion for edge-on spirals?

24. **Blue clumpy color cut**: We use `g-r <= 0.4`. Should this be `g-r <= 0.6` or another value? What magnitude range?

25. **Merger/interaction criteria**: We don't identify mergers. Should we add criteria based on asymmetry or multiple components?

### J. Positive Sample Handling

26. **Positive catalog sources**: We use 5,104 positives from DESI_master_full.csv. Does Paper IV use the same catalog, or a different/larger set?

27. **Positive-negative balance**: With ~28M negatives and ~5K positives, our ratio is ~5600:1 before stratified sampling. What ratio does Paper IV use for training?

28. **Grade/confidence weighting**: Should we weight positives by their confidence grade (A/B/C) during training?

### K. Data Quality

29. **PSF size distribution**: We store `psfsize_g/r/z` but don't filter on it. Does Paper IV require seeing < X arcsec?

30. **Depth requirements**: We store `psfdepth_g/r/z` and `galdepth_g/r/z`. Does Paper IV apply minimum depth cuts?

31. **E(B-V) threshold**: We store `ebv` but don't filter. Does Paper IV exclude high-extinction regions (e.g., E(B-V) > 0.1)?

---

## Requested Output

Please provide:

1. **Line-by-line filter parity check**: For each of our filters, confirm if it matches Paper IV or specify the correction needed.

2. **Missing filters**: List any filters Paper IV applies that we don't have.

3. **Parameter corrections**: For each configurable threshold (magnitude limits, radii, color cuts), provide the Paper IV value if different from ours.

4. **N2 category definitions**: Confirm or correct our confuser classification criteria.

5. **Cutout specifications**: Confirm size, pixel scale, band order, and any normalization details.

6. **Priority ranking**: If multiple issues exist, rank them by impact on Paper IV parity.

---

## Attached Files

The `code_review_package.zip` contains:
- `spark_negative_sampling.py` - Main negative sampling job
- `spark_generate_cutouts.py` - Cutout download job
- `spark_validate_cutouts.py` - Quality validation job
- `spark_stratified_sample.py` - Stratified sampling job
- `spark_crossmatch_positives_v2.py` - Positive catalog crossmatch
- `sampling_utils.py` - Utility functions (N2 classification, binning)
- `negative_sampling_v1.yaml` - Configuration file

LLM :

## Executive verdict

Your two fixes (min exposures in **g/r/z** and **DECaLS declination bounds**) are directionally correct and consistent with Paper IV. However, there are **two additional high-impact Paper IV parity gaps** in the current negative-sampling configuration:

1. **Magnitude cut is wrong**: Paper IV uses **z < 20** for the nonlens sampling pool; your YAML uses **z_mag_limit: 21.0**.
2. **TYPE set likely mismatched for the nonlens pool**: Paper IV’s nonlens selection text explicitly lists **SER/DEV/REX** (not EXP) for the pool they sample from; your N1 allows **EXP**.

Everything else is either (a) not specified in Paper IV (so you cannot claim parity), or (b) a design choice you can keep, but you must label it as *your extension* (example: N2 confusers).

---

## 1) Line-by-line parity check of your negative filters

### ✅ Filter 1: finite RA/Dec

* **Your code**: `np.isfinite(ra) & np.isfinite(dec)`
* **Paper IV parity**: Fine.

### ✅ Filter 2: DECaLS footprint declination

* **Your code**: `-18 < dec < 32`
* **Paper IV**: states DECaLS footprint in DR10: **−18° < δ < 32°**
* **Action**:

  * Your current implementation is correct.
  * Using `<` vs `<=` is negligible; keep `<` for consistency with the text.

### ✅ Filter 3: min exposures in each band

* **Your code**: `nobs_g >= 3 AND nobs_r >= 3 AND nobs_z >= 3`
* **Paper IV**: “at least three exposures in the g, r, and z bands”
* **Action**: Correct.

### ⚠️ Filter 4: TYPE values (major parity risk)

* **Your code**: `VALID_TYPES_N1 = {"SER","DEV","REX","EXP"}`
* **Paper IV**: the nonlens selection description explicitly enumerates **SER, DEV, REX**.
* **What to do** (for strict parity):

  * **N1 (Paper IV parity mode)**: use **SER/DEV/REX only**.
  * **Optional extension**: keep EXP in a separate “deployment-like” pool (call it N1b) and treat it as an ablation or robustness test, not parity.

### ⚠️ Filter 5: z-band magnitude limit (major parity risk)

* **Your code**:

  * Computes `mag_z = 22.5 - 2.5 log10(flux_z)` (nanomaggies assumption)
  * Applies `mag_z < z_mag_limit` where YAML has `z_mag_limit: 21.0`
* **Paper IV**: “limiting to **z < 20**”
* **Action**:

  * Change YAML to: `z_mag_limit: 20.0`
  * Also confirm that **flux_z is nanomaggies** in DR10 sweeps (it usually is). If you are not 100% sure, add a one-time validation: compare your computed `mag_z` distribution against a known DR10 mag column if present.

### ⚠️ Maskbits (unknown parity)

* **Your code**: excludes `{1,5,6,7,11,12,13}`
* **Paper IV**: does not clearly specify maskbit exclusions in the text you quoted.
* **Action**:

  * You cannot claim parity here.
  * Keep your maskbit exclusions as a defensible “quality screen,” but document it explicitly and run a sensitivity test: train with and without the maskbit filter and show impact on contaminants (spikes/ghosts especially).

### ⚠️ “Extragalactic sources” (unknown parity)

* **Your code**: no explicit |b| or E(B–V) cut
* **Paper IV**: says “extragalactic sources” but does not (in the text you provided) spell out an EBV or Galactic latitude threshold.
* **Action**:

  * Do **not** add an EBV cut “for parity” unless you can point to an explicit statement.
  * If you want it as an extension: implement EBV < 0.1 as an ablation and report whether it changes FPR on artifacts.

---

## 2) Cutout generation parity

### ✅ Cutout size and pixel scale

* **Your code**: 101×101 at 0.262″/pix
* **Paper IV**: 101×101 and same instrument scale (consistent).

### ✅ Band order

* **Your code**: request `bands=grz`, transpose `(3,H,W)->(H,W,3)` which yields channels `[g,r,z]`
* **Action**: Correct.

### ⚠️ Normalization / scaling

* **Your code**: cutout download, but no clearly enforced Paper-IV-matching normalization pipeline is visible in the snippets you showed.
* **Paper IV**: normalization details are not in what you quoted. If Paper IV doesn’t specify it clearly, you must:

  * either replicate **Paper I/II** preprocessing explicitly (if they document it),
  * or define your own normalization and label it as your implementation choice.
* **Practical recommendation** (robust + hard to cheat with):

  * per-image, per-band: subtract median, divide by MAD (or robust std), clip to a fixed range, optionally asinh.

---

## 3) N2 “confuser” pool: parity vs extension

Paper IV does not (in your excerpt) describe an explicit N2 confuser pool constructed from Tractor heuristics. Your N2 is therefore **not a parity requirement**; it is an **original extension**.

### What your run implies

* N2 is **5.8%** (not your targeted 15%)
* N2 is dominated by `ring_proxy` under your heuristic.

### Recommendation

* Keep N2 as a curated/heuristic hard-negative pool, but **do not try to force it to exist naturally at 15%**.
* Instead:

  * Let N2 be whatever you can harvest cleanly,
  * then **oversample N2 during stratified training** to reach your desired *training mix* (e.g., 10–20% of batches contain N2).
* This avoids distorting the base population while still giving the model hard negatives.

### Fixing the skew (ring_proxy domination)

Your current proxies are too blunt:

* `ring_proxy`: DEV + bright flux is going to scoop up many normal bright early-types.
* `edge_on_proxy`: using size alone is not an edge-on proxy.
* `blue_clumpy`: `g-r<=0.4` and `r<=19` is extremely restrictive.

If you want N2 to behave like “things that look like arcs”:

* **Edge-on**: use axis-ratio proxy if available (shape_e1/e2 → ellipticity), e.g. `e = sqrt(e1^2+e2^2)` and threshold high-e, plus EXP/REX.
* **Blue clumpy**: relax to `g-r <= 0.7` and allow fainter, e.g. `r <= 20.5`.
* **Ring**: require strong residual structure proxies if you have them (fitbits anomalies, high chi-sq, or multi-component flags). If not available, you cannot reliably identify rings from Tractor scalars alone.

---

## 4) Priority-ranked corrections (highest impact first)

### P0 (must fix for Paper IV parity claims)

1. **Set `z_mag_limit = 20.0`**, not 21.0.
2. **Match nonlens TYPE set to Paper IV**:

   * For strict parity: **SER/DEV/REX only** in N1.
   * Keep EXP as a separate ablation pool if you want deployment realism.

### P1 (strongly recommended, but parity-uncertain)

3. Make your normalization explicit, deterministic, and logged (median/MAD + clip).
4. Decide and document whether you apply any EBV or |b| cuts (treat as extension unless explicitly stated by Paper IV).

### P2 (quality and “original extension” improvements)

5. Redefine N2 confusers (or curate them from Galaxy Zoo / artifacts lists) rather than relying on weak Tractor-only heuristics.
6. Add a “bright star artifact” mining path (spikes/ghosts) because these dominate FPR in real survey scoring.

---

## 5) Direct answers to your discrepancy questions

### (1) Is the exposure filter correctly applied?

Yes, your vectorized filter `nobs_g>=3 & nobs_r>=3 & nobs_z>=3` applied before spatial exclusion is correct.

### (2) Should we enforce δ > −18° explicitly?

Yes. Paper IV states −18° < δ < 32°. You have now enforced it, which is correct.

### (3) Any other filters we missed?

Based on what you quoted from Paper IV, the big missing one is:

* **z < 20** (you currently use 21).
  And likely:
* **Nonlens TYPE list**: Paper IV’s nonlens selection text explicitly lists SER/DEV/REX.

Everything else (maskbits, EBV/galactic plane, star-galaxy separation via DCHISQ/FRACFLUX) is not confirmed from your excerpt, so treat those as non-parity unless you can point to explicit Paper IV text.

---

## 6) What to change right now (concrete edits)

### Update config (parity mode)

```yaml
negative_pools:
  pool_n1:
    valid_types: ["SER", "DEV", "REX"]   # parity mode
    z_mag_limit: 20.0                   # Paper IV
```

### Update constants (parity mode)

```python
VALID_TYPES_N1 = {"SER", "DEV", "REX"}  # parity mode
```

### Keep EXP as optional “deployment realism ablation”

* Add `pool_n1b_exp` with EXP and label it explicitly as “extension beyond Paper IV”.

---

## 7) About N2 being 5.8% not 15%

5.8% is acceptable because you control the **training mixture** in the stratified sampler. Do **not** distort the survey distribution trying to “find” 15% N2 in the wild. If you want 15% in training batches, oversample N2 at the batch sampler level.

---

If you want, I can do a second-pass audit specifically on:

* `spark_stratified_sample.py`: whether it truly reproduces Paper IV’s 100:1 per stratum logic.
* `spark_validate_cutouts.py`: whether your shortcut metrics and QC flags are sufficient to prevent silent data corruption (NaNs, misordered bands, wrong HDU, etc.).

=================

# Second-Pass Audit Request: Stratified Sampling and Cutout Validation

Following your excellent first-pass review, please audit these two additional components for correctness and Paper IV parity.

---

## 1. Stratified Sampling (`spark_stratified_sample.py`)

### Goal
Reproduce Paper IV's 100:1 negative:positive ratio per stratum, while maintaining 85:15 N1:N2 ratio within negatives.

### Current Implementation

```python
# Constants
NEG_POS_RATIO = 100  # 100 negatives per positive
N1_RATIO = 0.85  # 85% from N1 pool
N2_RATIO = 0.15  # 15% from N2 pool

# Strata definitions
NOBS_Z_BINS = ["1-2", "3-5", "6-10", "11+"]
TYPE_BINS = ["SER", "DEV", "REX", "EXP"]  # NOTE: Should this now be ["SER", "DEV", "REX"]?
```

### Sampling Logic (lines 141-175)

```python
# Compute target counts per stratum
target_counts = {}
for (nobs_bin, type_bin), pos_count in pos_counts.items():
    neg_target = pos_count * args.neg_pos_ratio
    n1_target = int(neg_target * args.n1_ratio)
    n2_target = neg_target - n1_target
    target_counts[(nobs_bin, type_bin)] = {
        "total": neg_target,
        "n1": n1_target,
        "n2": n2_target,
    }

# Count available negatives per stratum
neg_stratum_counts = negatives_df.groupBy("nobs_z_bin", "type_bin", "pool").count()

# Sample from each pool proportionally
sampled_dfs = []
for (nobs_bin, type_bin), targets in target_counts.items():
    # Sample N1
    n1_pool = negatives_df.filter(
        (F.col("nobs_z_bin") == nobs_bin) &
        (F.col("type_bin") == type_bin) &
        (F.col("pool") == "N1")
    )
    n1_available = n1_pool.count()
    n1_sample_size = min(targets["n1"], n1_available)
    
    if n1_sample_size > 0:
        fraction = n1_sample_size / n1_available
        sampled_n1 = n1_pool.sample(fraction=min(fraction * 1.1, 1.0), seed=42)
        sampled_n1 = sampled_n1.limit(n1_sample_size)
        sampled_dfs.append(sampled_n1)
    
    # Sample N2 (similar logic)
    # If N2 insufficient, backfill from N1
```

### Questions for Stratified Sampling

1. **Does Paper IV use 100:1 per stratum?** Or is it 100:1 overall with some other stratification scheme?

2. **TYPE_BINS mismatch**: The code still references `["SER", "DEV", "REX", "EXP"]` but we've now excluded EXP from N1. Should TYPE_BINS be updated to `["SER", "DEV", "REX"]`?

3. **Stratum granularity**: We stratify by `(nobs_z_bin, type_bin)` giving 4×4=16 strata (or 4×3=12 without EXP). Is this the right granularity? Should we add seeing or depth bins?

4. **N2 backfill logic**: When N2 is insufficient for a stratum, we backfill from N1. Is this the right approach, or should we oversample N2 globally regardless of stratum?

5. **Sampling seed determinism**: We use `seed=42` for reproducibility. Is this sufficient, or should we use the HEALPix-based split to ensure spatial independence?

---

## 2. Cutout Validation (`spark_validate_cutouts.py`)

### Goal
Detect data quality issues and shortcut features that could bias training.

### Current Implementation

#### Quality Validation (lines 83-130)

```python
def validate_cutout_quality(cutout: np.ndarray) -> Dict:
    h, w, c = cutout.shape
    
    # Size check
    size_ok = (h == CUTOUT_SIZE and w == CUTOUT_SIZE and c == 3)
    
    # NaN analysis
    nan_mask = np.isnan(cutout)
    total_nan_frac = np.sum(nan_mask) / cutout.size
    
    # Central region (50x50)
    center = h // 2
    margin = 25
    central = cutout[center-margin:center+margin, center-margin:center+margin, :]
    central_nan_frac = np.sum(np.isnan(central)) / central.size
    
    # Core region (16x16 around center)
    core = cutout[center-CORE_RADIUS:center+CORE_RADIUS, 
                  center-CORE_RADIUS:center+CORE_RADIUS, :]
    core_nan_frac = np.sum(np.isnan(core)) / core.size
    
    # Bands present
    bands_present = [not np.all(np.isnan(cutout[:, :, i])) for i in range(c)]
    all_bands_present = all(bands_present)
    
    # Quality gate
    quality_ok = (
        size_ok and
        central_nan_frac < MAX_NAN_FRAC_CENTER and  # 2%
        all_bands_present
    )
    
    return {
        "size_ok": size_ok,
        "total_nan_frac": float(total_nan_frac),
        "central_nan_frac": float(central_nan_frac),
        "core_nan_frac": float(core_nan_frac),
        "has_g": bands_present[0],
        "has_r": bands_present[1],
        "has_z": bands_present[2],
        "all_bands_present": all_bands_present,
        "quality_ok": quality_ok,
    }
```

#### Shortcut Feature Extraction (lines 133-200)

```python
def extract_shortcut_features(cutout: np.ndarray) -> Dict:
    h, w, c = cutout.shape
    center = h // 2
    
    # Use r-band (index 1) for brightness features
    r_band = cutout[:, :, 1]
    
    # Create distance map from center
    y, x = np.ogrid[:h, :w]
    dist = np.sqrt((y - center)**2 + (x - center)**2)
    
    # Regions
    core_mask = dist <= CORE_RADIUS  # 8 pixels
    annulus_mask = (dist >= 20) & (dist <= 40)  # where arcs appear
    outer_mask = dist >= OUTER_MARGIN  # edge pixels
    
    # Core brightness (shortcut: lenses have bright cores)
    core_pixels = r_band[core_mask]
    valid_core = core_pixels[~np.isnan(core_pixels)]
    core_brightness = float(np.median(valid_core)) if len(valid_core) > 0 else None
    core_max = float(np.max(valid_core)) if len(valid_core) > 0 else None
    
    # Annulus brightness (where lensing arcs should appear)
    annulus_pixels = r_band[annulus_mask]
    valid_annulus = annulus_pixels[~np.isnan(annulus_pixels)]
    annulus_brightness = float(np.median(valid_annulus)) if len(valid_annulus) > 0 else None
    annulus_std = float(np.std(valid_annulus)) if len(valid_annulus) > 0 else None
    
    # Outer background
    outer_pixels = r_band[outer_mask]
    valid_outer = outer_pixels[~np.isnan(outer_pixels)]
    outer_brightness = float(np.median(valid_outer)) if len(valid_outer) > 0 else None
    
    # Radial gradient (shortcut: lenses have steep gradients)
    radial_gradient = core_brightness - outer_brightness if (core_brightness and outer_brightness) else None
    
    # MAD for noise estimation
    mad_r = float(np.median(np.abs(valid_core - np.median(valid_core)))) if len(valid_core) > 0 else None
    
    # Color features (g-r color in core vs annulus)
    # ... similar logic for color gradient
```

#### AUC Computation for Shortcut Detection (lines 250-280)

```python
def compute_auc(pos_values: List[float], neg_values: List[float]) -> float:
    """Compute AUC for a single feature to detect shortcuts."""
    from sklearn.metrics import roc_auc_score
    
    labels = [1] * len(pos_values) + [0] * len(neg_values)
    values = pos_values + neg_values
    
    # Filter NaN
    valid = [(v, l) for v, l in zip(values, labels) if v is not None and not np.isnan(v)]
    if len(valid) < 10:
        return 0.5  # Not enough data
    
    values, labels = zip(*valid)
    return roc_auc_score(labels, values)

# Shortcut detection: flag features with AUC > 0.70
shortcut_features = {}
for feature in ["core_brightness", "radial_gradient", "core_max", "annulus_std"]:
    pos_vals = [r[feature] for r in pos_results if r.get(feature) is not None]
    neg_vals = [r[feature] for r in neg_results if r.get(feature) is not None]
    auc = compute_auc(pos_vals, neg_vals)
    shortcut_features[feature] = {
        "auc": auc,
        "is_shortcut": auc > AUC_THRESHOLD,  # 0.70
    }
```

### Questions for Cutout Validation

6. **NaN threshold**: We reject cutouts with >2% NaN in central 50×50. Is this too strict or too lenient?

7. **Band ordering verification**: We assume channel order is [g, r, z]. Should we add an explicit check (e.g., verify relative brightness or WCS metadata)?

8. **Core radius of 8 pixels**: This equals ~2.1 arcsec at 0.262"/pix. Is this appropriate for strong lens cores, or should it be larger (e.g., 4 arcsec)?

9. **Annulus range 20-40 pixels**: This equals 5.2-10.5 arcsec. Is this the right range for detecting lensing arcs? Paper IV may use different radii.

10. **AUC threshold of 0.70**: We flag features as shortcuts if AUC > 0.70. Is this threshold appropriate? Should it be 0.60 or 0.75?

11. **Missing shortcut features**: Should we also check:
    - Azimuthal symmetry (rings vs arcs)
    - Color gradients (arcs are often blue)
    - Edge sharpness / contrast
    - PSF concentration

12. **Normalization validation**: We don't currently validate that cutouts are consistently normalized. Should we add checks for:
    - Consistent flux scale across bands
    - No extreme outliers (e.g., saturated pixels)
    - Proper background level

---

## Summary of Questions (Q1-12)

| # | Topic | Question |
|---|-------|----------|
| 1 | Stratified | Does Paper IV use 100:1 per stratum? |
| 2 | Stratified | Should TYPE_BINS exclude EXP now? |
| 3 | Stratified | Is (nobs_z, type) the right stratification? |
| 4 | Stratified | Is N2 backfill from N1 correct? |
| 5 | Stratified | Is seed=42 sufficient for reproducibility? |
| 6 | Validation | Is 2% NaN threshold appropriate? |
| 7 | Validation | Should we verify band ordering explicitly? |
| 8 | Validation | Is 8-pixel core radius appropriate? |
| 9 | Validation | Is 20-40 pixel annulus range correct for arcs? |
| 10 | Validation | Is AUC > 0.70 the right shortcut threshold? |
| 11 | Validation | What shortcut features are we missing? |
| 12 | Validation | Should we validate normalization consistency? |

---

## Requested Output

1. **Stratified sampling audit**: Confirm the 100:1 logic matches Paper IV, or specify corrections.

2. **TYPE_BINS fix**: Confirm whether to remove EXP from the strata list.

3. **Validation thresholds**: For each threshold (NaN, core radius, annulus, AUC), confirm or provide Paper IV values.

4. **Missing QC checks**: List any critical quality checks we're missing.

5. **Shortcut detection improvements**: Recommend additional features to monitor.

LLM:

## Paper IV parity checkpoints that matter for these two modules

From Paper IV (Inchausti et al. 2025), the *nonlens construction* is described as:

* Require **at least 3 exposures in each of g, r, z**.
* Restrict to the **DECam footprint** described there as **δ < +32°** (Paper IV does not clearly state a hard southern bound in the text you quoted, even if DECaLS has practical limits).
* Use **Tractor TYPE = SER, DEV, REX** for the *nonlens* sampling procedure they describe (they explicitly mention those three types in the nonlens selection paragraph; EXP shows up for lenses but is not clearly included in the nonlens binning statement).

Those points drive the key answers below.

---

# 1) Stratified sampling audit: `spark_stratified_sample.py`

### Q1. Does Paper IV use 100:1 per stratum?

Paper IV’s description supports **100:1 as the training-set class ratio** and describes **binning nonlenses by TYPE and by z-band exposure count** to control distribution shift. It does **not** (in the text) claim “100:1 inside every stratum” as a formal requirement, but your interpretation is a *reasonable operationalization* of “match the nonlens distribution to the lens distribution across those bins”.

**Recommendation (parity-safe)**
Keep “targets per (nobs_z_bin, type_bin)” as you’re doing, but treat “100:1 per stratum” as a *goal*, not a hard constraint. When a stratum cannot supply enough negatives, you should:

1. record the shortfall explicitly, and
2. decide whether to backfill from adjacent strata (same TYPE, neighboring nobs_z_bin) rather than silently changing the within-stratum composition.

### Q2. TYPE_BINS mismatch: should EXP be removed?

**Yes for Paper IV parity**: set `TYPE_BINS = ["SER", "DEV", "REX"]` for the stratification bins.

If you keep EXP anywhere, do it knowingly and label it as a deliberate deviation. Otherwise you will fail a reviewer’s “did you actually match Paper IV?” sniff test.

### Q3. Is (nobs_z, type) the right granularity? Add seeing/depth?

For Paper IV parity: **(nobs_z_bin, type_bin)** is the correct minimum.

For your *new* contribution: adding **PSF FWHM bin** and/or **depth bin** is defensible, but only if you can still fill bins without collapsing into low-N noise. If you add them now, do it as an **ablation**:

* Baseline: stratify by (nobs_z, type) only
* Variant: stratify by (nobs_z, type, psf_bin) or (nobs_z, type, depth_bin)

### Q4. N2 backfill logic (when N2 insufficient): backfill from N1?

Backfilling is fine, but the way you do it matters.

**Best practice for your stated goal (“85:15 N1:N2 within negatives”)**

* Treat 85:15 as a **global constraint**, not per-stratum.
* Sample N2 wherever you can (across all strata), then allocate N1 per-stratum to hit the 100:1 ratio and distribution match.
* If you force 15% N2 per stratum, you will either (a) fail in sparse strata or (b) end up with weird N2 criteria to “manufacture” confusers in places they do not naturally occur.

Given your observed N2 rate (~5.8%), I would **not** force 15% per stratum. Make it global and report the achieved per-stratum N2 fraction.

### Q5. seed=42 determinism: sufficient?

Not sufficient by itself in Spark, because:

* `.sample()` + `.limit()` is **not guaranteed stable** across partitioning changes.
* repeated `.count()` calls trigger re-computation and can change sampling order if the plan changes.

**Make it deterministic**:

* Add a stable per-row key (you already have `galaxy_id, sweep_file, row_index`).
* Use `rand(seed)` or a hash of (`galaxy_id`, `seed`) to define an ordering, then take the first K per group using a Window.

Concretely: `row_number() over (partition by nobs_z_bin,type_bin,pool order by hash(galaxy_id, seed)) <= target`.

---

# 2) Cutout validation audit: `spark_validate_cutouts.py`

### Q6. NaN threshold (2% in central 50×50): too strict/lenient?

**2% in central 50×50 is reasonable** as a default. Two improvements:

* Add a stricter condition for the core: e.g., **core NaN frac must be 0** (or <0.1%) because the core is where many shortcuts live.
* Track *bandwise* NaN fraction; one missing band should be an automatic reject if you require grz.

### Q7. Band ordering verification: add explicit check?

Yes, but not by “brightness heuristics” (too unreliable). Do it by **provenance**:

* Store the requested band string (“grz”) in metadata.
* In FITS: check header keywords if present (sometimes the cutout service encodes band order).
* Add a unit test that downloads one known cutout and asserts the channel order matches expectations.

### Q8. Core radius = 8 pixels (~2.1″): appropriate?

For shortcut monitoring: **8 px is fine**, but it is not enough coverage for θ_E up to 3″ if you’re trying to distinguish “core-only” vs “arc-annulus” signals.

I recommend you compute features at **multiple radii**:

* core r = 4 px (~1.0″)
* core r = 8 px (~2.1″)
* core r = 12 px (~3.1″)

Then you can tell whether the “shortcut” is truly central or leaks outward.

### Q9. Annulus 20–40 px (5.2–10.5″): correct for arcs?

For your θ_E range (0.5″–3.0″), **20–40 px is wrong** for “where arcs appear” in most cases. It is way too far out.

If you don’t have θ_E per object, use fixed radii consistent with the training support:

* arc annulus: **r = 4–16 px** (1.0″–4.2″) as the primary
* outer annulus: r = 16–28 px (4.2″–7.3″) as secondary

If you *do* have θ_E for injected data (you will in injection-recovery), then define annulus relative to θ_E:

* arc band: r ∈ [0.8 θ_E, 1.6 θ_E] converted to pixels, clamped to sensible bounds.

### Q10. Shortcut AUC threshold 0.70: appropriate?

Keep 0.70 as a **red flag**, but add a **yellow flag at 0.60**.

Also: AUC point estimates will bounce with sample size. Add a cheap uncertainty:

* bootstrap AUC with 200–500 resamples, store mean and 68% CI.
  Flag “shortcut” if lower CI bound > 0.60 (yellow) or > 0.70 (red).

### Q11. Missing shortcut features?

Add these (high yield, low complexity):

1. **High-frequency core energy** (Laplacian / DoG) in core and arc annulus (you already used this gate elsewhere; reuse it here).
2. **Azimuthal asymmetry** after radial profile subtraction: e.g., standard deviation of residuals in angular bins (captures “inner-image asymmetry” and ring-like artifacts).
3. **Blue-ness of arc annulus**: (g−r) and (r−z) measured in the arc annulus minus the core (blue arcs vs red LRG core).
4. **Saturation / extreme pixel fraction**: fraction of pixels above the 99.9 percentile (detects bright-star ghosts/spikes).
5. **Edge artifact score**: difference between border median and central background (cutout edge issues).

### Q12. Normalization validation checks?

Yes. You want to detect “inconsistent flux scale” early:

* per-band percentiles (p1, p50, p99) and ensure they’re finite
* per-band robust sigma (MAD)
* clip fraction if you do clipping
* background median in an outer annulus

Store these as columns so you can later correlate model errors with normalization pathologies.

---

# Specific fixes to implement now (highest impact)

## Priority 1 (must fix)

1. **Update TYPE_BINS** in stratified sampling to `["SER","DEV","REX"]` for Paper IV parity.
2. **Replace annulus 20–40 px** in validation with radii that actually cover θ_E support (e.g., 4–16 px).
3. Make stratified sampling **deterministic via hash ordering + window**, not `.sample()+limit`.

## Priority 2 (strongly recommended)

4. Multi-radius core features (r=4/8/12) and bootstrap AUC for shortcut flags.
5. Add high-frequency and azimuthal asymmetry features into `extract_shortcut_features`.

## Priority 3 (nice-to-have)

6. Add “normalization sanity stats” and band-provenance checks.

---

# Answering your 12 questions directly (concise)

1. 100:1 per stratum: acceptable operationalization; treat as goal with explicit shortfall handling.
2. TYPE_BINS: remove EXP for Paper IV parity.
3. Stratification: (nobs_z,type) is minimum parity; add psf/depth only as ablation.
4. N2 backfill: OK, but enforce 85:15 globally, not per stratum.
5. seed=42: not enough; use stable hash ordering + window for determinism.
6. 2% NaN central: OK; add stricter core NaN constraint and bandwise thresholds.
7. Verify band order: yes, via provenance/header and unit test, not brightness heuristics.
8. Core r=8 px: fine; add r=4 and r=12 variants.
9. Annulus 20–40 px: wrong; use 4–16 px (or θ_E-relative when available).
10. AUC>0.70: keep as red; add yellow at 0.60 and bootstrap CI.
11. Missing shortcut features: add Laplacian/DoG energy, azimuthal asymmetry, arc-annulus color, saturation fraction, edge artifact score.
12. Normalize validation: yes; store per-band percentiles, MAD, outer-annulus background.

If you want, I can patch these two scripts in-place based on the zip you provided (deterministic stratified sampling + corrected radii and additional shortcut features) and return a new zip with the modifications.

=================================

# LLM Update - 2026-02-09

## Context

This is an update to our ongoing conversation about the `stronglens_calibration` project. The previous conversation ended with your recommendations on:
1. Stratified sampling (spark_stratified_sample.py)
2. Cutout validation (spark_validate_cutouts.py)

Since then, we have made progress and encountered issues that need your review before proceeding.

---

## Summary of Changes Since Last Conversation

### 1. N2 Classification Fix (CRITICAL)

**Problem Discovered:** The `classify_pool_n2()` function in `emr/sampling_utils.py` was producing **0% N2** (hard confusers) when run against DR10 data. All negatives were being classified as N1.

**Root Cause:** Thresholds were too restrictive:
- `flux_r_min: 10 nMgy` (only ~5% of DR10 galaxies have flux this high)
- `shape_r_min: 2.0 arcsec` (only ~5% have half-light radius this large)
- `g_minus_r_max: 0.4 AND r_mag_max: 19.0` (very few galaxies are both this blue AND this bright)

**Fix Applied:** Recalibrated thresholds against realistic DR10 distributions:

| Category | Old Threshold | New Threshold | Result |
|----------|---------------|---------------|--------|
| ring_proxy | DEV only, flux ≥ 10 | DEV/SER, flux ≥ 5, sersic ≥ 4.0 (SER) | 6.6% |
| edge_on_proxy | EXP only, shape_r ≥ 2.0 | Any type, ellipticity ≥ 0.55, shape_r ≥ 0.6 | 0.5% |
| blue_clumpy | g-r ≤ 0.4, mag ≤ 19.0 | g-r ≤ 0.4, mag ≤ 20.5 | 5.7% |
| large_galaxy | (new category) | shape_r ≥ 2.0, flux ≥ 3.0 | 3.7% |
| bright_core | (removed) | Redundant with ring_proxy | - |

**New N2 Rate:** 16.5% (target was 10-25%, previously 0%)

**Files Modified:**
- `emr/sampling_utils.py` - Updated `classify_pool_n2()` with new thresholds
- `configs/negative_sampling_v1.yaml` - Updated configuration
- `tests/test_n2_classification.py` - Updated test cases (23/23 pass)

### 2. Step 1 Crossmatch Complete

**Status:** ✅ COMPLETE

**Results:**
- 4,788/5,104 positives matched to DR10 Tractor (93.8%)
- 316 unmatched (outside DR10 coverage)
- Median separation: 0.059"
- Tier-A: 389 confident lenses
- Tier-B: 4,399 probable lenses
- Output: `s3://darkhaloscope/stronglens_calibration/positives_with_dr10/20260208_180524/`

### 3. New Documentation Created

- `docs/TECHNICAL_SPECIFICATIONS.md` - NPZ format, manifest schema, split logic, training config details
- `docs/FULL_PROJECT_CONTEXT.md` - Comprehensive project overview
- `docs/LESSONS_LEARNED.md` - Migrated from dark_halo_scope

### 4. Implementation Status

| Step | Status | Notes |
|------|--------|-------|
| Positive crossmatch | ✅ Done | 4,788 matched |
| N2 classification fix | ✅ Done | 16.5% N2 rate |
| Negative sampling (EMR) | ❌ Not run | Ready to run with fixed thresholds |
| Stratified 100:1 sampling | ❌ Not run | Blocked by negative sampling |
| Cutout generation | ❌ Not run | Blocked by stratified sampling |
| Training manifest | ❌ Not run | Blocked by cutouts |
| Training | ❌ Not run | Blocked by manifest |

---

## Your Previous Recommendations - Implementation Status

### Priority 1 (Must Fix)

| Recommendation | Status | Notes |
|----------------|--------|-------|
| Update TYPE_BINS to ["SER","DEV","REX"] | ✅ Done | EXP excluded per Paper IV |
| Replace annulus 20-40 px with 4-16 px | ✅ Done | In spark_validate_cutouts.py |
| Make stratified sampling deterministic | ⏳ Partial | Need to verify hash ordering implementation |

### Priority 2 (Strongly Recommended)

| Recommendation | Status | Notes |
|----------------|--------|-------|
| Multi-radius core features (r=4/8/12) | ✅ Done | CORE_RADII = [4, 8, 12] in validation |
| Bootstrap AUC for shortcut flags | ☐ Pending | Not yet implemented |
| High-frequency and azimuthal asymmetry features | ⏳ Partial | Azimuthal asymmetry added |

### Priority 3 (Nice-to-have)

| Recommendation | Status | Notes |
|----------------|--------|-------|
| Normalization sanity stats | ✅ Done | Per-band percentiles, MAD stored |
| Band-provenance checks | ☐ Pending | Unit test needed |

---

## Files Included in This Update

### Modified Files (since last conversation)
1. `emr/sampling_utils.py` - N2 classification fix
2. `configs/negative_sampling_v1.yaml` - Updated N2 thresholds
3. `tests/test_n2_classification.py` - Updated tests

### New Files
1. `docs/TECHNICAL_SPECIFICATIONS.md` - Technical specs document
2. `docs/FULL_PROJECT_CONTEXT.md` - Project overview
3. `docs/LESSONS_LEARNED.md` - Migrated lessons

---

## Questions for LLM Review

### Critical Decision: Should We Proceed with EMR Negative Sampling?

**Context:** We are ready to run `spark_negative_sampling.py` on EMR with the fixed N2 thresholds. This will process ~114M galaxies from DR10 sweeps.

**Question 1:** Given the N2 category distribution below, is this ready for production?

```
ring_proxy:    6.6% (bright DEV/SER with n≥4)
blue_clumpy:   5.7% (g-r ≤ 0.4, mag ≤ 20.5)
large_galaxy:  3.7% (shape_r ≥ 2.0", flux ≥ 3 nMgy)
edge_on_proxy: 0.5% (ellipticity ≥ 0.55)
Total N2:     16.5%
```

**Concerns:**
- Edge-on proxy is low (0.5%) - should we loosen ellipticity threshold?
- We removed bright_core category - was this correct?
- Is 16.5% N2 appropriate, or should we aim for exactly 15%?

### Question 2: N2 Category Completeness

Are we missing any important confuser categories that Paper IV or other lens-finding papers identify? Specifically:
- Spiral galaxies with prominent arms
- Mergers/disturbed morphology
- Star-forming clumps that could look like multiple images
- AGN/QSO hosts with point source cores

### Question 3: Stratified Sampling Determinism

You recommended using hash ordering + window for determinism. Here's our current implementation in `sampling_utils.py`:

```python
def assign_split(healpix_idx: int, allocations: Dict[str, float], seed: int = 42) -> str:
    hash_input = f"{healpix_idx}_{seed}"
    hash_bytes = hashlib.sha256(hash_input.encode()).digest()
    hash_value = int.from_bytes(hash_bytes[:4], "big") / (2**32)
    # ... assign based on cumulative thresholds
```

Is this implementation correct for spatial splits? For stratified sampling within strata, should we use a similar approach?

### Question 4: Cutout Size Decision

We currently have two sizes in the codebase:
- 101×101 pixels (downloaded from Legacy Survey)
- 64×64 pixels (center-cropped for training)

Paper IV uses what size? Should we:
a) Train on 64×64 (faster, matches some prior work)
b) Train on 101×101 (more context, larger field of view)
c) Train on both as ablation

### Question 5: Label Handling Implementation

We have not yet implemented the tier-based sample weights you recommended:
- Tier-A (confident): weight 0.9-1.0
- Tier-B (probable): weight 0.3-0.6

Before we implement, please confirm:
1. Is this applied as `sample_weight` in BCE loss?
2. Should we also apply label smoothing (1.0 → 0.95) for Tier-B?
3. How do we handle the 822 spectroscopic matches (DESI DR1) - are these separate from Tier-A?

### Question 6: Selection Function Grid

You previously confirmed the grid:
- θE: 0.5-3.0" in 0.25" steps (11 bins)
- PSF: 0.9-1.8" in 0.15" steps (7 bins)
- Depth: 22.5-24.5 in 0.5 mag steps (5 bins)
- Total: 385 cells, minimum 200 injections per cell

**Question:** Since we're doing real-image training (not injection-based training), when do injections happen?
a) After training, for selection function measurement only?
b) During validation, to test model response to synthetic lenses?
c) Both?

### Question 7: GO/NO-GO for Next Step

Given the current state:
- ✅ Positive crossmatch complete (4,788 matched)
- ✅ N2 classification fixed (16.5% rate)
- ✅ HEALPix spatial splits implemented
- ✅ Exclusion radius implemented (11")
- ☐ Full negative sampling not yet run
- ☐ Stratified 100:1 sampling not yet run
- ☐ Cutouts not yet generated

**Should we proceed with running `spark_negative_sampling.py` on EMR?**

Please provide:
1. Any blocking issues that must be fixed first
2. Any non-blocking issues to track
3. Expected output validation checks

---

## Appendix: Key Code Excerpts

### A. N2 Classification (emr/sampling_utils.py)

```python
DEFAULT_N2_THRESHOLDS = {
    "ring_proxy": {
        "types": ["DEV", "SER"],  # DEV always qualifies, SER needs high sersic
        "flux_r_min": 5.0,        # ~mag 20.8
        "sersic_min": 4.0,        # Very high concentration (SER only)
    },
    "edge_on_proxy": {
        "types": ["EXP", "SER", "DEV"],
        "ellipticity_min": 0.55,
        "shape_r_min": 0.6,
        "shape_r_min_legacy": 2.0,  # Fallback if no ellipticity
    },
    "blue_clumpy_proxy": {
        "g_minus_r_max": 0.4,
        "r_mag_max": 20.5,
    },
    "large_galaxy_proxy": {
        "shape_r_min": 2.0,
        "flux_r_min": 3.0,
    },
}
```

### B. Spatial Split Logic (emr/sampling_utils.py)

```python
def compute_healpix(ra: float, dec: float, nside: int) -> int:
    import healpy as hp
    theta = np.radians(90.0 - dec)
    phi = np.radians(ra)
    return int(hp.ang2pix(nside, theta, phi, nest=True))

def assign_split(healpix_idx: int, allocations: Dict[str, float], seed: int = 42) -> str:
    hash_input = f"{healpix_idx}_{seed}"
    hash_bytes = hashlib.sha256(hash_input.encode()).digest()
    hash_value = int.from_bytes(hash_bytes[:4], "big") / (2**32)
    cumulative = 0.0
    for split, proportion in sorted(allocations.items()):
        cumulative += proportion
        if hash_value < cumulative:
            return split
    return "train"
```

### C. Cutout NPZ Format

```python
# From emr/spark_generate_cutouts.py
np.savez_compressed(
    buffer, 
    cutout=cutout,  # shape (101, 101, 3), channels: g/r/z
    **{f"meta_{k}": v for k, v in metadata.items()}
)

# Metadata keys:
# meta_galaxy_id, meta_ra, meta_dec, meta_type, meta_nobs_z,
# meta_psfsize_z, meta_flux_r, meta_cutout_url, meta_download_timestamp
```

---

## Requested Output

1. **N2 thresholds validation:** Are the new thresholds appropriate? Any adjustments needed?

2. **Missing confuser categories:** List any N2 categories we should add.

3. **GO/NO-GO decision:** Can we proceed with EMR negative sampling?

4. **Cutout size recommendation:** 64×64 vs 101×101 for training.

5. **Label weighting confirmation:** Exact implementation for tier-based sample weights.

6. **Injection timing:** When should injections be used in the real-image training pipeline?

7. **Any other blocking issues** before running the EMR job.

---

*Update prepared: 2026-02-09*

LLM replied:

([Past chat][1])([Past chat][2])([Past chat][1])([Past chat][2])

## Paper IV parity facts that affect your next step

From Paper IV:

* Training/validation cutouts are **101×101 pixels** (≈26″×26″). 
* Training cuts include **z < 20.0 mag**, **δ < +32°**, and **≥3 exposures in each of g/r/z**. 
* Nonlenses are selected **per type (SER, DEV, REX)** and **binned by nobs_z**, maintaining **~100:1** proportionality to lenses in each bin. 
* Their train/val split is **70% / 30%** (not 70/15/15) while keeping ~100:1 in each split. 
* They include EXP in the *lens* training sample (small count), but their *nonlens selection sentence* explicitly names SER/DEV/REX.  

That last point is an ambiguity: the safest “Paper IV parity” move is: **stratify nonlenses by (nobs_z_bin, type) over SER/DEV/REX**, while still allowing EXP elsewhere (deployment/eval) as your pipeline already does.

---

## Q1. N2 thresholds: are you ready to run EMR negative sampling?

### GO with minor tweaks (non-blocking)

Your new N2 rate (**16.5%**) is in the right operational range. Do **not** try to hit exactly 15%. For training, what matters is: (a) you have enough hard confusers to prevent shortcut learning, (b) they are not dominated by one proxy, (c) they exist across your strata.

### The one thing I would change before the EMR run

**Make edge-on less rare without making it junk:**

* If you have axis-ratio-ish info (ellipticity from shape_e1/e2), loosen `ellipticity_min` from **0.55 → 0.50** and add a weak size cut (you already have `shape_r_min`).
* Keep a cap so it does not flood N2. A practical target is **edge_on ~2–4% of N2**, not necessarily more.

### Bright_core removal

Removing “bright_core” is fine if it was redundant with ring_proxy/large_galaxy. Bright-core by itself often becomes a shortcut, and you want confusers to be *morphological* more than “high SNR center”.

**Conclusion:** proceed. The current thresholds are not a blocker.

---

## Q2. Missing confuser categories to add (high value)

Paper IV does not give a formal N2 taxonomy, but your goal is robustness and failure-mode accounting. Add these as *additional N2 proxies* (even if noisy):

1. **Merger/tidal / multi-component proxy**

   * Use Tractor indicators that correlate with poor fits or blends (examples: unusual fitbits patterns, extreme FRACFLUX/FRACIN, multiple nearby bright neighbors within a few arcsec, large residual-like flags if available in sweeps).
   * If you do not have those columns in sweeps, do a geometric proxy: “has a bright neighbor within 3–6″” using your sweep partition and a fast spherical match.

2. **Spiral-arm / ringlike-but-not-ETG proxy**

   * Color + size: moderately blue (`g-r <= 0.7`) and moderate size (`shape_r >= 0.8`) but not extreme; optionally require EXP type if you decide to include EXP in N2 only.

3. **AGN/QSO host proxy**

   * Not PSF objects, but “pointy cores”: very high central concentration. If you have shape/sersic, use high sersic plus compact size; otherwise use flux ratios (core vs annulus) *after cutouts exist* (this might be better added in the cutout-validation stage rather than sweep-only).

4. **Bright-star artifact neighborhood**

   * You already exclude maskbits, but keep a *separate* “artifact-nearby” confuser category for training/eval if maskbits do not fully remove halos/ghosts. This can be done by crossmatching to Gaia bright stars and keeping a small curated pool.

If you add only one new category before EMR: add **merger/neighbor**. It tends to create the most painful false positives at scale.

---

## Q3. Stratified sampling determinism

Your HEALPix split hashing approach is directionally correct, but fix one subtle issue:

* In `assign_split`, you currently iterate `for split, proportion in sorted(allocations.items()):`
  This sorts by split name, not by your intended order. Make the order explicit: `["train","val","test"]` (or whatever you use), otherwise a rename can silently change allocation behavior.

For deterministic *within-stratum sampling*, do the same pattern:

* Compute a stable hash on `(galaxy_id, seed)` (or `(brickname,objid)`), convert to uniform [0,1), then select the smallest K per stratum. This avoids Spark sampling nondeterminism.

---

## Q4. Cutout size: 64×64 vs 101×101

Paper IV trains/validates on **101×101**. 

Recommendation:

* **Primary baseline:** train on **101×101** for parity.
* **Ablation:** center-crop to 64×64 and report the delta. (This is a clean, publishable “efficiency vs performance” result.)

---

## Q5. Tier weighting: exact implementation

Use **sample weights in the loss** as the primary mechanism.

* Labels: keep `y=1` for positives, `y=0` for negatives.
* Weights:

  * Literature-confirmed / strongest Tier-A: **1.0**
  * Your “confident” Tier-A (if not fully confirmed): **0.9–1.0**
  * Tier-B “probable”: **0.3–0.6** (pick one and justify)

Avoid doing both strong label smoothing and strong down-weighting at the same time. If you want label smoothing at all:

* Only for Tier-B, change target from 1.0 → **0.8–0.9**, while keeping a modest weight (for example 0.6). That is defensible as “positive-unlabeled noise mitigation”.

On the “822 DESI DR1 spectroscopic matches” question: treat them as **confirmed positives** if they are genuinely spectroscopically selected/confirmed systems; otherwise treat as an intermediate tier with explicit criteria. Do not merge categories until you can state the label provenance cleanly.

---

## Q6. When do injections happen in a real-image pipeline?

Use injections in **two places**, but never silently mix them into the “real-only baseline”:

1. **After training, for selection-function measurement (primary use).**
   Inject into representative real galaxies across (PSF, depth, nobs, etc.) and measure recovery.

2. **Optional stress test (secondary):** use a small injected validation set to probe sensitivity to θE/flux/PSF extremes.

Keep the “real-only training baseline” intact so reviewers cannot claim your baseline is confounded.

---

## Q7. GO/NO-GO for running `spark_negative_sampling.py`

### GO, assuming these are already true

* You enforce **δ bounds** and **nobs_g/r/z ≥ 3** and **z < 20** for the pool you want to match Paper IV. 
* Your exclusion against known lenses/candidates is working (counts and spot checks).

### Blocking issues (must fix before EMR)

1. **Split assignment ordering bug** (sorting allocations lexicographically). Fix as above.
2. **TYPE bin policy decision** for “Paper IV parity”:

   * If you want strict parity for nonlens selection: stratify on **SER/DEV/REX** per the text. 
   * If you keep EXP in negatives, document it explicitly as a divergence.

### Non-blocking issues to track

* Edge-on underrepresentation (tune after you see per-stratum counts).
* Add merger/neighbor proxy in the next iteration if you do not have time now.

### Expected output validation checks (do these immediately after the run)

1. **Filter parity counters**: report how many removed by each criterion (dec cut, nobs per band, z-mag, maskbits).
2. **Per-stratum availability**: counts by `(nobs_z_bin, type_bin, pool, split)` to ensure N2 exists across bins.
3. **Exclusion sanity**: random sample of negatives near positives should be near-zero within your radius.
4. **Distribution sanity**: compare negatives vs positives distributions for (nobs_z, type) because Paper IV explicitly preserves proportionality by these bins. 

---

## Bottom line

* **Proceed with EMR negative sampling now** after fixing the split-assignment ordering and locking the type-bin policy for Paper IV parity.
* Your N2 fix is directionally correct; do not over-optimize the exact N2 percentage before you have per-stratum diagnostics.
* Train the baseline on **101×101** first (Paper IV parity), then run the 64×64 crop ablation. 

[1]: https://chatgpt.com/c/698a40ca-0f3c-832f-a954-d4d773edb275 "Research Assistant Specifications"
[2]: https://chatgpt.com/c/697ce7ae-1f00-8327-bc88-0336b7f5c68f "Strong Lens - Dark Halo New Session 1"


============

# LLM Review: Training & Evaluation for Strong Lens Calibration (DESI DR10)

**Purpose:** We are preparing a paper on a detector audit / selection function for strong lens finding in DESI Legacy Survey DR10. Our model training and evaluation section must be comparable in rigor and correctness to the strong lens papers we reference (e.g. Huang et al., DESI Strong Lens Foundry, single-fiber/pairwise spectroscopic search papers). We need your careful review of the **code**, **metrics**, and **conclusions**, and your **Go/No-go** for next steps.

**What we are sharing:** (1) Full training **input** setup and **training** metrics; (2) Full **evaluation** metrics and methodology; (3) Our **conclusions** and an **honest audit** we already performed; (4) A **zip of relevant code** (`llm_review_training_eval.zip`) with a manifest (`MANIFEST.md`) explaining each file. This prompt is also included inside the zip at `llm_review_package/LLM_PROMPT_TRAINING_AND_EVAL_REVIEW.md`. We ask you to review the code for issues that could derail honest conclusions, answer direct clarity questions, and—if you can—provide **fully generated code** (e.g. scripts for split verification, bootstrap intervals) as **download links** or inline.

---

## PART A: Training input (data and config)

### A.1 Data sources and counts

| Item | Value |
|------|--------|
| **Manifest** | Single parquet: `training_v1.parquet` (path on training host: `/lambda/nfs/darkhaloscope-training-dc/stronglens_calibration/manifests/training_v1.parquet`). |
| **Total samples** | 416,449 |
| **Positives** | 4,788 |
| **Negatives** | 411,661 |
| **Neg:pos ratio** | ~86:1 |
| **Tier-A (confident) count** | 389 (sample_weight = 1.0 in loss) |
| **Tier-B (probable) count** | 4,399 (sample_weight = 0.5) |
| **Positive cutout source** | S3: `s3://darkhaloscope/stronglens_calibration/cutouts/positives/20260208_205758/` (5,101 .npz), synced to Lambda NFS for training. |
| **Negative cutout source** | S3: `s3://darkhaloscope/stronglens_calibration/cutouts/negatives/20260209_040454/` (~416K .npz), synced to Lambda NFS. |
| **Split assignment** | From NPZ metadata or manifest generator; exact method (HEALPix vs stratified, 70/15/15 vs 80/10/10) to be verified. Train/val/test disjointness **assumed** from upstream process, **not** verified by a script in this package. |
| **Seed** | 1337 (dataset/config). |

### A.2 Preprocessing and augmentation

| Item | Value |
|------|--------|
| **Preprocessing** | `raw_robust`: outer-annulus (r=20–32 px) median/MAD normalization, then clip ±10. |
| **Input size** | Cutouts 101×101 (g,r,z); center-cropped to **64×64** for training. |
| **Augmentation** | HFlip, VFlip, Rot90 (no brightness/contrast). |

### A.3 Training config (YAML)

- **Config file:** `configs/resnet18_baseline_v1.yaml` (included in zip).
- **Architecture:** ResNet-18 (torchvision, weights=None), conv1 → 3 channels, fc → 1 output.
- **Optimizer:** AdamW, lr = 0.0003, weight_decay = 0.0001.
- **Batch size:** 256.
- **Max epochs:** 50; **early stopping patience:** 10 (on val AUC).
- **LR schedule:** CosineAnnealingLR, T_max=50.
- **Mixed precision (AMP):** true.
- **Loss:** BCEWithLogitsLoss with reduction='none', then per-sample weighted mean using `sample_weight` from manifest (Tier-A=1.0, Tier-B=0.5; negatives=1.0).

---

## PART B: Training metrics (from run)

| Metric | Value |
|--------|--------|
| **Host** | Lambda Labs GPU, NFS mount `darkhaloscope-training-dc`. |
| **Epochs completed** | 16 (early stopped). |
| **Best epoch** | 6 (by val AUC). |
| **Best val AUC** | 0.9592 |
| **Final train loss (last epoch)** | 0.0013 |
| **Stop reason** | Early stopping: 10 consecutive epochs without improvement in val AUC. |
| **Command** | `python code/dhs/scripts/run_experiment.py --config code/configs/resnet18_baseline_v1.yaml` (cwd: project root on Lambda). |

---

## PART C: Evaluation metrics (test set, single run)

| Metric | Value |
|--------|--------|
| **Test samples** | 62,760 |
| **Test AUC (overall)** | 0.9579 |
| **Recall @ threshold 0.5 (overall)** | 0.4360 |
| **Precision @ 0.5 (overall)** | 0.7653 |
| **ECE (overall, 15 equal-frequency bins)** | 0.0027 |
| **MCE (overall)** | 0.0156 |
| **Tier-A in test (held-out)** | n = 48 (all positives). Recall @ 0.5 = 0.6042. We do **not** report ECE/MCE for this stratum (single-class; see honest audit). |
| **Tier-B in test** | n = 640 positives. Recall @ 0.5 = 0.4234. |

**Note on “Tier-A”:** These are rows in the **test** set with `tier == A` (confident lenses with DR10 match). They are **held out from training** but come from the **same** imaging-candidate catalog (lenscat), not from a separate spectroscopic-search catalog. We use the term “held-out Tier-A evaluation,” not “independent spectroscopic validation” in the strong sense.

---

## PART D: Our conclusions (and honest audit)

1. **Metrics are not faked:** The evaluation code computes AUC, recall, precision, ECE, MCE as stated; inference order aligns with manifest rows; we evaluate on test only and select the model by val AUC.
2. **Overall ECE** is low (0.0027) but **dominated by the majority class** (negatives). It does **not** certify calibration on the **positive** class; for that we would need a separate analysis (e.g. reliability diagram in the high-score region or positive-only metrics with caveats).
3. **Tier-A / Tier-B ECE/MCE** on positive-only strata were misleading and are no longer reported in the eval script for single-class strata.
4. **“Independent spectroscopic validation”** was overstated; we now say “held-out Tier-A evaluation” and clarify that we do not have a separate spectroscopic-channel catalog.
5. **Split disjointness** (train ∩ test = ∅) is **assumed**, not verified by script. We recommend adding a verification step and documenting it.
6. **Uncertainty:** We report point estimates only. For publication we need at least bootstrap (or similar) intervals for AUC and recall @ 0.5 on test and on Tier-A.

We have documented these points in `EVALUATION_HONEST_AUDIT.md` (included in zip).

---

## PART E: Direct questions (we need your clarity)

1. **Split assignment:** Our manifest has a `split` column (train/val/test). The exact upstream method (HEALPix-based vs label-stratified, and ratio 70/15/15 vs 80/10/10) is not fully confirmed. For the paper, should we (a) trace and document the exact assignment (which script, which logic), or (b) run an explicit check that train and test IDs (e.g. `galaxy_id` or `cutout_path`) are disjoint, or both?

2. **Tier-A wording:** Is it scientifically defensible to call our Tier-A test subset “evaluation on held-out Tier-A anchors” and to **avoid** claiming “independent spectroscopic validation” unless we add a truly separate catalog (e.g. DESI single-fiber or pairwise spectroscopic lens candidates)? Do you see any remaining overclaim in our current wording?

3. **Calibration on positives:** For the paper, is it sufficient to state that “overall ECE summarizes calibration over the full (imbalanced) set and does not certify calibration on the positive class,” and to defer positive-class calibration to future work? Or do you recommend we add a specific analysis (e.g. reliability diagram restricted to high-score region or to positives only) before submission?

4. **Uncertainty quantification:** What minimal uncertainty reporting do you recommend for the methods section? (e.g. bootstrap 95% or 68% for test AUC and recall @ 0.5; and for Tier-A recall given n=48?)

5. **Rigor vs referenced papers:** In your view, what are the 2–3 most important gaps between our current training/evaluation description and the level of rigor in typical DESI strong lens / Huang et al. / Foundry papers (e.g. split description, uncertainty, calibration, or independence of validation)?

6. **Shortcut gates:** We have not yet run “annulus-only” and “core-only” classifier checks (checklist 4.7, 4.8). How critical are these for a first submission, versus leaving them as “future work”?

7. **Reproducibility:** We do not currently log git commit or run timestamp in the checkpoint directory. Should we add a small `run_info.json` (or similar) next to `best.pt` with commit hash, config path, and command for every future run?

---

## PART E2: Meticulous operations (anything else?)

We want to be thorough before and during the paper. Please consider and advise on any of the following we may have missed:

8. **Environment and versions:** We have not pinned Python, PyTorch, torchvision, scikit-learn, pandas, numpy, pyarrow versions in the package. Should we add a `requirements.txt` or conda `environment.yml` (with versions) and cite it in the paper for reproducibility? If so, what minimal set do you recommend?

9. **Evaluation output:** The full evaluation JSON (reliability curve bins, exact counts) from the Lambda run is not in the zip; only the summary numbers are in this prompt. Should we include a sanitized (path-stripped) copy of the eval JSON in the supplement or repo for reviewers?

10. **FPR by negative pool (N1/N2):** We have not run FPR stratified by N2 confuser category. Is that important to run before or alongside the selection function, or can it follow as failure-mode analysis (Phase 6)?

11. **Preprocessing consistency check:** We assert “same preprocessing at train and eval” but do not have a unit test that compares preprocess_stack output for a fixed input across calls. Should we add a small regression test or assert (e.g. checksum on a reference cutout) to lock preprocessing?

12. **Path and portability:** Training/eval use absolute paths in the config (Lambda NFS). For reproducibility, should we support a path override (e.g. env var or `--data_root`) so the same config can be run on another machine with different mount points?

13. **Any other gaps:** From your experience with method papers, what other operational details (logging, random seeds, checkpoint naming, artifact versioning) would you add so our pipeline is “meticulous” and defensible under review?

---

## PART F: Code review request

We are providing a zip of relevant code for **training** and **evaluation** (and upstream manifest generation). Please:

1. **Review the code carefully** and point out **major issues** that could derail our honest conclusions (e.g. data leakage, wrong metric formula, order mismatch between predictions and labels, misuse of splits, or any subtle bug that would invalidate reported numbers).

2. **Focus on:**  
   - Correctness of train/val/test usage (no test in training or model selection).  
   - Correctness of AUC, recall, precision, ECE/MCE implementation.  
   - Alignment of inference output order with manifest row order (for tier/stratum reporting).  
   - Any place where “independent” or “spectroscopic” validation is overstated in comments or docstrings.

3. **Manifest:** The zip includes a `MANIFEST.md` that explains each file and its intended behavior. Use it to navigate; if any file’s stated behavior does not match the code, please flag it.

---

## PART G: Conclusion and next steps

- **Our conclusion:** The evaluation code does not fudge numbers; the main issues were **wording** (“independent spectroscopic validation”) and **interpretation** (overall ECE as proof of calibration on positives; reporting ECE/MCE for positive-only strata). We have tightened terminology and stopped reporting misleading stratum ECE/MCE. For publication we still need: split disjointness verification, uncertainty quantification (bootstrap intervals), and possibly positive-class calibration analysis and shortcut gates.

- **Next steps we envision:**  
  1. Add a script to verify train/test ID disjointness and document result.  
  2. Add bootstrap (or similar) intervals for test AUC, test recall @ 0.5, and Tier-A recall @ 0.5; append to evaluation JSON/summary.  
  3. Add `run_info.json` (or equivalent) to checkpoint directory for reproducibility.  
  4. Optionally: reliability diagram or calibration analysis restricted to positives/high-score region.  
  5. Optionally: run annulus-only and core-only shortcut gates (4.7, 4.8).  
  6. Proceed to selection function (injection-recovery grid with frozen model) once the above are done.

**We need from you:**

1. **Assessment:** Based on the data and code we shared, do our **conclusions** look **correct**, **accurate**, **scientifically rigorous**, and **defensible** for a paper whose model-training section will be compared to strong lens papers we reference?

2. **Go/No-go:** Do you recommend a **Go** for the next steps above (with or without modifications), or a **No-go** until specific issues are fixed? If No-go, please list the blocking issues.

3. **Anything else:** Is there anything else we should do before writing the training/evaluation section of the paper?

4. **Fully generated code:** If possible, please provide **fully generated code** (e.g. Python script to verify train/test disjointness, or to compute bootstrap intervals for AUC and recall) as **download links** or paste the code inline so we can drop it into the repo and run it. We will attribute and use it for reproducibility.

---

## PART H: Selection function (injection-recovery) – complete code and watchouts

Our **next major step** after the above is the **selection function**: injection-recovery grid with the **frozen** ResNet-18 model to estimate completeness C(θE, PSF, depth) with uncertainty. We need **complete, runnable code** (or a detailed blueprint we can implement) and a clear list of **things to watch out for** so we do not undermine our conclusions.

### H.1 Specification (from our checklist)

- **Grid axes:**  
  - **θE (Einstein radius):** 0.5" to 3.0" in **0.25"** steps → **11 bins**.  
  - **PSF FWHM:** 0.9" to 1.8" in **0.15"** steps → **7 bins**.  
  - **Depth (e.g. psfdepth_r or galdepth_r):** 22.5 to 24.5 mag in **0.5** mag steps → **5 bins**.  
  - Total cells: **11 × 7 × 5 = 385**.

- **Minimum injections per cell:** **200** (so ≥ 77,000 injections total). Stratify by DR10 conditions so each cell has enough real cutouts with matching conditions (or document how we sample cutouts per cell).

- **Frozen detector:** Load `best.pt` (ResNet-18), **no training**. Run inference with **same preprocessing** as training (`raw_robust`, 64×64 center crop). Record score (sigmoid output) or binary detection at a chosen threshold.

- **Completeness:** Per cell, completeness = (number of injections with score ≥ threshold) / (total injections in cell). Report **Bayesian binomial 68%** intervals (optionally 95% in appendix). **Low-N bins:** do not smooth unless justified; **mark** cells with N < 200 as “insufficient” or merge adjacent cells with a stated rule.

- **Output:** Lookup table or artifact (e.g. C(θE, PSF, depth) with lower/upper bounds) in a public-release format.

### H.2 Injection realism (from our prior LLM guidance)

Injections must be **photometrically calibrated** to DR10:

- **Zeropoint:** AB zeropoint **22.5** (nanomaggies); we have `AB_ZEROPOINT_MAG = 22.5` in constants.
- **Source magnitude prior:** e.g. r-band 22–26 (unlensed), magnification proxy μ ~ 5–30; target **arc annulus SNR** in a range comparable to real Tier-A anchors (e.g. median within 0.5×–2×).
- **PSF:** Prefer **per-cutout PSF** when available; otherwise document use of brick/cell-average and impact.
- **Noise:** Add noise using **measured background** (e.g. MAD-based) from the same cutout, not a global constant.
- **Acceptance (GO/NO-GO):** Before trusting the selection function, we should pass: (1) arc annulus SNR distribution (injections vs real) within 0.5×–2× at median and percentiles; (2) color (g-r, r-z) median within ±0.2 mag; (3) noise histogram KS test p > 0.05; (4) visual sanity (injections not systematically “cleaner” than real).

**Base for injections:** Use **negative cutouts only** (no positive cutouts) so we are measuring “recovery of synthetic lenses on real non-lens backgrounds.” Do not use any cutout that appears in the training set for injection hosts (or document and justify if a subset is shared).

### H.3 What we are asking from you

1. **Complete code ** Please provide **full, runnable code** (or a step-by-step blueprint with pseudocode and file layout) for:  
   - (a) Building or loading the injection grid (θE, PSF, depth) and sampling/assigning negative cutouts to cells (with stratification if needed).  
   - (b) Generating synthetic lens injections on each cutout (flux in DR10 units, PSF, noise) and saving or streaming “injected” images.  
   - (c) Running the **frozen** model on injected images with the **same** preprocessing as training, recording scores (and optionally binary at threshold).  
   - (d) Aggregating by cell: completeness, counts, and **Bayesian binomial 68%** intervals.  
   - (e) Outputting a lookup table or structured artifact (e.g. CSV/Parquet with cell axes and C_lo, C, C_hi).  
   Prefer Python; we use PyTorch for the model and NumPy/Pandas for data. If you cannot provide full code, please give a **detailed blueprint** (modules, inputs/outputs, formulas) so we can implement it without guessing.

2. **Things to watch out for:** Please list **concrete pitfalls** that could derail an honest selection function, for example:  
   - Using wrong zeropoint or flux units so injections are not DR10-like.  
   - Using training or positive cutouts as injection hosts (leakage).  
   - Preprocessing mismatch (e.g. different normalization or crop) between training and injection scoring.  
   - Ignoring per-cutout PSF/depth so cell assignment is inconsistent with real conditions.  
   - Reporting completeness without uncertainty or with inappropriate smoothing.  
   - Any other subtle bug or methodological slip that would make the selection function invalid or overstated.

3. **Deliverable:** If possible, provide the code (or blueprint) and watchouts as **download links** or paste them inline. We will integrate and attribute them.

Thank you for a rigorous and honest review.

-----

LLM Response:

## A. Code and methodology review

### What looks correct and defensible

* **Model selection uses validation only.** Training uses early stopping on **val AUC** (not test), which is methodologically correct.
* **Loss weighting implementation is reasonable.** `BCEWithLogitsLoss(reduction="none")` followed by a per-sample weighted mean is the right way to apply Tier-A/Tier-B weights without distorting the logits.
* **Preprocessing is consistent in principle.** Your “outer-annulus median/MAD then clip then 64×64 center crop” pipeline is internally coherent, and augmentation is conservative (no brightness/contrast).
* **You already fixed the biggest evaluation mistake.** Not reporting ECE/MCE for single-class strata is correct.

### High-risk issues that can derail “honest conclusions”

1. **Split disjointness is assumed, not proven.** This is the only truly blocking risk because leakage can produce AUCs like yours without obvious red flags. You must verify disjointness by at least:

   * `galaxy_id` disjointness across train/val/test
   * `npz_path` disjointness across splits (catching accidental duplicate cutouts across splits)
2. **“Held-out Tier-A anchors” wording is fine; anything implying independent spectroscopic validation is not.** Ensure comments, variable names, plots, and captions do not reintroduce “spectroscopic validation” unless you actually evaluate on an external spectroscopic-selected catalog.
3. **ECE on the full imbalanced set is not evidence of positive-class calibration.** Your current interpretation is correct; for a paper, you should either:

   * add a positive-focused calibration check (recommended), or
   * explicitly scope calibration claims to “overall calibration dominated by negatives” and avoid any statement that sounds like calibrated probabilities for lenses.

## B. Answers to your direct questions

1. **Split assignment: trace, verify, or both?**
   **Both.** For a methods paper: (i) trace and document the exact split mechanism (HEALPix/hash and proportions), and (ii) run and report a disjointness check. The check is the hard guarantee reviewers trust.

2. **Tier-A wording**
   Yes: “**held-out Tier-A evaluation**” / “**held-out anchors from the same imaging-derived catalog**” is defensible. Avoid “independent” and avoid “spectroscopic” unless the evaluation set is sourced from an external spectroscopic channel.

3. **Calibration on positives: is deferring OK?**
   For a detector audit paper, deferring is survivable, but I recommend adding **one minimal positive-relevant calibration analysis** now:

   * reliability diagram or calibration error **restricted to high-score region** (e.g. top 0.1–1% scores), and/or
   * report **precision-recall** plus calibration only for bins with non-trivial positive counts.
     This is a small addition and prevents reviewers from attacking the ECE result.

4. **Minimal uncertainty reporting**
   Do at least **bootstrap intervals** (prefer 68% and/or 95%) for:

   * test AUC
   * test recall at your operating threshold (0.5 or your chosen working point)
   * Tier-A recall at the same threshold (n=48 needs uncertainty stated very clearly)

5. **Top rigor gaps vs typical DESI lens papers**

   * **Split provenance + leakage guardrails** (explicit, verified)
   * **Uncertainty intervals** (bootstraps / binomial)
   * **Independence of validation** (either truly independent catalog or clearly scoped “held-out from same channel”)

6. **Shortcut gates (core-only / annulus-only) criticality**
   Not a hard blocker for an initial submission, but they are high value. If you do only one, do the **core-only** check (it most directly probes central-brightness shortcuts). If time is tight, put them in “future work” but do not overclaim robustness.

7. **Reproducibility metadata**
   Yes. Add `run_info.json` next to `best.pt` with: git commit, timestamp, config content/hash, command, environment summary, random seeds.

8. **Environment pinning**
   Yes. Add a minimal `requirements.txt` or `environment.yml` with pinned versions for: python, torch, torchvision, numpy, pandas, pyarrow, scikit-learn, astropy. This is expected in rigorous ML methods work.

9. **Include evaluation JSON**
   Yes, include a sanitized eval JSON (no absolute paths), at least as supplementary material or a tagged release artifact.

10. **FPR by negative pool/category**
    Strongly recommended but not blocking. It becomes very important when you present the selection function and failure modes.

11. **Preprocessing regression test**
    Recommended. A tiny checksum-style test on 1–2 reference NPZs prevents silent drift.

12. **Path portability**
    Recommended. Add `--data_root` or environment override so configs are portable.

13. **Other operational gaps**

* deterministic dataloader seeding and logging of seeds
* consistent artifact naming (model id ties to manifest id)
* store the exact manifest used for a run inside the checkpoint directory (or hash + immutable location)

## C. Go / No-go

**No-go for paper claims until you run and report split disjointness checks.**
After that, **Go** for writing the training/evaluation section, provided you also add bootstrap intervals. The positive-class calibration analysis is “strongly recommended” but can be scoped if necessary.

## D. Downloadable scripts (disjointness, bootstrap intervals, selection function runner)

I generated a small add-on package with:

* `scripts/verify_splits.py` (train/val/test disjointness by `galaxy_id` and `npz_path`)
* `scripts/bootstrap_eval.py` (bootstrap CIs for AUC/recall/precision from saved predictions)
* `scripts/run_selection_function.py` (injection-recovery grid runner using frozen ResNet-18; outputs completeness with Bayesian binomial 68% intervals)
* minimal shared utilities (`stronglens_calib/`)

[Download the package](sandbox:/mnt/data/stronglens_selection_function_addons.zip)

## E. Selection function watchouts (the ones that actually bite)

1. **Leakage via host cutouts:** do not inject onto any host cutout that appears in training (use host split = test or a dedicated holdout host pool).
2. **Preprocessing mismatch:** the scoring path must use the exact same normalization and crop as training.
3. **Flux units / zeropoint:** ensure injected flux is in **nanomaggies** consistent with DR10 cutouts (AB zeropoint 22.5).
4. **PSF handling:** if you bin by PSF, document whether you (i) select hosts already matching the PSF bin, or (ii) convolve injected source to a target PSF. Do not silently mix approaches.
5. **Depth axis definition:** be explicit whether depth is `psfdepth_r` or `galdepth_r` and how you map it to “mag depth.”
6. **Acceptance diagnostics:** do not publish completeness surfaces until you pass basic realism checks (SNR/color/noise sanity) and document thresholds.

If you want, paste the column names in `training_v1.parquet` (or just the schema), and I’ll adjust the scripts’ default column names to match your manifest exactly.

===========

Generate full code and give download links. # Verified schema: training_v1.parquet

---

## Short prompt to paste to the LLM

**Copy the following to the LLM:**

> Our training manifest is `training_v1.parquet`. Below is the **verified schema** (column names and Parquet/PyArrow types) read from the actual file on Lambda NFS using PyArrow. Please adjust your scripts’ default column names to match this manifest exactly: use `label` (int64) for labels, `split` (string) for train/val/test, `cutout_path` (string) for the path to each .npz, `sample_weight` (double) for per-sample loss weights, and `tier` (string) for stratum evaluation. Types are as listed; handle nullable optional columns as needed.

---

**Source:** `manifests/training_v1.parquet` on Lambda NFS  
`/lambda/nfs/darkhaloscope-training-dc/stronglens_calibration/manifests/training_v1.parquet`

**Verified:** 2026-02-10 via PyArrow on Lambda (parquet binary read). Use these column names and types so your scripts match our manifest exactly.

---

## Column names and Parquet/Arrow types

| Column name         | Parquet / PyArrow type | Notes |
|---------------------|------------------------|--------|
| cutout_path         | string                 | Full path to .npz cutout file. **Required for training/eval.** |
| filename            | string                 | Basename of cutout. |
| galaxy_id           | string                 | Identifier (e.g. brick_objid or from path stem). |
| label               | int64                  | **Required.** 1 = positive (lens), 0 = negative. |
| cutout_type         | string                 | "positive" or "negative". |
| ra                  | double                 | Right ascension. |
| dec                 | double                 | Declination. |
| size                | int64                  | Cutout size (e.g. 101). |
| pixel_scale         | double                 | Arcsec per pixel. |
| timestamp           | string                 | Cutout generation timestamp. |
| pipeline_version    | string                 | Pipeline version. |
| cutout_url          | string                 | URL if from remote. |
| layer               | string                 | Layer / survey. |
| bands_requested     | string                 | Bands (e.g. g,r,z). |
| tier                | string                 | **Required for stratum eval.** "A" | "B" (positives) or "N1" | "N2" (negatives). |
| weight              | double                 | Legacy weight column. |
| nobs_z              | int64                  | z-band exposure count. |
| match_type          | string                 | Match type if from crossmatch. |
| brickname           | string                 | DR10 brick name. |
| nan_count_g         | int64                  | NaN count in g band. |
| nan_count_r         | int64                  | NaN count in r band. |
| nan_count_z         | int64                  | NaN count in z band. |
| nan_frac            | double                 | Fraction of NaN pixels. |
| central_nan_frac    | double                 | NaN fraction in central region. |
| has_g               | bool                   | g band present. |
| has_r               | bool                   | r band present. |
| has_z               | bool                   | z band present. |
| core_brightness_r   | double                 | Core brightness (r). |
| core_max_r          | double                 | Core max (r). |
| annulus_brightness_r| double                 | Annulus brightness (r). |
| annulus_std_r       | double                 | Annulus std (r). |
| outer_brightness_r  | double                 | Outer brightness (r). |
| radial_gradient_r   | double                 | Radial gradient (r). |
| mad_r               | double                 | MAD (r). |
| std_r               | double                 | Std (r). |
| median_r            | double                 | Median (r). |
| mean_r              | double                 | Mean (r). |
| percentile_1_r      | double                 | 1st percentile (r). |
| percentile_99_r     | double                 | 99th percentile (r). |
| clip_frac_r         | double                 | Clip fraction (r). |
| quality_ok          | bool                   | Quality flag. |
| split               | string                 | **Required.** "train" | "val" | "test". |
| sample_weight       | double                 | **Required for training.** Per-sample loss weight (e.g. 1.0 Tier-A, 0.5 Tier-B, 1.0 N1/N2). |
| pool                | string                 | Negative pool: "N1" | "N2". |
| confuser_category   | string                 | N2 confuser category if applicable. |
| nobs_z_bin          | string                 | Binned nobs_z. |
| type                | string                 | Tractor type (e.g. SER, DEV, REX). |
| type_bin            | string                 | Binned type. |
| healpix_128         | double                 | HEALPix index (nside=128). |
| psfsize_r           | double                 | PSF size (r). |
| psfdepth_r          | double                 | PSF depth (r). |
| galdepth_r          | double                 | Galaxy depth (r). |
| ebv                 | double                 | E(B-V). |

---

## Script defaults to align with

- **Label column:** `label` (int64): 0 or 1.
- **Split column:** `split` (string): "train" | "val" | "test".
- **Cutout path column:** `cutout_path` (string): path to .npz file.
- **Sample weight column:** `sample_weight` (double): use for weighted loss.
- **Tier column (optional):** `tier` (string): for stratum / held-out Tier-A evaluation.
- **ID for disjointness checks:** `galaxy_id` (string) or `cutout_path` (string); both unique per row.

---

## Type mapping (for implementers)

- **string** → PyArrow `string`; Pandas `object` or `string`.
- **int64** → PyArrow `int64`; Pandas `int64`.
- **double** → PyArrow `double`; Pandas `float64`.
- **bool** → PyArrow `bool`; Pandas `bool`.

All types above are nullable in Parquet unless the column has no nulls; scripts should handle possible nulls for optional columns (e.g. `tier`, `pool`).

-----

LLM : 

[Download the full code package (schema-aligned)](sandbox:/mnt/data/stronglens_calibration_code_schema_aligned.zip)

Contents:

* `scripts/verify_splits.py`
  Verifies split integrity and disjointness using **`galaxy_id`** and **`cutout_path`** (per your verified schema). Optionally fails the run if overlaps exist.
* `scripts/bootstrap_eval.py`
  Computes **AUC / recall / precision** on a chosen split (default `test`) and adds **bootstrap 68% + 95% intervals**. Uses `cutout_path` to merge predictions onto the manifest. Accepts predictions as Parquet/CSV with `cutout_path` and `score` (or `logit`).
* `scripts/run_selection_function.py`
  Runnable selection function scaffold: builds the (θE, PSF, depth) grid and performs injection-recovery scoring with the frozen model. The injector is a **minimal arc proxy** (replace with your calibrated Phase 4c renderer for publication-grade realism). Uses only negative hosts (`label==0`) from a chosen host split.

All scripts default to your verified manifest column names:

* `label` (int64), `split` (string), `cutout_path` (string), `sample_weight` (double), `tier` (string), `galaxy_id` (string).

=====================
