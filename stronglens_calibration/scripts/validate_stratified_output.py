#!/usr/bin/env python3
"""
Validate Stratified Sampling Output

Checks the output of spark_stratified_sample.py for correctness:
- Row counts, schema completeness
- No nulls in critical columns, no duplicate galaxy_ids
- Pool N1:N2 ratio within tolerance
- Split ratio 70/15/15 within tolerance
- All strata represented
- Coordinate ranges valid

Usage:
    python scripts/validate_stratified_output.py --input /path/to/output/data/
    python scripts/validate_stratified_output.py --input s3://bucket/prefix/data/ --summary s3://bucket/prefix/summary.json

Author: Generated for stronglens_calibration project
Date: 2026-02-11
"""
import argparse
import json
import sys
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd


def validate(input_path: str, min_rows: int = 200000) -> dict:
    """Run all validation checks. Returns report dict."""
    print(f"Loading data from: {input_path}")

    if input_path.startswith("s3://"):
        # For S3, use pyarrow with s3fs
        import pyarrow.parquet as pq
        import s3fs
        fs = s3fs.S3FileSystem()
        dataset = pq.ParquetDataset(input_path.replace("s3://", ""), filesystem=fs)
        df = dataset.read().to_pandas()
    else:
        # Local path
        parquet_files = list(Path(input_path).glob("*.parquet"))
        if not parquet_files:
            print(f"ERROR: No parquet files found in {input_path}")
            sys.exit(1)
        dfs = [pd.read_parquet(f) for f in parquet_files]
        df = pd.concat(dfs, ignore_index=True)

    print(f"Loaded {len(df):,} rows, {len(df.columns)} columns")
    print(f"Columns: {list(df.columns)}")

    checks = {}
    details = {}

    # 1. Row count
    checks["row_count_ok"] = len(df) >= min_rows
    details["row_count"] = len(df)
    details["min_rows_expected"] = min_rows

    # 2. No nulls in critical columns
    critical_cols = ["galaxy_id", "ra", "dec", "pool", "split", "type_bin", "nobs_z_bin"]
    null_report = {}
    for col in critical_cols:
        if col in df.columns:
            null_count = int(df[col].isna().sum())
            null_report[col] = null_count
        else:
            null_report[col] = f"MISSING COLUMN"
    checks["no_nulls_critical"] = all(
        v == 0 for v in null_report.values() if isinstance(v, int)
    ) and all(isinstance(v, int) for v in null_report.values())
    details["null_report"] = null_report

    # 3. No duplicate galaxy_ids
    if "galaxy_id" in df.columns:
        unique_count = df["galaxy_id"].nunique()
        dup_count = len(df) - unique_count
        checks["no_duplicates"] = dup_count == 0
        details["duplicates"] = dup_count
    else:
        checks["no_duplicates"] = False
        details["duplicates"] = "galaxy_id column missing"

    # 4. Pool N1:N2 ratio within 75-95% N1
    if "pool" in df.columns:
        pool_dist = df["pool"].value_counts(normalize=True) * 100
        n1_pct = float(pool_dist.get("N1", 0))
        checks["n1_n2_ratio_ok"] = 75.0 <= n1_pct <= 95.0
        details["pool_distribution"] = df["pool"].value_counts().to_dict()
        details["n1_percentage"] = round(n1_pct, 2)
    else:
        checks["n1_n2_ratio_ok"] = False

    # 5. Split ratio ~70/15/15 within +-5%
    if "split" in df.columns:
        split_dist = df["split"].value_counts(normalize=True) * 100
        train_pct = float(split_dist.get("train", 0))
        val_pct = float(split_dist.get("val", 0))
        test_pct = float(split_dist.get("test", 0))
        checks["split_ratio_ok"] = (
            65.0 <= train_pct <= 75.0 and
            10.0 <= val_pct <= 20.0 and
            10.0 <= test_pct <= 20.0
        )
        details["split_distribution"] = df["split"].value_counts().to_dict()
        details["split_percentages"] = {
            "train": round(train_pct, 2),
            "val": round(val_pct, 2),
            "test": round(test_pct, 2),
        }
    else:
        checks["split_ratio_ok"] = False

    # 6. All strata represented (LLM: TYPE_BINS = SER/DEV/REX for Paper IV parity)
    if "type_bin" in df.columns and "nobs_z_bin" in df.columns:
        strata = df.groupby(["nobs_z_bin", "type_bin"]).size().reset_index(name="count")
        n_strata = len(strata)
        n_type_bins = df["type_bin"].nunique()
        n_nobs_bins = df["nobs_z_bin"].nunique()
        checks["all_strata_ok"] = n_type_bins >= 3 and n_nobs_bins >= 3
        details["n_strata"] = n_strata
        details["type_bins"] = sorted(df["type_bin"].unique().tolist())
        details["nobs_z_bins"] = sorted(df["nobs_z_bin"].unique().tolist())
        
        # LLM check: Paper IV parity requires SER/DEV/REX (not EXP)
        expected_types = {"SER", "DEV", "REX"}
        actual_types = set(df["type_bin"].unique())
        has_exp = "EXP" in actual_types
        checks["paper_iv_type_parity"] = expected_types.issubset(actual_types)
        details["has_EXP_in_negatives"] = has_exp
        if has_exp:
            details["EXP_warning"] = "EXP present in negatives; Paper IV parity uses SER/DEV/REX only"
    else:
        checks["all_strata_ok"] = False
        checks["paper_iv_type_parity"] = False

    # 7. Coordinate ranges valid
    if "ra" in df.columns and "dec" in df.columns:
        ra_ok = df["ra"].between(0, 360).all()
        dec_ok = df["dec"].between(-90, 90).all()
        checks["coords_valid"] = bool(ra_ok and dec_ok)
        details["ra_range"] = [round(float(df["ra"].min()), 4), round(float(df["ra"].max()), 4)]
        details["dec_range"] = [round(float(df["dec"].min()), 4), round(float(df["dec"].max()), 4)]
    else:
        checks["coords_valid"] = False

    # ---------------------------------------------------------------
    # LLM-specified validation checks (from conversation_with_llm.txt)
    # ---------------------------------------------------------------
    
    # 8. Per-stratum availability: counts by (nobs_z_bin, type_bin, pool, split)
    #    LLM: "ensure N2 exists across bins"
    if all(c in df.columns for c in ["nobs_z_bin", "type_bin", "pool", "split"]):
        stratum_detail = df.groupby(["nobs_z_bin", "type_bin", "pool", "split"]).size()
        details["per_stratum_counts"] = {
            str(k): int(v) for k, v in stratum_detail.items()
        }
        # Check N2 exists in at least some strata
        n2_strata = df[df["pool"] == "N2"].groupby(["nobs_z_bin", "type_bin"]).size()
        details["n2_strata_count"] = len(n2_strata)
        details["n2_strata_missing"] = n_strata - len(n2_strata) if "n_strata" in dir() else "unknown"
        checks["n2_exists_across_bins"] = len(n2_strata) > 0
    
    # 9. N1:N2 global ratio check (LLM: "enforce 85:15 globally, not per stratum")
    if "pool" in df.columns:
        total = len(df)
        n1_global = len(df[df["pool"] == "N1"])
        n2_global = len(df[df["pool"] == "N2"])
        n1_global_pct = (n1_global / total * 100) if total > 0 else 0
        n2_global_pct = (n2_global / total * 100) if total > 0 else 0
        details["n1_n2_global"] = {
            "N1": n1_global, "N2": n2_global,
            "N1_pct": round(n1_global_pct, 2),
            "N2_pct": round(n2_global_pct, 2),
        }
        # LLM: observed N2 rate ~5.8%, global 85:15 is a goal not hard constraint
        # Accept anything from 5% to 25% N2 globally
        checks["n2_global_ratio_ok"] = 5.0 <= n2_global_pct <= 25.0
    
    # 10. Distribution sanity: compare neg type distribution vs expected
    #     LLM: "compare negatives vs positives distributions for (nobs_z, type)"
    if "type_bin" in df.columns:
        type_dist = df["type_bin"].value_counts(normalize=True).to_dict()
        details["type_distribution_pct"] = {
            k: round(v * 100, 2) for k, v in type_dist.items()
        }
    if "nobs_z_bin" in df.columns:
        nobs_dist = df["nobs_z_bin"].value_counts(normalize=True).to_dict()
        details["nobs_distribution_pct"] = {
            k: round(v * 100, 2) for k, v in nobs_dist.items()
        }
    
    # 11. Deterministic sampling check: no duplicates implies stable hash ordering
    #     LLM: "use stable hash ordering + window for determinism"
    #     (Already checked via no_duplicates above)

    # Summary
    passed = sum(checks.values())
    total = len(checks)
    all_passed = passed == total

    report = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "input_path": input_path,
        "checks": checks,
        "details": details,
        "passed": passed,
        "total": total,
        "all_passed": all_passed,
    }

    # Print summary
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    for check, result in checks.items():
        status = "PASS" if result else "FAIL"
        print(f"  [{status}] {check}")
    print(f"\nPassed: {passed}/{total}")
    if all_passed:
        print("\nALL CHECKS PASSED - Output is valid")
    else:
        print("\nSOME CHECKS FAILED - Review before proceeding")

    return report


def main():
    parser = argparse.ArgumentParser(description="Validate stratified sampling output")
    parser.add_argument("--input", required=True, help="Path to output data/ directory (local or S3)")
    parser.add_argument("--min-rows", type=int, default=200000,
                       help="Minimum expected row count (default: 200000, use lower for test runs)")
    parser.add_argument("--report", help="Path to save validation report JSON")

    args = parser.parse_args()

    report = validate(args.input, min_rows=args.min_rows)

    if args.report:
        with open(args.report, "w") as f:
            json.dump(report, f, indent=2, default=str)
        print(f"\nReport saved to: {args.report}")

    sys.exit(0 if report["all_passed"] else 1)


if __name__ == "__main__":
    main()
