# Paper IV Parity: EfficientNetV2-S (ImageNet pretrained) â€” v2 with fixes
# ~21.5M params, matches Paper IV's EfficientNetV2 architecture
# Protocol: 160 epochs, StepLR halve@130, effective batch 512, unweighted CE, 101x101
# Date: 2026-02-12
# Fixes vs v1:
#   1. Epoch-aware augmentation seed (prevents memorisation of fixed augmented images)
#   2. Backbone frozen for first 5 epochs (prevents catastrophic forgetting of ImageNet features)
#   3. Linear LR warmup over 5 epochs (prevents large initial gradient updates)

dataset:
  parquet_path: ""
  manifest_path: /lambda/nfs/darkhaloscope-training-dc/stronglens_calibration/manifests/training_parity_70_30_v1.parquet
  mode: file_manifest
  preprocessing: raw_robust
  label_col: label
  cutout_path_col: cutout_path
  sample_weight_col: sample_weight
  seed: 42
  crop: false           # Keep 101x101 for Paper IV parity
  crop_size: 0

augment:
  hflip: true
  vflip: true
  rot90: true

train:
  arch: efficientnet_v2_s
  epochs: 160
  batch_size: 64          # Micro-batch (EfficientNetV2-S needs more memory)
  effective_batch: 512    # 512/64 = 8 accumulation steps
  lr: 0.000388            # 3.88e-4 per Paper IV
  weight_decay: 0.0001
  lr_schedule: step
  lr_step_epoch: 130      # Halve LR at epoch 130
  lr_gamma: 0.5
  num_workers: 8
  device: cuda
  early_stopping_patience: 0   # Run all 160 epochs
  out_dir: /lambda/nfs/darkhaloscope-training-dc/stronglens_calibration/checkpoints/paperIV_efficientnet_v2_s_v2
  mixed_precision: true
  unweighted_loss: true
  pretrained: true        # ImageNet pretrained
  freeze_backbone_epochs: 5    # Freeze backbone for 5 epochs, train classifier head only
  warmup_epochs: 5             # Linear LR warmup from lr/100 to lr over 5 epochs
